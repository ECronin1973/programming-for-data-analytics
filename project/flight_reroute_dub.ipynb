{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae6fbf9",
   "metadata": {},
   "source": [
    "# Flight Rerouting Analysis at Dublin Airport\n",
    "\n",
    "This project investigates flight rerouting events at Dublin Airport and their relationship to local weather conditions. The analysis combines flight activity data with historical and forecast weather data from Met Ã‰ireann to identify trends, visualise reroute reasons, and project future rerouting probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037f7b5",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Step 2 â€“ Install and Import Required Libraries\n",
    "\n",
    "This step prepares the environment for the Dublin Airport Flight Rerouting Project.  \n",
    "It ensures that all required Python packages are available and sets up the projectâ€™s directory structure inside the `project` root.\n",
    "\n",
    "The notebook imports essential libraries for:\n",
    "\n",
    "- ğŸ“Š **Data manipulation** (`pandas`, `numpy`)\n",
    "- ğŸ“… **Date and time handling** (`datetime`, `matplotlib.dates`)\n",
    "- ğŸ“ˆ **Plotting and visualisation** (`matplotlib`, `seaborn`, `plotly`)\n",
    "- ğŸ¤– **Machine learning and model persistence** (`scikit-learn`, `joblib`)\n",
    "- ğŸ“‚ **File handling and paths** (`os`, `pathlib`, `json`)\n",
    "- ğŸŒ **Web access** (`requests`)\n",
    "- ğŸ§© **Interactivity and display** (`ipywidgets`, `IPython.display`)\n",
    "\n",
    "It also defines key directories (`data`, `outputs`, `models`, `docs`) inside the `project` folder and ensures they exist.  \n",
    "This structure keeps raw data, processed outputs, trained models, and documentation organised and reproducible.\n",
    "\n",
    "ğŸ“Œ *Note: `%pip install` commands can be used inside Jupyter notebooks if a package is missing.  \n",
    "For scripts or terminal use, run `pip install` directly.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86305781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Project root: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\n",
      "Data directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\data\n",
      "Output directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\outputs\n",
      "Model directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\models\n",
      "Docs directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\docs\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly --quiet\n",
    "\n",
    "# Setup: imports, paths and basic config\n",
    "import json  # for any config files\n",
    "from pathlib import Path  # for path management\n",
    "import numpy as np  # numerical operations\n",
    "import pandas as pd  # data manipulation\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "import seaborn as sns  # enhanced plotting\n",
    "import plotly.express as px  # interactive plotting\n",
    "import warnings  # to manage warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score  # model validation\n",
    "from sklearn.linear_model import LogisticRegression  # example model\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # example model\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix  # model evaluation\n",
    "import joblib  # model persistence\n",
    "\n",
    "# Plotting style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Explicit project root: programming-for-data-analytics/project\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name != \"project\":\n",
    "    # climb up until we find project folder\n",
    "    for parent in ROOT.parents:\n",
    "        if parent.name == \"project\":\n",
    "            ROOT = parent\n",
    "            break\n",
    "\n",
    "# Define key directories inside project\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\"\n",
    "MODEL_DIR = ROOT / \"models\"\n",
    "DOCS_DIR = ROOT / \"docs\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for path in [DATA_DIR, OUTPUT_DIR, MODEL_DIR, DOCS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Docs directory: {DOCS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468332e9",
   "metadata": {},
   "source": [
    "### Step 3 â€“ Utilise Helper Functions for Dublin Airport Data Processing\n",
    "\n",
    "This section defines a set of reusable helper functions that simplify common tasks in the project.  \n",
    "They are designed specifically to support the analysis of **Dublin Airport flight activity and weather data** by handling messy inputs and preparing clean datasets for exploration and modelling.\n",
    "\n",
    "The functions help with:\n",
    "\n",
    "- âœ… Detecting and parsing inconsistent datetime formats in flight and weather logs  \n",
    "- âœ… Standardising and cleaning temperature columns from Met Ã‰ireann datasets  \n",
    "- âœ… Loading and preparing Dublin Airport daily weather data from local CSV files  \n",
    "- âœ… Defining Irish seasonal boundaries for rerouting analysis (Winter, Spring, Summer, Autumn)  \n",
    "- âœ… Filtering weather data for a custom date range to align with flight events  \n",
    "- âœ… Validating user-provided date inputs for reproducible analysis  \n",
    "- âœ… Detecting header rows in raw CSV files downloaded from dashboards  \n",
    "\n",
    "Each helper is **modular** â€” it performs one clear task and can be reused across notebooks and scripts.  \n",
    "This improves readability, reduces duplication, and supports good programming practices for the final project.\n",
    "\n",
    "ğŸ“Œ *Tip: These helpers are written to be beginner-friendly, with comments explaining their purpose and logic. They make it easier to align flight activity with weather conditions when investigating rerouting events.*\n",
    "\n",
    "ğŸ“– References:  \n",
    "- [Real Python â€“ Python Helper Functions](https://realpython.com/defining-your-own-python-function/)  \n",
    "- [GeeksforGeeks â€“ Python Helper Functions](https://www.geeksforgeeks.org/python-helper-functions/)  \n",
    "- [Wikipedia â€“ DRY Principle (Don't Repeat Yourself)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607be6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‚ Helper Functions for Dublin Airport Project\n",
    "# These functions help with parsing dates, cleaning weather data, handling temperature columns,\n",
    "# defining Irish seasons, preparing data ranges, and detecting CSV headers.\n",
    "# Keep them in one cell so they are easy to reuse across the notebook.\n",
    "\n",
    "# important imports already loaded above\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\"\"\"\n",
    "\n",
    "# ğŸ“… Detect the most likely datetime format from sample strings\n",
    "def detect_datetime_format(samples, formats, dayfirst=True):\n",
    "    \"\"\"\n",
    "    Try each format and return the one that matches at least 70% of samples.\n",
    "    Helps ensure consistent parsing of date strings.\n",
    "    \"\"\"\n",
    "    for fmt in formats:\n",
    "        parsed = pd.to_datetime(samples, format=fmt, dayfirst=dayfirst, errors='coerce')\n",
    "        if parsed.notna().sum() >= max(1, int(len(samples) * 0.7)):\n",
    "            return fmt\n",
    "    return None\n",
    "\n",
    "# ğŸ“… Parse a datetime column using format detection or fallback\n",
    "def parse_datetime_column(df, date_col, candidate_formats=None, dayfirst=True):\n",
    "    \"\"\"\n",
    "    Parse a datetime column using known formats.\n",
    "    Falls back to flexible parsing if none match.\n",
    "    \"\"\"\n",
    "    if candidate_formats is None:\n",
    "        candidate_formats = [\n",
    "            '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%d-%b-%Y %H:%M',\n",
    "            '%d/%m/%Y %H:%M:%S', '%d/%m/%Y %H:%M', '%d-%m-%Y %H:%M',\n",
    "            '%d %b %Y %H:%M', '%d %B %Y %H:%M',\n",
    "        ]\n",
    "\n",
    "    sample_vals = df[date_col].dropna().astype(str).head(80).tolist()\n",
    "    chosen_fmt = detect_datetime_format(sample_vals, candidate_formats, dayfirst=dayfirst)\n",
    "\n",
    "    if chosen_fmt:\n",
    "        print(f\"âœ… Detected datetime format: {chosen_fmt}\")\n",
    "        return pd.to_datetime(df[date_col], format=chosen_fmt, dayfirst=dayfirst, errors='coerce')\n",
    "    else:\n",
    "        print(\"âš ï¸ No single format matched. Falling back to flexible parsing.\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', message='Could not infer format')\n",
    "            return pd.to_datetime(df[date_col], dayfirst=dayfirst, errors='coerce')\n",
    "\n",
    "# ğŸŒ¡ï¸ Ensure temperature column is numeric and named 'temp'\n",
    "def parse_temperature_column(df, col_name='temp'):\n",
    "    \"\"\"\n",
    "    Convert the temperature column to numeric and rename it to 'temp'.\n",
    "    If no exact match, look for any column containing 'temp'.\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        col_name = next((c for c in df.columns if 'temp' in c.lower()), None)\n",
    "        if col_name is None:\n",
    "            raise KeyError(\"No temperature column found.\")\n",
    "    df['temp'] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# ğŸ“‚ Load cleaned weather data from local CSV\n",
    "def load_cleaned_weather_data(filepath=\"data/dublin_airport_daily.csv\"):\n",
    "    \"\"\"\n",
    "    Load weather dataset from CSV and strip spaces from column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "# ğŸ‚ Define Irish seasonal boundaries for a given year\n",
    "def define_irish_seasons(year=2025):\n",
    "    \"\"\"\n",
    "    Return start and end dates for Irish meteorological seasons.\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        (\"Winter\", pd.Timestamp(f\"{year-1}-12-01\"), pd.Timestamp(f\"{year}-02-28 23:59\")),\n",
    "        (\"Spring\", pd.Timestamp(f\"{year}-03-01\"), pd.Timestamp(f\"{year}-05-31 23:59\")),\n",
    "        (\"Summer\", pd.Timestamp(f\"{year}-06-01\"), pd.Timestamp(f\"{year}-08-31 23:59\")),\n",
    "        (\"Autumn\", pd.Timestamp(f\"{year}-09-01\"), pd.Timestamp(f\"{year}-11-30 23:59\")),\n",
    "    ]\n",
    "    return pd.DataFrame(data, columns=[\"season\", \"start\", \"end\"])\n",
    "\n",
    "# ğŸ“Š Filter and prepare temperature data for a custom date range\n",
    "def prepare_temperature_data(df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Filter weather data to a date range and add useful time features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    if 'date' not in df.columns:\n",
    "        raise KeyError(\"Expected 'date' column not found.\")\n",
    "\n",
    "    # Try parsing with a common format, fallback to flexible parsing\n",
    "    try:\n",
    "        df['datetime'] = pd.to_datetime(df['date'], format='%d-%b-%Y %H:%M', errors='raise')\n",
    "    except Exception:\n",
    "        df['datetime'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "    df = df.dropna(subset=['datetime'])\n",
    "    mask = (df['datetime'] >= start_date) & (df['datetime'] <= end_date)\n",
    "    range_df = df.loc[mask].copy()\n",
    "\n",
    "    # Add date and hour columns for plotting\n",
    "    range_df['date'] = range_df['datetime'].dt.date\n",
    "    range_df['hour'] = range_df['datetime'].dt.strftime('%H:%M')\n",
    "\n",
    "    return range_df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# ğŸ“† Convert user input strings into a validated date range\n",
    "def get_custom_range(start_str, end_str):\n",
    "    \"\"\"\n",
    "    Convert string inputs into datetime objects and validate order.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start = pd.to_datetime(start_str)\n",
    "        end = pd.to_datetime(end_str)\n",
    "        if start > end:\n",
    "            raise ValueError(\"Start date must be before end date.\")\n",
    "        return start, end\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Invalid date range: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ğŸ” Detect the header row in a CSV file\n",
    "def detect_header(lines):\n",
    "    \"\"\"\n",
    "    Detect the most likely header row in a CSV file.\n",
    "    Looks for lines starting with 'date' or 'station' and containing commas.\n",
    "    \"\"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        line_lower = line.strip().lower()\n",
    "        if (line_lower.startswith(\"station\") or line_lower.startswith(\"date\")) and \",\" in line:\n",
    "            columns = line.split(\",\")\n",
    "            if len(columns) > 5:  # Header rows usually have multiple columns\n",
    "                return i\n",
    "    print(\"âš ï¸ Warning: header row not found. Defaulting to first line.\")\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecf4c3",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Step 4 â€“ Download Dublin Airport Daily Data and Detect Header Row\n",
    "\n",
    "In this step, the notebook retrieves the **Dublin Airport Daily Data CSV** directly from Met Ã‰ireannâ€™s open data service.  \n",
    "This dataset contains daily weather observations (e.g., precipitation, temperature, wind speed, radiation) recorded at Dublin Airport, which will later be aligned with flight activity logs to analyse rerouting events.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸŒ **Downloading the raw CSV** from Met Ã‰ireann using the `requests` library.  \n",
    "- ğŸ“‚ **Defining a local output path** (`data/dublin_airport_daily.csv`) to store the file inside the projectâ€™s `data` folder.  \n",
    "- âœ… **Checking the HTTP response** to ensure the download was successful.  \n",
    "- ğŸ“‘ **Splitting the file into lines** so the structure can be inspected before loading into pandas.  \n",
    "- ğŸ” **Detecting the header row** using the `detect_header` helper function defined earlier.  \n",
    "  This ensures that column names (such as `date`, `maxtp`, `mintp`, `rain`, `wdsp`) are correctly identified even if the file contains metadata lines at the top.  \n",
    "- ğŸ–¨ï¸ **Printing the detected header row** to confirm the correct starting point for parsing.\n",
    "\n",
    "ğŸ“Œ *Tip: Detecting the header row is important because Met Ã‰ireann CSVs often include metadata lines before the actual data table.  \n",
    "By confirming the header row, you avoid misaligned columns and ensure clean parsing in later steps.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94cc44a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Header row detected at line 25:\n",
      "date,ind,maxtp,ind,mintp,igmin,gmin,ind,rain,cbl,wdsp,ind,hm,ind,ddhm,ind,hg,sun,dos,g_rad,soil,pe,evap,smd_wd,smd_md,smd_pd\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ Step 4 â€“ Download Dublin Airport Daily Data CSV and Detect Header Row\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# --- Define output path for cleaned CSV ---\n",
    "DATA_PATH = Path(\"data/dublin_airport_daily.csv\")\n",
    "\n",
    "# --- Download raw CSV from Met Ã‰ireann (Dublin Airport Daily Data) ---\n",
    "url = \"https://cli.fusio.net/cli/climate_data/webdata/dly532.csv\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# âœ… Check for successful response\n",
    "if response.status_code != 200:\n",
    "    raise RuntimeError(f\"âŒ Failed to download data: HTTP {response.status_code}\")\n",
    "\n",
    "# --- Split response into lines ---\n",
    "lines = response.text.splitlines()\n",
    "\n",
    "# --- Detect header row using helper function ---\n",
    "header_index = detect_header(lines)\n",
    "\n",
    "# âœ… Confirm detected header row\n",
    "print(f\"âœ… Header row detected at line {header_index}:\")\n",
    "print(lines[header_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f4588",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 4b â€“ Load and Inspect Dublin Airport Daily Data\n",
    "\n",
    "After detecting the correct header row in the raw CSV file, the next step is to **load the dataset into pandas**.  \n",
    "This allows us to immediately inspect the structure of the Dublin Airport Daily Data and confirm that the columns (e.g., `date`, `maxtp`, `mintp`, `rain`, `wdsp`) are correctly aligned.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸ“‚ Reading the CSV into a pandas DataFrame, starting from the detected header row  \n",
    "- ğŸ” Displaying the first few rows with `head()` to verify column names and sample values  \n",
    "- ğŸ§¾ Using `info()` to check datatypes and identify potential missing values  \n",
    "- ğŸ“Š Summarising numeric columns with `describe()` to get a quick statistical overview  \n",
    "\n",
    "ğŸ“Œ *Why this matters:* Inspecting the dataset before saving ensures that the header detection worked correctly and that the file is ready for consistent downstream analysis.  \n",
    "This step acts as a validation checkpoint before committing the cleaned file to the `data/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c71e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of Dublin Airport Daily Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ind</th>\n",
       "      <th>maxtp</th>\n",
       "      <th>ind.1</th>\n",
       "      <th>mintp</th>\n",
       "      <th>igmin</th>\n",
       "      <th>gmin</th>\n",
       "      <th>ind.2</th>\n",
       "      <th>rain</th>\n",
       "      <th>cbl</th>\n",
       "      <th>...</th>\n",
       "      <th>hg</th>\n",
       "      <th>sun</th>\n",
       "      <th>dos</th>\n",
       "      <th>g_rad</th>\n",
       "      <th>soil</th>\n",
       "      <th>pe</th>\n",
       "      <th>evap</th>\n",
       "      <th>smd_wd</th>\n",
       "      <th>smd_md</th>\n",
       "      <th>smd_pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.3</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1016.2</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1001.5</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1013.4</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>3.4</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  ind  maxtp  ind.1  mintp  igmin gmin  ind.2  rain     cbl  \\\n",
       "0  01-jan-1942    0    9.7      0    6.8      0  4.7      2   0.0  1020.3   \n",
       "1  02-jan-1942    0    9.9      0    7.9      0  6.7      0   0.1  1016.2   \n",
       "2  03-jan-1942    0   11.2      0    8.9      0  7.2      0   1.5  1006.8   \n",
       "3  04-jan-1942    0    9.2      0    2.7      0  3.4      0   3.5  1001.5   \n",
       "4  05-jan-1942    0    3.5      1   -0.8      0  0.0      0   0.6  1013.4   \n",
       "\n",
       "   ...  hg  sun dos  g_rad soil   pe evap  smd_wd smd_md smd_pd  \n",
       "0  ...      0.0   0              1.1  1.4                        \n",
       "1  ...      0.0   0              0.7  0.9                        \n",
       "2  ...      0.1   0              0.5  0.6                        \n",
       "3  ...      0.6   0              0.6  0.7                        \n",
       "4  ...      3.4   0              0.6  0.7                        \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30620 entries, 0 to 30619\n",
      "Data columns (total 26 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    30620 non-null  object \n",
      " 1   ind     30620 non-null  int64  \n",
      " 2   maxtp   30620 non-null  float64\n",
      " 3   ind.1   30620 non-null  int64  \n",
      " 4   mintp   30620 non-null  float64\n",
      " 5   igmin   30620 non-null  int64  \n",
      " 6   gmin    30620 non-null  object \n",
      " 7   ind.2   30620 non-null  int64  \n",
      " 8   rain    30620 non-null  float64\n",
      " 9   cbl     30620 non-null  float64\n",
      " 10  wdsp    30620 non-null  float64\n",
      " 11  ind.3   30620 non-null  int64  \n",
      " 12  hm      30620 non-null  object \n",
      " 13  ind.4   30620 non-null  int64  \n",
      " 14  ddhm    30620 non-null  object \n",
      " 15  ind.5   30620 non-null  int64  \n",
      " 16  hg      30620 non-null  object \n",
      " 17  sun     30620 non-null  float64\n",
      " 18  dos     30620 non-null  object \n",
      " 19  g_rad   30620 non-null  object \n",
      " 20  soil    30620 non-null  object \n",
      " 21  pe      30620 non-null  float64\n",
      " 22  evap    30620 non-null  object \n",
      " 23  smd_wd  30620 non-null  object \n",
      " 24  smd_md  30620 non-null  object \n",
      " 25  smd_pd  30620 non-null  object \n",
      "dtypes: float64(7), int64(7), object(12)\n",
      "memory usage: 6.1+ MB\n",
      "None\n",
      "\n",
      "Summary statistics:\n",
      "               date           ind         maxtp         ind.1         mintp  \\\n",
      "count         30620  30620.000000  30620.000000  30620.000000  30620.000000   \n",
      "unique        30620           NaN           NaN           NaN           NaN   \n",
      "top     01-jan-1942           NaN           NaN           NaN           NaN   \n",
      "freq              1           NaN           NaN           NaN           NaN   \n",
      "mean            NaN      0.001078     13.089902      0.083377      6.169069   \n",
      "std             NaN      0.033792      4.911569      0.276574      4.385220   \n",
      "min             NaN      0.000000     -4.700000      0.000000    -12.200000   \n",
      "25%             NaN      0.000000      9.500000      0.000000      2.900000   \n",
      "50%             NaN      0.000000     13.000000      0.000000      6.300000   \n",
      "75%             NaN      0.000000     16.900000      0.000000      9.600000   \n",
      "max             NaN      2.000000     29.100000      2.000000     18.400000   \n",
      "\n",
      "               igmin   gmin         ind.2          rain           cbl  ...  \\\n",
      "count   30620.000000  30620  30620.000000  30620.000000  30620.000000  ...   \n",
      "unique           NaN    301           NaN           NaN           NaN  ...   \n",
      "top              NaN    6.1           NaN           NaN           NaN  ...   \n",
      "freq             NaN    272           NaN           NaN           NaN  ...   \n",
      "mean        0.217799    NaN      0.885565      2.072227   1003.497169  ...   \n",
      "std         0.416302    NaN      1.232408      4.393421     11.732616  ...   \n",
      "min         0.000000    NaN      0.000000      0.000000    949.600000  ...   \n",
      "25%         0.000000    NaN      0.000000      0.000000    996.200000  ...   \n",
      "50%         0.000000    NaN      0.000000      0.200000   1004.500000  ...   \n",
      "75%         0.000000    NaN      2.000000      2.200000   1011.700000  ...   \n",
      "max         4.000000    NaN      4.000000     92.600000   1037.400000  ...   \n",
      "\n",
      "           hg          sun    dos  g_rad   soil            pe   evap  smd_wd  \\\n",
      "count   30620  30620.00000  30620  30620  30620  30620.000000  30620   30620   \n",
      "unique     74          NaN     22   2767    984           NaN     76     795   \n",
      "top        19          NaN      0                         NaN    0.7           \n",
      "freq     1335          NaN  30428  12537   4322           NaN   1161   13814   \n",
      "mean      NaN      4.02096    NaN    NaN    NaN      1.508465    NaN     NaN   \n",
      "std       NaN      3.76516    NaN    NaN    NaN      1.003198    NaN     NaN   \n",
      "min       NaN      0.00000    NaN    NaN    NaN      0.000000    NaN     NaN   \n",
      "25%       NaN      0.50000    NaN    NaN    NaN      0.700000    NaN     NaN   \n",
      "50%       NaN      3.20000    NaN    NaN    NaN      1.300000    NaN     NaN   \n",
      "75%       NaN      6.50000    NaN    NaN    NaN      2.200000    NaN     NaN   \n",
      "max       NaN     15.90000    NaN    NaN    NaN      5.700000    NaN     NaN   \n",
      "\n",
      "       smd_md smd_pd  \n",
      "count   30620  30620  \n",
      "unique    896    934  \n",
      "top                   \n",
      "freq    13814  13814  \n",
      "mean      NaN    NaN  \n",
      "std       NaN    NaN  \n",
      "min       NaN    NaN  \n",
      "25%       NaN    NaN  \n",
      "50%       NaN    NaN  \n",
      "75%       NaN    NaN  \n",
      "max       NaN    NaN  \n",
      "\n",
      "[11 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‘ Step 4b â€“ Load and Inspect Dublin Airport Daily Data\n",
    "\n",
    "# --- Load CSV into pandas using detected header row ---\n",
    "df = pd.read_csv(\n",
    "    url,  # still using the online source\n",
    "    skiprows=header_index  # skip metadata lines before the header\n",
    ")\n",
    "\n",
    "# âœ… Inspect the first few rows\n",
    "print(\"First 5 rows of Dublin Airport Daily Data:\")\n",
    "display(df.head())\n",
    "\n",
    "# âœ… Check column types and missing values\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df.info())\n",
    "\n",
    "# âœ… Quick statistical summary\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292af427",
   "metadata": {},
   "source": [
    "### ğŸ“ Step 5 â€“ Save the Cleaned Dublin Airport Daily Data CSV\n",
    "\n",
    "After detecting the correct header row in the raw Met Ã‰ireann dataset, we now save a **cleaned version** of the Dublin Airport Daily Data file into the projectâ€™s `data/` folder.  \n",
    "\n",
    "This step ensures:\n",
    "\n",
    "- ğŸ“‚ The dataset is stored locally for reuse without needing to re-download from Met Ã‰ireann each time  \n",
    "- ğŸ“‘ All future analysis references a consistent, structured version of the data (starting at the correct header row)  \n",
    "- ğŸ”„ The workflow remains reproducible and version-controlled, supporting transparent project documentation  \n",
    "- ğŸ› ï¸ Analysts and reviewers can always work from the same baseline dataset, avoiding inconsistencies caused by raw file metadata  \n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "Saving cleaned data locally is a best practice in data science. It guarantees consistency across runs, makes collaboration easier, and allows you to track changes over time.  \n",
    "This approach supports reproducibility and version control in your Dublin Airport rerouting analysis.  \n",
    "\n",
    "ğŸ“– Reference:  \n",
    "- [GeeksforGeeks â€“ Explain Data Versioning](https://www.geeksforgeeks.org/machine-learning/explain-data-versioning/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fab2a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Saved cleaned climate data for Dublin Airport to: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\data\\dublin_airport_daily.csv\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ Step 5 â€“ Save the Cleaned CSV File\n",
    "\n",
    "# --- Ensure 'data' folder exists ---\n",
    "DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Save cleaned data starting from the detected header row ---\n",
    "with open(DATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines[header_index:]:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# âœ… Confirm save location\n",
    "print(f\"ğŸ“ Saved cleaned climate data for Dublin Airport to: {DATA_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe3644",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Step 6 â€“ Validate Saved CSV Against Stepâ€¯4 Output\n",
    "\n",
    "Instead of reâ€‘printing the same inspection results, this step **confirms that the locally saved CSV file is identical to the dataset inspected in Stepâ€¯4**.  \n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸ“‚ Reloading the locally saved CSV (`data/dublin_airport_daily.csv`)  \n",
    "- ğŸŒ Reloading the online CSV directly from Met Ã‰ireann (skipping metadata lines)  \n",
    "- âœ… Comparing the two DataFrames with `equals()` to check for exact match  \n",
    "- ğŸ“Š Printing a simple confirmation message and shape comparison  \n",
    "\n",
    "ğŸ“Œ *Why this matters:* This validation ensures reproducibility. It proves that the cleaned file saved in Stepâ€¯5 is a faithful copy of the dataset originally inspected in Stepâ€¯4.  \n",
    "Reviewers can trust that all downstream analysis is based on the same consistent dataset.\n",
    "\n",
    "[https://www.geeksforgeeks.org/create-effective-and-reproducible-code-using-pandas/](https://www.geeksforgeeks.org/create-effective-and-reproducible-code-using-pandas/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53874e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Validation successful: Local CSV matches the online dataset from Step 4.\n",
      "Local shape: (30620, 26), Online shape: (30620, 26)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ Step 6 â€“ Validate Saved CSV Against Step 4 Output\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Reload the locally saved CSV ---\n",
    "df_local = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# --- Reload the online CSV (using header_index from Step 4) ---\n",
    "df_online = pd.read_csv(url, skiprows=header_index)\n",
    "\n",
    "# âœ… Compare the two DataFrames\n",
    "if df_local.equals(df_online):\n",
    "    print(\"âœ… Validation successful: Local CSV matches the online dataset from Step 4.\")\n",
    "else:\n",
    "    print(\"âŒ Validation failed: Local CSV differs from the online dataset.\")\n",
    "\n",
    "# Optional: show shape comparison\n",
    "print(f\"Local shape: {df_local.shape}, Online shape: {df_online.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af503292",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 7 â€“ Download and Save Flight Activity Data\n",
    "\n",
    "In this step, the notebook retrieves and stores **flight activity data** for Dublin Airport.  \n",
    "This dataset will later be aligned with weather observations to analyse rerouting events.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸŒ Collecting flight activity logs from Dublin Airport dashboards or trackers (arrivals, departures, runway usage)  \n",
    "- ğŸ“‚ Defining a local output path (`data/dublin_airport_flights.csv`) to store the file inside the projectâ€™s `data` folder  \n",
    "- âœ… Checking the response to ensure the download or export was successful  \n",
    "- ğŸ“‘ Splitting the file into lines and detecting the header row (similar to weather data)  \n",
    "- ğŸ“ Saving a cleaned version of the dataset locally for reproducibility  \n",
    "\n",
    "Additionally, you will begin constructing an **annotated reroute CSV** (`data/reroutes.csv`) where you log rerouting events manually or semiâ€‘automatically.  \n",
    "This file should include:  \n",
    "- `date`, `time`, `flight_id`, `runway_before`, `runway_after`, `reason`  \n",
    "\n",
    "ğŸ“Œ *Why this matters:* Having both flight activity logs and reroute annotations stored locally ensures that your analysis can consistently align flight events with weather conditions.  \n",
    "It also supports reproducibility and version control across the project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
