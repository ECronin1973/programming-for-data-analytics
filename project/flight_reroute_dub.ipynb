{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae6fbf9",
   "metadata": {},
   "source": [
    "# Flight Punctuality Analysis at Dublin Airport\n",
    "\n",
    "This project examines how weather conditions influence flight punctuality at Dublin Airport.  \n",
    "The analysis combines flight activity data (arrivals, departures, delays, cancellations) with historical and forecast weather data from Met √âireann to identify trends, quantify the impact of adverse conditions, and project future delay probabilities. \n",
    " \n",
    "By aligning operational flight records with local weather observations, the study provides insights into how rain, wind, and visibility affect airport performance and passenger reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96525b",
   "metadata": {},
   "source": [
    "### Notebook Control Flag Explanation\n",
    "\n",
    "This notebook contains code to download flight history data from the Aviation Edge API.  \n",
    "Because downloading six months of data can take a long time and may stress the API, we use a **control flag** called `RUN_DOWNLOAD` to decide whether the download should run.\n",
    "\n",
    "- **RUN_DOWNLOAD = False** ‚Üí The download section is skipped.  \n",
    "  Use this setting when you want to run analysis, visualizations, or other notebook functionality without refreshing the data.\n",
    "\n",
    "- **RUN_DOWNLOAD = True** ‚Üí The download section executes.  \n",
    "  Use this setting only when you deliberately want to refresh the flight history data and update the cumulative JSON files.\n",
    "\n",
    "This design ensures:\n",
    "- The notebook can be safely re-run without triggering unwanted downloads.\n",
    "- Existing JSON files are preserved and can be loaded for analysis.\n",
    "- You have full control over when heavy API calls are made.\n",
    "\n",
    "üëâ In practice: keep `RUN_DOWNLOAD = False` most of the time, and flip it to `True` only when you need new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c9585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Control flag to enable/disable data refresh ---\n",
    "RUN_DOWNLOAD = False   # Change to True only when you want to refresh data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037f7b5",
   "metadata": {},
   "source": [
    "### üì¶ Step 2 ‚Äì Install and Import Required Libraries\n",
    "\n",
    "This step prepares the environment for the Dublin Airport Flight Rerouting Project.  \n",
    "It ensures that all required Python packages are available and sets up the project‚Äôs directory structure inside the `project` root.\n",
    "\n",
    "The notebook imports essential libraries for:\n",
    "\n",
    "- üìä **Data manipulation** (`pandas`, `numpy`)\n",
    "- üìÖ **Date and time handling** (`datetime`, `matplotlib.dates`)\n",
    "- üìà **Plotting and visualisation** (`matplotlib`, `seaborn`, `plotly`)\n",
    "- ü§ñ **Machine learning and model persistence** (`scikit-learn`, `joblib`)\n",
    "- üìÇ **File handling and paths** (`os`, `pathlib`, `json`)\n",
    "- üåê **Web access** (`requests`)\n",
    "- üß© **Interactivity and display** (`ipywidgets`, `IPython.display`)\n",
    "\n",
    "It also defines key directories (`data`, `outputs`, `models`, `docs`) inside the `project` folder and ensures they exist.  \n",
    "This structure keeps raw data, processed outputs, trained models, and documentation organised and reproducible.\n",
    "\n",
    "üìå *Note: `%pip install` commands can be used inside Jupyter notebooks if a package is missing.  \n",
    "For scripts or terminal use, run `pip install` directly.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86305781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Project root: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\n",
      "Data directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\data\n",
      "Output directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\outputs\n",
      "Model directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\models\n",
      "Docs directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\docs\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly --quiet\n",
    "\n",
    "# Setup: imports, paths and basic config\n",
    "\n",
    "# --- Core Python modules ---\n",
    "import json              # config files / JSON handling\n",
    "import os                # operating system interactions\n",
    "import time              # time management\n",
    "import warnings          # manage warnings\n",
    "from datetime import date, timedelta  # date calculations\n",
    "from pathlib import Path              # path management\n",
    "\n",
    "# --- Data science / numerical libraries ---\n",
    "import numpy as np       # numerical operations\n",
    "import pandas as pd      # data manipulation\n",
    "\n",
    "# --- Plotting libraries ---\n",
    "import matplotlib.pyplot as plt   # static plotting\n",
    "import plotly.express as px       # interactive plotting\n",
    "import seaborn as sns             # enhanced plotting\n",
    "\n",
    "# --- Machine learning libraries ---\n",
    "import joblib                     # model persistence\n",
    "from sklearn.ensemble import GradientBoostingClassifier   # example model\n",
    "from sklearn.linear_model import LogisticRegression       # example model\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score\n",
    ")  # model evaluation\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    train_test_split\n",
    ")  # model validation\n",
    "\n",
    "# --- API / external requests ---\n",
    "import requests                   # API calls\n",
    "\n",
    "# Plotting style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Explicit project root: programming-for-data-analytics/project\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name != \"project\":\n",
    "    # climb up until we find project folder\n",
    "    for parent in ROOT.parents:\n",
    "        if parent.name == \"project\":\n",
    "            ROOT = parent\n",
    "            break\n",
    "\n",
    "# Define key directories inside project\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\"\n",
    "MODEL_DIR = ROOT / \"models\"\n",
    "DOCS_DIR = ROOT / \"docs\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for path in [DATA_DIR, OUTPUT_DIR, MODEL_DIR, DOCS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Docs directory: {DOCS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468332e9",
   "metadata": {},
   "source": [
    "### Step 3 ‚Äì Utilise Helper Functions for Dublin Airport Data Processing\n",
    "\n",
    "This section defines a set of reusable helper functions that simplify common tasks in the project.  \n",
    "They are designed specifically to support the analysis of **Dublin Airport flight activity and weather data** by handling messy inputs and preparing clean datasets for exploration and modelling.\n",
    "\n",
    "The functions help with:\n",
    "\n",
    "- ‚úÖ Detecting and parsing inconsistent datetime formats in flight and weather logs  \n",
    "- ‚úÖ Standardising and cleaning temperature and precipitation columns from Met √âireann datasets  \n",
    "- ‚úÖ Loading and preparing Dublin Airport daily weather data from local CSV files  \n",
    "- ‚úÖ Defining Irish seasonal boundaries (Winter, Spring, Summer, Autumn) for comparative analysis  \n",
    "- ‚úÖ Filtering weather data for a custom date range to align with flight events  \n",
    "- ‚úÖ Validating user-provided date inputs for reproducible analysis  \n",
    "- ‚úÖ Detecting header rows in raw CSV files downloaded from dashboards  \n",
    "\n",
    "Each helper is **modular** ‚Äî it performs one clear task and can be reused across notebooks and scripts.  \n",
    "This improves readability, reduces duplication, and supports good programming practices for the final project.\n",
    "\n",
    "üìå *Tip: These helpers are written to be beginner-friendly, with comments explaining their purpose and logic. They make it easier to align flight activity with weather conditions when investigating delays and cancellations.*\n",
    "\n",
    "üìñ References:  \n",
    "- [Real Python ‚Äì Python Helper Functions](https://realpython.com/defining-your-own-python-function/)  \n",
    "- [GeeksforGeeks ‚Äì Python Helper Functions](https://www.geeksforgeeks.org/python-helper-functions/)  \n",
    "- [Wikipedia ‚Äì DRY Principle (Don't Repeat Yourself)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607be6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Helper Functions for Dublin Airport Project\n",
    "# These functions handle parsing dates, cleaning weather data, preparing ranges,\n",
    "# defining Irish seasons, and detecting CSV headers.\n",
    "# Keep them in one cell so they are easy to reuse across the notebook.\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from calendar import monthrange\n",
    "\n",
    "# üìÖ Detect the most likely datetime format from sample strings\n",
    "def detect_datetime_format(samples, formats, dayfirst=True, min_match_ratio=0.7, min_absolute=5):\n",
    "    \"\"\"\n",
    "    Try each format and return the one that matches at least 70% of samples\n",
    "    or at least 'min_absolute' matches. Helps ensure consistent parsing of date strings.\n",
    "    \"\"\"\n",
    "    for fmt in formats:\n",
    "        parsed = pd.to_datetime(samples, format=fmt, dayfirst=dayfirst, errors='coerce')\n",
    "        matches = parsed.notna().sum()\n",
    "        if matches >= max(min_absolute, int(len(samples) * min_match_ratio)):\n",
    "            return fmt\n",
    "    return None\n",
    "\n",
    "# üìÖ Parse a datetime column using format detection or fallback\n",
    "def parse_datetime_column(df, date_col, candidate_formats=None, dayfirst=True):\n",
    "    \"\"\"\n",
    "    Parse a datetime column using known formats.\n",
    "    Falls back to flexible parsing if none match.\n",
    "    \"\"\"\n",
    "    if candidate_formats is None:\n",
    "        candidate_formats = [\n",
    "            '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%d-%b-%Y %H:%M',\n",
    "            '%d/%m/%Y %H:%M:%S', '%d/%m/%Y %H:%M', '%d-%m-%Y %H:%M',\n",
    "            '%d %b %Y %H:%M', '%d %B %Y %H:%M',\n",
    "        ]\n",
    "\n",
    "    sample_vals = df[date_col].dropna().astype(str).head(100).tolist()\n",
    "    chosen_fmt = detect_datetime_format(sample_vals, candidate_formats, dayfirst=dayfirst)\n",
    "\n",
    "    if chosen_fmt:\n",
    "        print(f\"‚úÖ Detected datetime format: {chosen_fmt}\")\n",
    "        return pd.to_datetime(df[date_col], format=chosen_fmt, dayfirst=dayfirst, errors='coerce')\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No single format matched. Falling back to flexible parsing.\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', message='Could not infer format')\n",
    "            return pd.to_datetime(df[date_col], dayfirst=dayfirst, errors='coerce')\n",
    "\n",
    "# üå°Ô∏è Ensure temperature column is numeric and named 'temp'\n",
    "def parse_temperature_column(df, col_name='temp'):\n",
    "    \"\"\"\n",
    "    Convert the temperature column to numeric and rename it to 'temp'.\n",
    "    If no exact match, look for any column containing 'temp'.\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        col_name = next((c for c in df.columns if 'temp' in c.lower()), None)\n",
    "        if col_name is None:\n",
    "            raise KeyError(\"No temperature column found.\")\n",
    "    if 'temp' in df.columns and col_name != 'temp':\n",
    "        df.rename(columns={col_name: 'temp'}, inplace=True)\n",
    "    else:\n",
    "        df['temp'] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# üìÇ Load cleaned weather data from local CSV\n",
    "def load_cleaned_weather_data(filepath=\"data/dublin_airport_daily.csv\"):\n",
    "    \"\"\"\n",
    "    Load weather dataset from CSV and strip spaces from column names.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "# üçÇ Define Irish seasonal boundaries for a given year (leap year safe)\n",
    "def define_irish_seasons(year=2025):\n",
    "    \"\"\"\n",
    "    Return start and end dates for Irish meteorological seasons.\n",
    "    Handles leap years correctly for February.\n",
    "    \"\"\"\n",
    "    feb_days = monthrange(year, 2)[1]  # 28 or 29\n",
    "    data = [\n",
    "        (\"Winter\", pd.Timestamp(f\"{year-1}-12-01\"), pd.Timestamp(f\"{year}-02-{feb_days} 23:59\")),\n",
    "        (\"Spring\", pd.Timestamp(f\"{year}-03-01\"), pd.Timestamp(f\"{year}-05-31 23:59\")),\n",
    "        (\"Summer\", pd.Timestamp(f\"{year}-06-01\"), pd.Timestamp(f\"{year}-08-31 23:59\")),\n",
    "        (\"Autumn\", pd.Timestamp(f\"{year}-09-01\"), pd.Timestamp(f\"{year}-11-30 23:59\")),\n",
    "    ]\n",
    "    return pd.DataFrame(data, columns=[\"season\", \"start\", \"end\"])\n",
    "\n",
    "# üìä Filter and prepare weather data for a custom date range\n",
    "def prepare_weather_data(df, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Filter weather data to a date range and add useful time features.\n",
    "    Handles separate 'date' and 'time' columns if present.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    if 'date' not in df.columns:\n",
    "        raise KeyError(\"Expected 'date' column not found.\")\n",
    "\n",
    "    if 'time' in df.columns:\n",
    "        dt_strings = df['date'].astype(str) + \" \" + df['time'].astype(str)\n",
    "        df['datetime'] = pd.to_datetime(dt_strings, errors='coerce', dayfirst=True)\n",
    "    else:\n",
    "        df['datetime'] = pd.to_datetime(df['date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "    df = df.dropna(subset=['datetime'])\n",
    "    mask = (df['datetime'] >= pd.to_datetime(start_date)) & (df['datetime'] <= pd.to_datetime(end_date))\n",
    "    range_df = df.loc[mask].copy()\n",
    "\n",
    "    # Add date and hour columns for plotting\n",
    "    range_df['date'] = range_df['datetime'].dt.date\n",
    "    range_df['hour'] = range_df['datetime'].dt.strftime('%H:%M')\n",
    "\n",
    "    return range_df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# üìÜ Convert user input strings into a validated date range\n",
    "def get_custom_range(start_str, end_str):\n",
    "    \"\"\"\n",
    "    Convert string inputs into datetime objects and validate order.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start = pd.to_datetime(start_str, dayfirst=True)\n",
    "        end = pd.to_datetime(end_str, dayfirst=True)\n",
    "        if start > end:\n",
    "            raise ValueError(\"Start date must be before end date.\")\n",
    "        return start, end\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Invalid date range: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# üîç Detect the header row in a CSV file\n",
    "def detect_header(lines, keywords=(\"station\",\"date\",\"rain\",\"temp\",\"wind\")):\n",
    "    \"\"\"\n",
    "    Detect the most likely header row in a CSV file.\n",
    "    Looks for lines containing known weather keywords and multiple columns.\n",
    "    \"\"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        line_lower = line.strip().lower()\n",
    "        if any(line_lower.startswith(k) for k in keywords) and \",\" in line:\n",
    "            columns = line.split(\",\")\n",
    "            if len(columns) > 3:  # header rows usually have multiple columns\n",
    "                return i\n",
    "    print(\"‚ö†Ô∏è Warning: header row not found. Defaulting to first line.\")\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecf4c3",
   "metadata": {},
   "source": [
    "### üìÇ Step 4 ‚Äì Download Dublin Airport Daily Data and Detect Header Row\n",
    "\n",
    "In this step, the notebook retrieves the **Dublin Airport Daily Data CSV** directly from Met √âireann‚Äôs open data service.  \n",
    "This dataset contains daily weather observations (e.g., precipitation, temperature, wind speed, radiation) recorded at Dublin Airport, which will later be aligned with flight activity logs to analyse rerouting events.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üåê **Downloading the raw CSV** from Met √âireann using the `requests` library.  \n",
    "- üìÇ **Defining a local output path** (`data/dublin_airport_daily.csv`) to store the file inside the project‚Äôs `data` folder.  \n",
    "- ‚úÖ **Checking the HTTP response** to ensure the download was successful.  \n",
    "- üìë **Splitting the file into lines** so the structure can be inspected before loading into pandas.  \n",
    "- üîç **Detecting the header row** using the `detect_header` helper function defined earlier.  \n",
    "  This ensures that column names (such as `date`, `maxtp`, `mintp`, `rain`, `wdsp`) are correctly identified even if the file contains metadata lines at the top.  \n",
    "- üñ®Ô∏è **Printing the detected header row** to confirm the correct starting point for parsing.\n",
    "\n",
    "üìå *Tip: Detecting the header row is important because Met √âireann CSVs often include metadata lines before the actual data table.  \n",
    "By confirming the header row, you avoid misaligned columns and ensure clean parsing in later steps.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cc44a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Header row detected at line 25:\n",
      "date,ind,maxtp,ind,mintp,igmin,gmin,ind,rain,cbl,wdsp,ind,hm,ind,ddhm,ind,hg,sun,dos,g_rad,soil,pe,evap,smd_wd,smd_md,smd_pd\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 4 ‚Äì Download Dublin Airport Daily Data CSV and Detect Header Row\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# --- Define output path for cleaned CSV ---\n",
    "DATA_PATH = Path(\"data/dublin_airport_daily.csv\")\n",
    "\n",
    "# --- Download raw CSV from Met √âireann (Dublin Airport Daily Data) ---\n",
    "url = \"https://cli.fusio.net/cli/climate_data/webdata/dly532.csv\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# ‚úÖ Check for successful response\n",
    "if response.status_code != 200:\n",
    "    raise RuntimeError(f\"‚ùå Failed to download data: HTTP {response.status_code}\")\n",
    "\n",
    "# --- Split response into lines ---\n",
    "lines = response.text.splitlines()\n",
    "\n",
    "# --- Detect header row using helper function ---\n",
    "header_index = detect_header(lines)\n",
    "\n",
    "# ‚úÖ Confirm detected header row\n",
    "print(f\"‚úÖ Header row detected at line {header_index}:\")\n",
    "print(lines[header_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f4588",
   "metadata": {},
   "source": [
    "### üìë Step 4b ‚Äì Load and Inspect Dublin Airport Daily Data\n",
    "\n",
    "After detecting the correct header row in the raw CSV file, the next step is to **load the dataset into pandas**.  \n",
    "This allows us to immediately inspect the structure of the Dublin Airport Daily Data and confirm that the columns (e.g., `date`, `maxtp`, `mintp`, `rain`, `wdsp`) are correctly aligned.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üìÇ Reading the CSV into a pandas DataFrame, starting from the detected header row  \n",
    "- üîç Displaying the first few rows with `head()` to verify column names and sample values  \n",
    "- üßæ Using `info()` to check datatypes and identify potential missing values  \n",
    "- üìä Summarising numeric columns with `describe()` to get a quick statistical overview  \n",
    "\n",
    "üìå *Why this matters:* Inspecting the dataset before saving ensures that the header detection worked correctly and that the file is ready for consistent downstream analysis.  \n",
    "This step acts as a validation checkpoint before committing the cleaned file to the `data/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c71e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of Dublin Airport Daily Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ind</th>\n",
       "      <th>maxtp</th>\n",
       "      <th>ind.1</th>\n",
       "      <th>mintp</th>\n",
       "      <th>igmin</th>\n",
       "      <th>gmin</th>\n",
       "      <th>ind.2</th>\n",
       "      <th>rain</th>\n",
       "      <th>cbl</th>\n",
       "      <th>...</th>\n",
       "      <th>hg</th>\n",
       "      <th>sun</th>\n",
       "      <th>dos</th>\n",
       "      <th>g_rad</th>\n",
       "      <th>soil</th>\n",
       "      <th>pe</th>\n",
       "      <th>evap</th>\n",
       "      <th>smd_wd</th>\n",
       "      <th>smd_md</th>\n",
       "      <th>smd_pd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1020.3</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1016.2</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1001.5</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05-jan-1942</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1013.4</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>3.4</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  ind  maxtp  ind.1  mintp  igmin gmin  ind.2  rain     cbl  \\\n",
       "0  01-jan-1942    0    9.7      0    6.8      0  4.7      2   0.0  1020.3   \n",
       "1  02-jan-1942    0    9.9      0    7.9      0  6.7      0   0.1  1016.2   \n",
       "2  03-jan-1942    0   11.2      0    8.9      0  7.2      0   1.5  1006.8   \n",
       "3  04-jan-1942    0    9.2      0    2.7      0  3.4      0   3.5  1001.5   \n",
       "4  05-jan-1942    0    3.5      1   -0.8      0  0.0      0   0.6  1013.4   \n",
       "\n",
       "   ...  hg  sun dos  g_rad soil   pe evap  smd_wd smd_md smd_pd  \n",
       "0  ...      0.0   0              1.1  1.4                        \n",
       "1  ...      0.0   0              0.7  0.9                        \n",
       "2  ...      0.1   0              0.5  0.6                        \n",
       "3  ...      0.6   0              0.6  0.7                        \n",
       "4  ...      3.4   0              0.6  0.7                        \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30620 entries, 0 to 30619\n",
      "Data columns (total 26 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   date    30620 non-null  object \n",
      " 1   ind     30620 non-null  int64  \n",
      " 2   maxtp   30620 non-null  float64\n",
      " 3   ind.1   30620 non-null  int64  \n",
      " 4   mintp   30620 non-null  float64\n",
      " 5   igmin   30620 non-null  int64  \n",
      " 6   gmin    30620 non-null  object \n",
      " 7   ind.2   30620 non-null  int64  \n",
      " 8   rain    30620 non-null  float64\n",
      " 9   cbl     30620 non-null  float64\n",
      " 10  wdsp    30620 non-null  float64\n",
      " 11  ind.3   30620 non-null  int64  \n",
      " 12  hm      30620 non-null  object \n",
      " 13  ind.4   30620 non-null  int64  \n",
      " 14  ddhm    30620 non-null  object \n",
      " 15  ind.5   30620 non-null  int64  \n",
      " 16  hg      30620 non-null  object \n",
      " 17  sun     30620 non-null  float64\n",
      " 18  dos     30620 non-null  object \n",
      " 19  g_rad   30620 non-null  object \n",
      " 20  soil    30620 non-null  object \n",
      " 21  pe      30620 non-null  float64\n",
      " 22  evap    30620 non-null  object \n",
      " 23  smd_wd  30620 non-null  object \n",
      " 24  smd_md  30620 non-null  object \n",
      " 25  smd_pd  30620 non-null  object \n",
      "dtypes: float64(7), int64(7), object(12)\n",
      "memory usage: 6.1+ MB\n",
      "None\n",
      "\n",
      "Summary statistics:\n",
      "               date           ind         maxtp         ind.1         mintp  \\\n",
      "count         30620  30620.000000  30620.000000  30620.000000  30620.000000   \n",
      "unique        30620           NaN           NaN           NaN           NaN   \n",
      "top     01-jan-1942           NaN           NaN           NaN           NaN   \n",
      "freq              1           NaN           NaN           NaN           NaN   \n",
      "mean            NaN      0.001078     13.089902      0.083377      6.169069   \n",
      "std             NaN      0.033792      4.911569      0.276574      4.385220   \n",
      "min             NaN      0.000000     -4.700000      0.000000    -12.200000   \n",
      "25%             NaN      0.000000      9.500000      0.000000      2.900000   \n",
      "50%             NaN      0.000000     13.000000      0.000000      6.300000   \n",
      "75%             NaN      0.000000     16.900000      0.000000      9.600000   \n",
      "max             NaN      2.000000     29.100000      2.000000     18.400000   \n",
      "\n",
      "               igmin   gmin         ind.2          rain           cbl  ...  \\\n",
      "count   30620.000000  30620  30620.000000  30620.000000  30620.000000  ...   \n",
      "unique           NaN    301           NaN           NaN           NaN  ...   \n",
      "top              NaN    6.1           NaN           NaN           NaN  ...   \n",
      "freq             NaN    272           NaN           NaN           NaN  ...   \n",
      "mean        0.217799    NaN      0.885565      2.072227   1003.497169  ...   \n",
      "std         0.416302    NaN      1.232408      4.393421     11.732616  ...   \n",
      "min         0.000000    NaN      0.000000      0.000000    949.600000  ...   \n",
      "25%         0.000000    NaN      0.000000      0.000000    996.200000  ...   \n",
      "50%         0.000000    NaN      0.000000      0.200000   1004.500000  ...   \n",
      "75%         0.000000    NaN      2.000000      2.200000   1011.700000  ...   \n",
      "max         4.000000    NaN      4.000000     92.600000   1037.400000  ...   \n",
      "\n",
      "           hg          sun    dos  g_rad   soil            pe   evap  smd_wd  \\\n",
      "count   30620  30620.00000  30620  30620  30620  30620.000000  30620   30620   \n",
      "unique     74          NaN     22   2767    984           NaN     76     795   \n",
      "top        19          NaN      0                         NaN    0.7           \n",
      "freq     1335          NaN  30428  12537   4322           NaN   1161   13814   \n",
      "mean      NaN      4.02096    NaN    NaN    NaN      1.508465    NaN     NaN   \n",
      "std       NaN      3.76516    NaN    NaN    NaN      1.003198    NaN     NaN   \n",
      "min       NaN      0.00000    NaN    NaN    NaN      0.000000    NaN     NaN   \n",
      "25%       NaN      0.50000    NaN    NaN    NaN      0.700000    NaN     NaN   \n",
      "50%       NaN      3.20000    NaN    NaN    NaN      1.300000    NaN     NaN   \n",
      "75%       NaN      6.50000    NaN    NaN    NaN      2.200000    NaN     NaN   \n",
      "max       NaN     15.90000    NaN    NaN    NaN      5.700000    NaN     NaN   \n",
      "\n",
      "       smd_md smd_pd  \n",
      "count   30620  30620  \n",
      "unique    896    934  \n",
      "top                   \n",
      "freq    13814  13814  \n",
      "mean      NaN    NaN  \n",
      "std       NaN    NaN  \n",
      "min       NaN    NaN  \n",
      "25%       NaN    NaN  \n",
      "50%       NaN    NaN  \n",
      "75%       NaN    NaN  \n",
      "max       NaN    NaN  \n",
      "\n",
      "[11 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# üìë Step 4b ‚Äì Load and Inspect Dublin Airport Daily Data\n",
    "\n",
    "# --- Load CSV into pandas using detected header row ---\n",
    "df = pd.read_csv(\n",
    "    url,  # still using the online source\n",
    "    skiprows=header_index  # skip metadata lines before the header\n",
    ")\n",
    "\n",
    "# ‚úÖ Inspect the first few rows\n",
    "print(\"First 5 rows of Dublin Airport Daily Data:\")\n",
    "display(df.head())\n",
    "\n",
    "# ‚úÖ Check column types and missing values\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df.info())\n",
    "\n",
    "# ‚úÖ Quick statistical summary\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292af427",
   "metadata": {},
   "source": [
    "### üìÅ Step 5 ‚Äì Save the Cleaned Dublin Airport Daily Data CSV\n",
    "\n",
    "After detecting the correct header row in the raw Met √âireann dataset, we now save a **cleaned version** of the Dublin Airport Daily Data file into the project‚Äôs `data/` folder.  \n",
    "\n",
    "This step ensures:\n",
    "\n",
    "- üìÇ The dataset is stored locally for reuse without needing to re-download from Met √âireann each time  \n",
    "- üìë All future analysis references a consistent, structured version of the data (starting at the correct header row)  \n",
    "- üîÑ The workflow remains reproducible and version-controlled, supporting transparent project documentation  \n",
    "- üõ†Ô∏è Analysts and reviewers can always work from the same baseline dataset, avoiding inconsistencies caused by raw file metadata  \n",
    "\n",
    "üìå *Why this matters:*  \n",
    "Saving cleaned data locally is a best practice in data science. It guarantees consistency across runs, makes collaboration easier, and allows you to track changes over time.  \n",
    "This approach supports reproducibility and version control in your Dublin Airport rerouting analysis.  \n",
    "\n",
    "üìñ Reference:  \n",
    "- [GeeksforGeeks ‚Äì Explain Data Versioning](https://www.geeksforgeeks.org/machine-learning/explain-data-versioning/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fab2a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Saved cleaned climate data for Dublin Airport to: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\data\\dublin_airport_daily.csv\n"
     ]
    }
   ],
   "source": [
    "# üìÅ Step 5 ‚Äì Save the Cleaned CSV File\n",
    "\n",
    "# --- Ensure 'data' folder exists ---\n",
    "DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Save cleaned data starting from the detected header row ---\n",
    "with open(DATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines[header_index:]:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# ‚úÖ Confirm save location\n",
    "print(f\"üìÅ Saved cleaned climate data for Dublin Airport to: {DATA_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe3644",
   "metadata": {},
   "source": [
    "### üìÇ Step 6 ‚Äì Validate Saved CSV Against Step‚ÄØ4 Output\n",
    "\n",
    "Instead of re‚Äëprinting the same inspection results, this step **confirms that the locally saved CSV file is identical to the dataset inspected in Step‚ÄØ4**.  \n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üìÇ Reloading the locally saved CSV (`data/dublin_airport_daily.csv`)  \n",
    "- üåê Reloading the online CSV directly from Met √âireann (skipping metadata lines)  \n",
    "- ‚úÖ Comparing the two DataFrames with `equals()` to check for exact match  \n",
    "- üìä Printing a simple confirmation message and shape comparison  \n",
    "\n",
    "üìå *Why this matters:* This validation ensures reproducibility. It proves that the cleaned file saved in Step‚ÄØ5 is a faithful copy of the dataset originally inspected in Step‚ÄØ4.  \n",
    "Reviewers can trust that all downstream analysis is based on the same consistent dataset.\n",
    "\n",
    "[https://www.geeksforgeeks.org/create-effective-and-reproducible-code-using-pandas/](https://www.geeksforgeeks.org/create-effective-and-reproducible-code-using-pandas/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53874e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation successful: Local CSV matches the online dataset from Step 4.\n",
      "Local shape: (30620, 26), Online shape: (30620, 26)\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 6 ‚Äì Validate Saved CSV Against Step 4 Output\n",
    "\n",
    "# --- Reload the locally saved CSV ---\n",
    "df_local = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# --- Reload the online CSV (using header_index from Step 4) ---\n",
    "df_online = pd.read_csv(url, skiprows=header_index)\n",
    "\n",
    "# ‚úÖ Compare the two DataFrames\n",
    "if df_local.equals(df_online):\n",
    "    print(\"‚úÖ Validation successful: Local CSV matches the online dataset from Step 4.\")\n",
    "else:\n",
    "    print(\"‚ùå Validation failed: Local CSV differs from the online dataset.\")\n",
    "\n",
    "# Optional: show shape comparison\n",
    "print(f\"Local shape: {df_local.shape}, Online shape: {df_online.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af503292",
   "metadata": {},
   "source": [
    "### üìë Step 7 ‚Äì Download and Save Flight Activity Data\n",
    "\n",
    "In this step, the notebook retrieves and stores **flight activity data** for Dublin Airport.  \n",
    "This dataset will later be aligned with Met √âireann weather observations to analyse how conditions such as rain, wind, and visibility impact flight punctuality.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üåê Collecting flight schedules and activity logs (arrivals, departures, delays, cancellations) from public APIs or dashboards  \n",
    "- üìÇ Defining a local output path (`data/dublin_airport_flights.csv`) to store the file inside the project‚Äôs `data` folder  \n",
    "- ‚úÖ Checking the response to ensure the download or export was successful  \n",
    "- üìë Parsing the raw data into a structured format, including scheduled vs actual times and delay minutes  \n",
    "- üìÅ Saving a cleaned version of the dataset locally for reproducibility and future analysis  \n",
    "\n",
    "üìå *Why this matters:* Having flight activity data stored locally ensures that the project can consistently align flight events with weather conditions.  \n",
    "It also supports reproducibility, version control, and enables predictive modelling of delays and cancellations without repeatedly querying external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d600a9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-05-20 to 2025-11-18\n"
     ]
    }
   ],
   "source": [
    "# --- Compute date range for the past six months ---\n",
    "today = date.today()\n",
    "six_months_ago = today - timedelta(days=182)  # approx 6 months\n",
    "\n",
    "DATE_FROM = six_months_ago.isoformat()\n",
    "DATE_TO = today.isoformat()\n",
    "\n",
    "# --- Output directories ---\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DIR = DATA_DIR / \"raw_flights\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Date range: {DATE_FROM} to {DATE_TO}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79b275",
   "metadata": {},
   "source": [
    "### üìë Step 8 ‚Äì Dublin Airport flight information analysis \n",
    "\n",
    "This cell prepares the environment for **Dublin Airport flight information analysis** by defining key date ranges and output directories:\n",
    "\n",
    "- üóìÔ∏è **Date range:**  \n",
    "  - Calculates today‚Äôs date and subtracts ~six months (182 days) to define the analysis window.  \n",
    "  - Converts both dates into ISO format (`YYYY-MM-DD`) for use in API queries.  \n",
    "  - These values (`DATE_FROM`, `DATE_TO`) specify the six‚Äëmonth period of **flight activity data** (arrivals, departures, delays, cancellations) to be downloaded.\n",
    "\n",
    "- üìÇ **Output directories:**  \n",
    "  - Creates a root `data/` folder for project storage.  \n",
    "  - Inside it, a `raw_flights/` subfolder is created to hold raw JSON files retrieved from the Aviation Edge API.  \n",
    "  - This ensures reproducibility and a clear separation between raw flight inputs and processed datasets.\n",
    "\n",
    "- ‚úÖ **Checkpoint:**  \n",
    "  - Prints the computed date range so you can confirm the correct six‚Äëmonth window before downloading flight information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a82153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-05-20 to 2025-11-18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Compute date range for the past six months ---\n",
    "today = date.today()\n",
    "six_months_ago = today - timedelta(days=182)  # approx 6 months\n",
    "\n",
    "DATE_FROM = six_months_ago.isoformat()\n",
    "DATE_TO = today.isoformat()\n",
    "\n",
    "# --- Output directories ---\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DIR = DATA_DIR / \"raw_flights\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Date range: {DATE_FROM} to {DATE_TO}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb27b0",
   "metadata": {},
   "source": [
    "### ‚úàÔ∏è Step 9 ‚Äî Download Six Months of Flight History for Dublin (Arrivals and Departures)\n",
    "\n",
    "In this step we use **Aviation Edge‚Äôs Flights History API** to collect six months of flight schedules for Dublin Airport (IATA: DUB).  \n",
    "The endpoint provides detailed records for each flight, including:\n",
    "\n",
    "- **Scheduled, estimated, and actual times** (departure and arrival)\n",
    "- **Delay minutes** (either reported or inferred)\n",
    "- **Flight status** (e.g., scheduled, landed, cancelled, diverted)\n",
    "- **Airline and flight identifiers**\n",
    "\n",
    "We request **both arrivals and departures** for the date range **2025‚Äë05‚Äë20 to 2025‚Äë11‚Äë18**, ensuring coverage of the most recent six months.  \n",
    "The raw JSON files are saved for reproducibility in the folder:\n",
    "\n",
    "- `data/raw_flights/dub_arrival_history.json`  \n",
    "- `data/raw_flights/dub_departure_history.json`\n",
    "\n",
    "Additionally, a `fetch_log.txt` file is generated to record progress, errors, and confirmation of successful downloads.  \n",
    "This log provides transparency and makes troubleshooting easier if API requests fail or return incomplete data.\n",
    "\n",
    "**Important notes for reproducibility:**\n",
    "- The code cell was executed on **18 November 2025** using a private API key from Aviation Edge.\n",
    "- To run the download yourself, you must:\n",
    "  1. Sign up for an account at [aviation-edge.com](https://aviation-edge.com/) and obtain an API key.\n",
    "  2. Store the key securely (e.g., as an environment variable).\n",
    "  3. Set the notebook control flag `RUN_DOWNLOAD = True` to enable downloading.\n",
    "- By default, the notebook will skip downloading if `RUN_DOWNLOAD = False`, and instead use the existing JSON files.  \n",
    "  This prevents unnecessary API calls and ensures consistent results for reviewers.\n",
    "\n",
    "‚ö†Ô∏è **Best practice:** Only re‚Äërun the download when you want to refresh the dataset.  \n",
    "Frequent downloads are unnecessary and may exceed API rate limits.\n",
    "\n",
    "**References:**\n",
    "- [Aviation Edge official site](https://aviation-edge.com/)  \n",
    "- [Aviation Edge API documentation on GitHub](https://github.com/AviationEdgeAPI/Aviation-Edge-Complete-API)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c68251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è© Skipping download step (RUN_DOWNLOAD=False). Using existing JSON files.\n"
     ]
    }
   ],
   "source": [
    "# --- API setup ---\n",
    "API_KEY = os.getenv(\"AVIATION_EDGE_API_KEY\")   # Read API key from environment variable\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"API key not found. Please set AVIATION_EDGE_API_KEY.\")\n",
    "\n",
    "BASE_URL = \"https://aviation-edge.com/v2/public/flightsHistory\"  # Endpoint for flight history\n",
    "IATA_CODE = \"DUB\"  # Airport code for Dublin\n",
    "\n",
    "# --- Directory setup ---\n",
    "DATA_DIR = Path(\"data\")              # Root data folder\n",
    "RAW_DIR = DATA_DIR / \"raw_flights\"   # Subfolder for raw flight data\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)  # Create folders if missing\n",
    "\n",
    "# --- Log file path ---\n",
    "LOG_FILE = RAW_DIR / \"fetch_log.txt\"  # Text log for progress and errors\n",
    "\n",
    "def log_message(message: str):\n",
    "    \"\"\"Print message and append to log file for tracking progress.\"\"\"\n",
    "    print(message)\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(message + \"\\n\")\n",
    "\n",
    "def fetch_day(iata_code: str, flight_type: str, day: date, retries: int = 3):\n",
    "    \"\"\"\n",
    "    Fetch flight history for a single day (arrival/departure).\n",
    "    Retries up to 'retries' times if errors occur.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"key\": API_KEY,\n",
    "        \"code\": iata_code,\n",
    "        \"type\": flight_type,\n",
    "        \"date_from\": day.isoformat(),\n",
    "        \"date_to\": day.isoformat()\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        resp = requests.get(BASE_URL, params=params, timeout=60)  # API request\n",
    "        if resp.status_code == 200:\n",
    "            try:\n",
    "                data = resp.json()  # Parse JSON response\n",
    "                log_message(f\"‚úÖ {flight_type.capitalize()} {day}: {len(data)} records fetched\")\n",
    "                return data\n",
    "            except Exception:\n",
    "                log_message(f\"‚ö†Ô∏è Non-JSON response on {day}: {resp.text[:200]}\")\n",
    "                return []\n",
    "        else:\n",
    "            wait = 2 ** attempt  # Exponential backoff\n",
    "            log_message(f\"‚ö†Ô∏è Error {resp.status_code} on {day} (attempt {attempt+1}/{retries}). Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    log_message(f\"‚ùå Failed after {retries} retries on {day}\")\n",
    "    return []\n",
    "\n",
    "def fetch_history(iata_code: str, flight_type: str, start_date: date, end_date: date):\n",
    "    \"\"\"\n",
    "    Loop through each day in the date range and fetch daily history.\n",
    "    Append results into one cumulative JSON file (avoids overwriting with empty data).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_days = (end_date - start_date).days + 1\n",
    "    filename = RAW_DIR / f\"{iata_code.lower()}_{flight_type}_history.json\"\n",
    "\n",
    "    # Load existing cumulative file if present\n",
    "    if filename.exists():\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                results = json.load(f)\n",
    "            except Exception:\n",
    "                results = []\n",
    "                log_message(f\"‚ö†Ô∏è Existing {filename.name} could not be read, starting fresh.\")\n",
    "\n",
    "    # Loop through each day in range\n",
    "    for i in range(total_days):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        log_message(f\"Day {i+1}/{total_days}: {day}\")\n",
    "        day_data = fetch_day(iata_code, flight_type, day)\n",
    "\n",
    "        # Save only if data was fetched\n",
    "        if day_data:\n",
    "            results.extend(day_data)\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            log_message(f\"üíæ Saved {len(day_data)} records for {day} into {filename.name}\")\n",
    "        else:\n",
    "            log_message(f\"‚è© Skipped saving {day}, no data returned\")\n",
    "\n",
    "        time.sleep(1)  # Pause politely between requests\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Conditional download control ---\n",
    "if RUN_DOWNLOAD:\n",
    "    # Define date range (last ~6 months)\n",
    "    today = date.today()\n",
    "    start_date = today - timedelta(days=182)\n",
    "    end_date = today\n",
    "\n",
    "    log_message(f\"Fetching flights from {start_date} to {end_date} for {IATA_CODE}...\")\n",
    "\n",
    "    # Fetch arrivals and departures\n",
    "    arrivals = fetch_history(IATA_CODE, \"arrival\", start_date, end_date)\n",
    "    departures = fetch_history(IATA_CODE, \"departure\", start_date, end_date)\n",
    "\n",
    "    log_message(f\"‚úÖ Completed: {len(arrivals)} arrivals and {len(departures)} departures fetched.\")\n",
    "else:\n",
    "    log_message(\"‚è© Skipping download step (RUN_DOWNLOAD=False). Using existing JSON files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a966b",
   "metadata": {},
   "source": [
    "### üìÇ Step 10 ‚Äì Inspect Headings of Downloaded JSON Files\n",
    "\n",
    "Before tidying the flight history data, it‚Äôs important to **inspect the structure of the raw JSON files**.  \n",
    "The Aviation Edge API responses can vary depending on whether the file contains arrivals or departures, and not all fields are always present.\n",
    "\n",
    "**What this step does:**\n",
    "- Loads the raw JSON files for Dublin Airport arrivals (`dub_arrival_history.json`) and departures (`dub_departure_history.json`).\n",
    "- Uses `pandas.json_normalize` to flatten the nested JSON into a tabular structure.\n",
    "- Prints out all available column headings so we can see which fields exist.\n",
    "- Shows a sample record (truncated for readability) to preview the nested structure.\n",
    "\n",
    "**Why this matters:**\n",
    "- Helps identify which fields are consistently available and relevant for analysis.\n",
    "- Prevents errors later by ensuring we only select columns that actually exist.\n",
    "- Guides the design of the tidy DataFrame schema in Step‚ÄØ11 (e.g. keeping scheduled/actual times, delays, status, airline, etc., while dropping baggage or codeshare metadata).\n",
    "\n",
    "üëâ This inspection step is a diagnostic tool: it gives us visibility into the raw data so we can confidently build the parsing logic in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ff4cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Keys in dub_arrival_history.json ---\n",
      "['airline.iataCode', 'airline.icaoCode', 'airline.name', 'arrival.actualRunway', 'arrival.actualTime', 'arrival.baggage', 'arrival.delay', 'arrival.estimatedRunway', 'arrival.estimatedTime', 'arrival.gate', 'arrival.iataCode', 'arrival.icaoCode', 'arrival.scheduledTime', 'arrival.terminal', 'codeshared.airline.iataCode', 'codeshared.airline.icaoCode', 'codeshared.airline.name', 'codeshared.flight.iataNumber', 'codeshared.flight.icaoNumber', 'codeshared.flight.number', 'departure.actualRunway', 'departure.actualTime', 'departure.delay', 'departure.estimatedRunway', 'departure.estimatedTime', 'departure.gate', 'departure.iataCode', 'departure.icaoCode', 'departure.scheduledTime', 'departure.terminal', 'flight.iataNumber', 'flight.icaoNumber', 'flight.number', 'status', 'type']\n",
      "\n",
      "Sample record:\n",
      "{\n",
      "  \"type\": \"arrival\",\n",
      "  \"status\": \"landed\",\n",
      "  \"departure\": {\n",
      "    \"iataCode\": \"vlc\",\n",
      "    \"icaoCode\": \"levc\",\n",
      "    \"terminal\": \"1\",\n",
      "    \"gate\": \"3\",\n",
      "    \"delay\": 49,\n",
      "    \"scheduledTime\": \"2025-05-19t23:10:00.000\",\n",
      "    \"estimatedTime\": \"2025-05-19t23:10:00.000\",\n",
      "    \"actualTime\": \"2025-05-19t23:58:00.000\",\n",
      "    \"estimatedRunway\": \"2025-05-19t23:58:00.000\",\n",
      "    \"actualRunway\": \"2025-05-19t23:58:00.000\"\n",
      "  },\n",
      "  \"arrival\": {\n",
      "    \"iataCode\": \"dub\",\n",
      "    \"icaoCode\": \"eidw\",\n",
      "    \"terminal\": \"t1\",\n",
      "    \"bagga\n",
      "\n",
      "--- Keys in dub_departure_history.json ---\n",
      "['airline.iataCode', 'airline.icaoCode', 'airline.name', 'arrival.actualRunway', 'arrival.actualTime', 'arrival.baggage', 'arrival.delay', 'arrival.estimatedRunway', 'arrival.estimatedTime', 'arrival.gate', 'arrival.iataCode', 'arrival.icaoCode', 'arrival.scheduledTime', 'arrival.terminal', 'codeshared.airline.iataCode', 'codeshared.airline.icaoCode', 'codeshared.airline.name', 'codeshared.flight.iataNumber', 'codeshared.flight.icaoNumber', 'codeshared.flight.number', 'departure.actualRunway', 'departure.actualTime', 'departure.delay', 'departure.estimatedRunway', 'departure.estimatedTime', 'departure.gate', 'departure.iataCode', 'departure.icaoCode', 'departure.scheduledTime', 'departure.terminal', 'flight.iataNumber', 'flight.icaoNumber', 'flight.number', 'status', 'type']\n",
      "\n",
      "Sample record:\n",
      "{\n",
      "  \"type\": \"departure\",\n",
      "  \"status\": \"active\",\n",
      "  \"departure\": {\n",
      "    \"iataCode\": \"dub\",\n",
      "    \"icaoCode\": \"eidw\",\n",
      "    \"delay\": 5,\n",
      "    \"scheduledTime\": \"2025-05-20t02:20:00.000\",\n",
      "    \"estimatedTime\": \"2025-05-20t02:31:00.000\",\n",
      "    \"actualTime\": \"2025-05-20t02:24:00.000\",\n",
      "    \"estimatedRunway\": \"2025-05-20t02:24:00.000\",\n",
      "    \"actualRunway\": \"2025-05-20t02:24:00.000\"\n",
      "  },\n",
      "  \"arrival\": {\n",
      "    \"iataCode\": \"fra\",\n",
      "    \"icaoCode\": \"eddf\",\n",
      "    \"scheduledTime\": \"2025-05-20t05:15:00.000\",\n",
      "    \"estimatedTime\": \n"
     ]
    }
   ],
   "source": [
    "# Step 10 - inspect headings of downloaded JSON files\n",
    "RAW_DIR = Path(\"data\") / \"raw_flights\"\n",
    "ARR_FILE = RAW_DIR / \"dub_arrival_history.json\"\n",
    "DEP_FILE = RAW_DIR / \"dub_departure_history.json\"\n",
    "\n",
    "def inspect_keys(json_file, sample_size=50):\n",
    "    \"\"\"Inspect nested keys in a JSON file by sampling records.\"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    # Use pandas.json_normalize to flatten structure\n",
    "    import pandas as pd\n",
    "    df = pd.json_normalize(records)\n",
    "\n",
    "    # Show all column headings\n",
    "    print(f\"\\n--- Keys in {json_file.name} ---\")\n",
    "    print(sorted(df.columns.tolist()))\n",
    "\n",
    "    # Optionally preview first record\n",
    "    print(\"\\nSample record:\")\n",
    "    print(json.dumps(records[0], indent=2)[:500])  # truncate for readability\n",
    "\n",
    "# Inspect both files\n",
    "inspect_keys(ARR_FILE)\n",
    "inspect_keys(DEP_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907d287",
   "metadata": {},
   "source": [
    "### üìÇ Step 11 ‚Äì Parse Flight History JSON into Tidy DataFrames\n",
    "\n",
    "This step takes the raw JSON files downloaded from the Aviation Edge API  \n",
    "(`dub_arrival_history.json` and `dub_departure_history.json`) and converts them into clean, analysis‚Äëready DataFrames.\n",
    "\n",
    "**What the code does:**\n",
    "- Loads the raw JSON files for arrivals and departures.\n",
    "- Flattens the nested JSON structure into tabular form using `pandas.json_normalize`.\n",
    "- Keeps only the **relevant fields** for weather analysis:\n",
    "  - Flight number (`flight_iata`)\n",
    "  - Airline name\n",
    "  - Flight status (landed, cancelled, active, etc.)\n",
    "  - Scheduled, estimated, and actual times\n",
    "  - Delay (either reported by API or calculated if missing)\n",
    "  - Optional operational details (terminal, runway)\n",
    "- Converts timestamps into proper datetime objects.\n",
    "- Derives `date` and `time` columns to align flights with hourly weather data.\n",
    "- Adds a `type` column to distinguish arrivals vs departures.\n",
    "- Returns three DataFrames:\n",
    "  - `df_arrivals` ‚Üí tidy arrivals\n",
    "  - `df_departures` ‚Üí tidy departures\n",
    "  - `df_all` ‚Üí combined dataset of both arrivals and departures\n",
    "\n",
    "**Why this matters:**\n",
    "- The raw JSON files are large and verbose; this step filters them down to only the fields needed for analysis.\n",
    "- The tidy DataFrames provide a consistent schema that can be merged directly with weather observations.\n",
    "- Having both separate and combined views makes it easy to analyse arrivals and departures independently or together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e1522b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrivals shape: (131556, 14)\n",
      "Departures shape: (137720, 14)\n",
      "Combined shape: (269276, 14)\n",
      "  flight_iata            airline  status               sched  \\\n",
      "0      fr1739            ryanair  landed 2025-05-20 01:00:00   \n",
      "1      fr9612            ryanair  landed 2025-05-20 01:10:00   \n",
      "2       fr651            ryanair  landed 2025-05-20 01:15:00   \n",
      "3      aa8330  american airlines  landed 2025-05-20 04:25:00   \n",
      "4      ba6124    british airways  landed 2025-05-20 04:25:00   \n",
      "\n",
      "                  est                 act  delay terminal  \\\n",
      "0 2025-05-20 01:15:00 2025-05-20 01:15:00   15.0       t1   \n",
      "1 2025-05-20 01:11:00 2025-05-20 01:03:00    NaN       t1   \n",
      "2 2025-05-20 01:12:00 2025-05-20 01:05:00    NaN       t1   \n",
      "3 2025-05-20 03:39:00 2025-05-20 03:39:00    NaN       t2   \n",
      "4 2025-05-20 03:39:00 2025-05-20 03:39:00    NaN       t2   \n",
      "\n",
      "                    runway  delay_calc        date   time  is_cancelled  \\\n",
      "0  2025-05-20t01:15:00.000        15.0  2025-05-20  01:00         False   \n",
      "1  2025-05-20t01:03:00.000         NaN  2025-05-20  01:10         False   \n",
      "2  2025-05-20t01:05:00.000         NaN  2025-05-20  01:15         False   \n",
      "3  2025-05-20t03:39:00.000         NaN  2025-05-20  04:25         False   \n",
      "4  2025-05-20t03:39:00.000         NaN  2025-05-20  04:25         False   \n",
      "\n",
      "      type  \n",
      "0  arrival  \n",
      "1  arrival  \n",
      "2  arrival  \n",
      "3  arrival  \n",
      "4  arrival  \n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 11 ‚Äì Parse Flight History JSON into Tidy DataFrames\n",
    "\n",
    "RAW_DIR = Path(\"data\") / \"raw_flights\"\n",
    "ARR_FILE = RAW_DIR / \"dub_arrival_history.json\"\n",
    "DEP_FILE = RAW_DIR / \"dub_departure_history.json\"\n",
    "\n",
    "def parse_flights(json_file, flight_type=\"arrival\"):\n",
    "    \"\"\"\n",
    "    Load a JSON file (arrivals or departures) and return a tidy DataFrame.\n",
    "    Drops irrelevant fields and keeps only those useful for weather analysis.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    if not records:  # safeguard against empty files\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.json_normalize(records)\n",
    "\n",
    "    # Column mapping based on flight type\n",
    "    if flight_type == \"arrival\":\n",
    "        cols_map = {\n",
    "            \"flight.iataNumber\": \"flight_iata\",\n",
    "            \"airline.name\": \"airline\",\n",
    "            \"status\": \"status\",\n",
    "            \"arrival.scheduledTime\": \"sched\",\n",
    "            \"arrival.estimatedTime\": \"est\",\n",
    "            \"arrival.actualTime\": \"act\",\n",
    "            \"arrival.delay\": \"delay\",\n",
    "            \"arrival.terminal\": \"terminal\",\n",
    "            \"arrival.actualRunway\": \"runway\"\n",
    "        }\n",
    "    else:  # departure\n",
    "        cols_map = {\n",
    "            \"flight.iataNumber\": \"flight_iata\",\n",
    "            \"airline.name\": \"airline\",\n",
    "            \"status\": \"status\",\n",
    "            \"departure.scheduledTime\": \"sched\",\n",
    "            \"departure.estimatedTime\": \"est\",\n",
    "            \"departure.actualTime\": \"act\",\n",
    "            \"departure.delay\": \"delay\",\n",
    "            \"departure.terminal\": \"terminal\",\n",
    "            \"departure.actualRunway\": \"runway\"\n",
    "        }\n",
    "\n",
    "    # Keep only available columns\n",
    "    available = {k: v for k, v in cols_map.items() if k in df.columns}\n",
    "    df = df[list(available.keys())].rename(columns=available)\n",
    "\n",
    "    # Parse timestamps\n",
    "    for c in [\"sched\", \"est\", \"act\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Compute delay if missing\n",
    "    if \"delay\" not in df.columns or df[\"delay\"].isna().all():\n",
    "        df[\"delay_calc\"] = (df[\"act\"] - df[\"sched\"]).dt.total_seconds() / 60.0\n",
    "    else:\n",
    "        df[\"delay_calc\"] = df[\"delay\"]\n",
    "\n",
    "    # Merge keys for weather alignment\n",
    "    df[\"date\"] = df[\"sched\"].dt.date\n",
    "    df[\"time\"] = df[\"sched\"].dt.strftime(\"%H:%M\")\n",
    "    df[\"is_cancelled\"] = df[\"status\"].str.lower().eq(\"cancelled\")\n",
    "\n",
    "    df[\"type\"] = flight_type  # add explicit type column\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Load both arrivals and departures ---\n",
    "df_arrivals = parse_flights(ARR_FILE, \"arrival\")\n",
    "df_departures = parse_flights(DEP_FILE, \"departure\")\n",
    "\n",
    "# --- Optionally combine into one DataFrame ---\n",
    "df_all = pd.concat([df_arrivals, df_departures], ignore_index=True)\n",
    "\n",
    "print(\"Arrivals shape:\", df_arrivals.shape)\n",
    "print(\"Departures shape:\", df_departures.shape)\n",
    "print(\"Combined shape:\", df_all.shape)\n",
    "\n",
    "print(df_all.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef302a60",
   "metadata": {},
   "source": [
    "### üìÇ Step 12 ‚Äì Save Tidy Flight DataFrames\n",
    "\n",
    "In this step, the cleaned flight DataFrames created in Step‚ÄØ11 are written out to disk.  \n",
    "Saving them ensures that the tidy, analysis‚Äëready datasets are preserved and can be reused without re‚Äëparsing the raw JSON files.\n",
    "\n",
    "**What is saved:**\n",
    "- `dub_arrivals_tidy.csv` ‚Üí arrivals into Dublin Airport, with relevant fields (flight number, airline, status, times, delay, etc.).\n",
    "- `dub_departures_tidy.csv` ‚Üí departures from Dublin Airport, with the same tidy structure.\n",
    "- `dub_flights_tidy.csv` ‚Üí combined dataset containing both arrivals and departures, distinguished by a `type` column.\n",
    "\n",
    "**Why this matters:**\n",
    "- The tidy CSVs are much smaller and easier to work with than the raw JSON files.\n",
    "- They provide a consistent schema for merging with weather data (using `date` and `time`).\n",
    "- Reviewers and collaborators can immediately use these files without needing to rerun the full API download.\n",
    "\n",
    "üëâ This step finalises the preprocessing pipeline: raw JSON ‚Üí tidy DataFrames ‚Üí reusable CSVs in the `data/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac5d01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved tidy datasets into data/ folder\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 12 ‚Äì Save Tidy Flight DataFrames\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# Save arrivals\n",
    "df_arrivals.to_csv(DATA_DIR / \"dub_arrivals_tidy.csv\", index=False)\n",
    "\n",
    "# Save departures\n",
    "df_departures.to_csv(DATA_DIR / \"dub_departures_tidy.csv\", index=False)\n",
    "\n",
    "# Save combined dataset\n",
    "df_all.to_csv(DATA_DIR / \"dub_flights_tidy.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Saved tidy datasets into data/ folder\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
