{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae6fbf9",
   "metadata": {},
   "source": [
    "# Flight Punctuality Analysis at Dublin Airport\n",
    "\n",
    "This project examines how weather conditions influence flight punctuality at Dublin Airport.  \n",
    "The analysis combines flight activity data (arrivals, departures, delays, cancellations) with historical and forecast weather data from Met √âireann to identify trends, quantify the impact of adverse conditions, and project future delay probabilities. \n",
    " \n",
    "By aligning operational flight records with local weather observations, the study provides insights into how rain, wind, and visibility affect airport performance and passenger reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96525b",
   "metadata": {},
   "source": [
    "### Notebook Control Flag Explanation\n",
    "\n",
    "This notebook contains code to download flight history data from the Aviation Edge API.  \n",
    "Because downloading six months of data can take a long time and may stress the API, we use a **control flag** called `RUN_DOWNLOAD` to decide whether the download should run.\n",
    "\n",
    "- **RUN_DOWNLOAD = False** ‚Üí The download section is skipped.  \n",
    "  Use this setting when you want to run analysis, visualizations, or other notebook functionality without refreshing the data.\n",
    "\n",
    "- **RUN_DOWNLOAD = True** ‚Üí The download section executes.  \n",
    "  Use this setting only when you deliberately want to refresh the flight history data and update the cumulative JSON files.\n",
    "\n",
    "This design ensures:\n",
    "- The notebook can be safely re-run without triggering unwanted downloads.\n",
    "- Existing JSON files are preserved and can be loaded for analysis.\n",
    "- You have full control over when heavy API calls are made.\n",
    "\n",
    "üëâ In practice: keep `RUN_DOWNLOAD = False` most of the time, and flip it to `True` only when you need new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c9585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Control flag to enable/disable data refresh ---\n",
    "RUN_DOWNLOAD = False   # Change to True only when you want to refresh data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037f7b5",
   "metadata": {},
   "source": [
    "### üì¶ Step 2 ‚Äì Install and Import Required Libraries\n",
    "\n",
    "This step prepares the environment for the Dublin Airport Flight Rerouting Project.  \n",
    "It ensures that all required Python packages are available and sets up the project‚Äôs directory structure inside the `project` root.\n",
    "\n",
    "The notebook imports essential libraries for:\n",
    "\n",
    "- üìä **Data manipulation** (`pandas`, `numpy`)\n",
    "- üìÖ **Date and time handling** (`datetime`, `matplotlib.dates`)\n",
    "- üìà **Plotting and visualisation** (`matplotlib`, `seaborn`, `plotly`)\n",
    "- ü§ñ **Machine learning and model persistence** (`scikit-learn`, `joblib`)\n",
    "- üìÇ **File handling and paths** (`os`, `pathlib`, `json`)\n",
    "- üåê **Web access** (`requests`)\n",
    "- üß© **Interactivity and display** (`ipywidgets`, `IPython.display`)\n",
    "\n",
    "It also defines key directories (`data`, `outputs`, `models`, `docs`) inside the `project` folder and ensures they exist.  \n",
    "This structure keeps raw data, processed outputs, trained models, and documentation organised and reproducible.\n",
    "\n",
    "üìå *Note: `%pip install` commands can be used inside Jupyter notebooks if a package is missing.  \n",
    "For scripts or terminal use, run `pip install` directly.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86305781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Project root: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\n",
      "Data directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\data\n",
      "Output directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\outputs\n",
      "Model directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\models\n",
      "Docs directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\docs\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly --quiet\n",
    "\n",
    "# --- Core Python modules ---\n",
    "import json              # config files / JSON handling\n",
    "import os                # operating system interactions\n",
    "import time              # time management\n",
    "import warnings          # manage warnings\n",
    "from datetime import date, timedelta  # date calculations\n",
    "from pathlib import Path              # path management\n",
    "from calendar import monthrange       # leap-year safe month calculations\n",
    "\n",
    "# --- Data science / numerical libraries ---\n",
    "import numpy as np       # numerical operations\n",
    "import pandas as pd      # data manipulation\n",
    "\n",
    "# --- Plotting libraries ---\n",
    "import matplotlib.pyplot as plt   # static plotting\n",
    "import plotly.express as px       # interactive plotting\n",
    "import seaborn as sns             # enhanced plotting\n",
    "\n",
    "# --- Machine learning libraries ---\n",
    "import joblib                     # model persistence\n",
    "from sklearn.ensemble import GradientBoostingClassifier   # example model\n",
    "from sklearn.linear_model import LogisticRegression       # example model\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score\n",
    ")  # model evaluation\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    train_test_split\n",
    ")  # model validation\n",
    "\n",
    "# --- API / external requests ---\n",
    "import requests                   # API calls\n",
    "\n",
    "# --- Plotting style ---\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# --- Explicit project root: programming-for-data-analytics/project ---\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name != \"project\":\n",
    "    # climb up until we find project folder\n",
    "    for parent in ROOT.parents:\n",
    "        if parent.name == \"project\":\n",
    "            ROOT = parent\n",
    "            break\n",
    "\n",
    "# --- Define key directories inside project ---\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\"\n",
    "MODEL_DIR = ROOT / \"models\"\n",
    "DOCS_DIR = ROOT / \"docs\"\n",
    "\n",
    "# --- Ensure directories exist ---\n",
    "for path in [DATA_DIR, OUTPUT_DIR, MODEL_DIR, DOCS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Docs directory: {DOCS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468332e9",
   "metadata": {},
   "source": [
    "### Step 3 ‚Äì Utilise Helper Functions for Dublin Airport Data Processing\n",
    "\n",
    "This section defines a set of reusable helper functions that simplify common tasks in the project.  \n",
    "They are designed specifically to support the analysis of **Dublin Airport flight activity and weather data** by handling messy inputs and preparing clean datasets for exploration and modelling.\n",
    "\n",
    "The functions help with:\n",
    "\n",
    "- ‚úÖ Detecting and parsing inconsistent datetime formats in flight and weather logs  \n",
    "- ‚úÖ Standardising and cleaning temperature and precipitation columns from Met √âireann datasets  \n",
    "- ‚úÖ Loading and preparing Dublin Airport daily weather data from local CSV files  \n",
    "- ‚úÖ Defining Irish seasonal boundaries (Winter, Spring, Summer, Autumn) for comparative analysis  \n",
    "- ‚úÖ Filtering weather data for a custom date range to align with flight events  \n",
    "- ‚úÖ Validating user-provided date inputs for reproducible analysis  \n",
    "- ‚úÖ Detecting header rows in raw CSV files downloaded from dashboards  \n",
    "\n",
    "Each helper is **modular** ‚Äî it performs one clear task and can be reused across notebooks and scripts.  \n",
    "This improves readability, reduces duplication, and supports good programming practices for the final project.\n",
    "\n",
    "üìå *Tip: These helpers are written to be beginner-friendly, with comments explaining their purpose and logic. They make it easier to align flight activity with weather conditions when investigating delays and cancellations.*\n",
    "\n",
    "üìñ References:  \n",
    "- [Real Python ‚Äì Python Helper Functions](https://realpython.com/defining-your-own-python-function/)  \n",
    "- [GeeksforGeeks ‚Äì Python Helper Functions](https://www.geeksforgeeks.org/python-helper-functions/)  \n",
    "- [Wikipedia ‚Äì DRY Principle (Don't Repeat Yourself)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607be6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Helper Functions for Dublin Airport Project\n",
    "# These functions handle parsing dates, cleaning weather data, preparing ranges,\n",
    "# defining Irish seasons, and detecting CSV headers.\n",
    "# Keep them in one cell so they are easy to reuse across the notebook.\n",
    "\n",
    "# üîç Detect the header row in a CSV file\n",
    "def detect_header(lines, keywords=(\"station\",\"date\",\"rain\",\"temp\",\"wind\")):\n",
    "    \"\"\"\n",
    "    Detect the most likely header row in a CSV file.\n",
    "    Looks for lines containing known weather keywords and multiple columns.\n",
    "    \"\"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        line_lower = line.strip().lower()\n",
    "        if any(line_lower.startswith(k) for k in keywords) and \",\" in line:\n",
    "            columns = line.split(\",\")\n",
    "            if len(columns) > 3:  # header rows usually have multiple columns\n",
    "                return i\n",
    "    print(\"‚ö†Ô∏è Warning: header row not found. Defaulting to first line.\")\n",
    "    return 0\n",
    "\n",
    "# üìÖ Detect the most likely datetime format from sample strings\n",
    "def detect_datetime_format(samples, formats, dayfirst=True, min_match_ratio=0.7, min_absolute=5):\n",
    "    \"\"\"\n",
    "    Try each format and return the one that matches at least 70% of samples\n",
    "    or at least 'min_absolute' matches. Helps ensure consistent parsing of date strings.\n",
    "    \"\"\"\n",
    "    for fmt in formats:\n",
    "        parsed = pd.to_datetime(samples, format=fmt, dayfirst=dayfirst, errors='coerce')\n",
    "        matches = parsed.notna().sum()\n",
    "        if matches >= max(min_absolute, int(len(samples) * min_match_ratio)):\n",
    "            return fmt\n",
    "    return None\n",
    "\n",
    "# üìÖ Parse a datetime column using format detection or fallback\n",
    "def parse_datetime_column(df, date_col, candidate_formats=None, dayfirst=True):\n",
    "    \"\"\"\n",
    "    Parse a datetime column using known formats.\n",
    "    Falls back to flexible parsing if none match.\n",
    "    \"\"\"\n",
    "    if candidate_formats is None:\n",
    "        candidate_formats = [\n",
    "            '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%d-%b-%Y %H:%M',\n",
    "            '%d/%m/%Y %H:%M:%S', '%d/%m/%Y %H:%M', '%d-%m-%Y %H:%M',\n",
    "            '%d %b %Y %H:%M', '%d %B %Y %H:%M',\n",
    "        ]\n",
    "\n",
    "    sample_vals = df[date_col].dropna().astype(str).head(100).tolist()\n",
    "    chosen_fmt = detect_datetime_format(sample_vals, candidate_formats, dayfirst=dayfirst)\n",
    "\n",
    "    if chosen_fmt:\n",
    "        print(f\"‚úÖ Detected datetime format: {chosen_fmt}\")\n",
    "        return pd.to_datetime(df[date_col], format=chosen_fmt, dayfirst=dayfirst, errors='coerce')\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No single format matched. Falling back to flexible parsing.\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', message='Could not infer format')\n",
    "            return pd.to_datetime(df[date_col], dayfirst=dayfirst, errors='coerce')\n",
    "\n",
    "# üïí Ensure full datetime column for arrivals/departures and weather\n",
    "def prepare_datetime(df, date_col='date', time_col=None):\n",
    "    \"\"\"\n",
    "    Ensure DataFrame has a full datetime column.\n",
    "    Works for datasets with combined 'date' + time or already combined datetime.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    if 'datetime' in df.columns:\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    elif date_col in df.columns and time_col:\n",
    "        dt_strings = df[date_col].astype(str) + \" \" + df[time_col].astype(str)\n",
    "        df['datetime'] = pd.to_datetime(dt_strings, format=\"%d-%b-%Y %H:%M\", errors='coerce')\n",
    "    elif date_col in df.columns:\n",
    "        # Explicit format for Met √âireann hourly data: '01-jan-1945 00:00'\n",
    "        df['datetime'] = pd.to_datetime(df[date_col], format=\"%d-%b-%Y %H:%M\", errors='coerce')\n",
    "    else:\n",
    "        raise KeyError(\"No suitable date/time columns found\")\n",
    "\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    return df.dropna(subset=['datetime']).reset_index(drop=True)\n",
    "\n",
    "    # Add convenience fields\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    \n",
    "    return df.dropna(subset=['datetime']).reset_index(drop=True)\n",
    "\n",
    "# üå°Ô∏è Ensure temperature column is numeric and named 'temp'\n",
    "def parse_temperature_column(df, col_name='temp'):\n",
    "    \"\"\"\n",
    "    Convert the temperature column to numeric and rename it to 'temp'.\n",
    "    If no exact match, look for any column containing 'temp'.\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        col_name = next((c for c in df.columns if 'temp' in c.lower()), None)\n",
    "        if col_name is None:\n",
    "            raise KeyError(\"No temperature column found.\")\n",
    "    if 'temp' in df.columns and col_name != 'temp':\n",
    "        df.rename(columns={col_name: 'temp'}, inplace=True)\n",
    "    else:\n",
    "        df['temp'] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# üìÇ Load cleaned weather data from local CSV\n",
    "def load_cleaned_weather_data(filepath=\"data/dublin_airport_hourly.csv\"):\n",
    "    \"\"\"\n",
    "    Load weather dataset from CSV and strip spaces from column names.\n",
    "    Default path now points to hourly Dublin Airport data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "# üìä Prepare weather data with proper datetime column\n",
    "def prepare_weather_data(df, date_col='date'):\n",
    "    \"\"\"\n",
    "    Ensure weather DataFrame has a proper datetime column.\n",
    "    Works for Met √âireann hourly datasets where 'date' already includes time.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    if 'datetime' in df.columns:\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    elif date_col in df.columns:\n",
    "        # Explicit format: '01-jan-1945 00:00'\n",
    "        df['datetime'] = pd.to_datetime(df[date_col], format=\"%d-%b-%Y %H:%M\", errors='coerce')\n",
    "    else:\n",
    "        raise ValueError(\"No suitable date column found in weather dataset\")\n",
    "\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    return df\n",
    "\n",
    "# üõ†Ô∏è Clean and standardise key weather columns\n",
    "def clean_weather_columns(df):\n",
    "    \"\"\"\n",
    "    Standardise Dublin Airport hourly weather data:\n",
    "    - Convert rainfall, temperature, wind speed, and pressure to numeric\n",
    "    - Handle 'Tr' (trace) rainfall as 0.0\n",
    "    - Ensure consistent column naming\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # Handle rainfall column\n",
    "    if 'rain' in df.columns:\n",
    "        df['rain'] = df['rain'].replace('Tr', 0.0)   # trace rainfall ‚Üí 0\n",
    "        df['rain'] = pd.to_numeric(df['rain'], errors='coerce')\n",
    "\n",
    "    # Handle temperature column\n",
    "    temp_col = next((c for c in df.columns if 'temp' in c), None)\n",
    "    if temp_col:\n",
    "        df['temp'] = pd.to_numeric(df[temp_col], errors='coerce')\n",
    "\n",
    "    # Handle wind speed column\n",
    "    if 'wdsp' in df.columns:\n",
    "        df['wdsp'] = pd.to_numeric(df['wdsp'], errors='coerce')\n",
    "\n",
    "    # Handle pressure column\n",
    "    if 'msl' in df.columns:\n",
    "        df['msl'] = pd.to_numeric(df['msl'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "# üìÜ Convert user input strings into a validated date range\n",
    "def get_custom_range(start_str, end_str):\n",
    "    \"\"\"\n",
    "    Convert string inputs into datetime objects and validate order.\n",
    "    Handles ISO (YYYY-MM-DD) and European (DD/MM/YYYY) formats gracefully.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try ISO format first\n",
    "        try:\n",
    "            start = pd.to_datetime(start_str, format=\"%Y-%m-%d\", errors=\"raise\")\n",
    "        except Exception:\n",
    "            start = pd.to_datetime(start_str, dayfirst=True)\n",
    "\n",
    "        try:\n",
    "            end = pd.to_datetime(end_str, format=\"%Y-%m-%d %H:%M\", errors=\"raise\")\n",
    "        except Exception:\n",
    "            end = pd.to_datetime(end_str, dayfirst=True)\n",
    "\n",
    "        if start > end:\n",
    "            raise ValueError(\"Start date must be before end date.\")\n",
    "        return start, end\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Invalid date range: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# üçÇ Define Irish seasonal boundaries for a given year (leap year safe)\n",
    "def define_irish_seasons(year=2025):\n",
    "    \"\"\"\n",
    "    Return start and end dates for Irish meteorological seasons.\n",
    "    Handles leap years correctly for February.\n",
    "    Seasons are defined as:\n",
    "    - Winter: 1 December (previous year) ‚Üí end of February\n",
    "    - Spring: 1 March ‚Üí 31 May\n",
    "    - Summer: 1 June ‚Üí 31 August\n",
    "    - Autumn: 1 September ‚Üí 30 November\n",
    "    \"\"\"\n",
    "    # Determine the number of days in February for leap year safety\n",
    "    feb_days = monthrange(year, 2)[1]\n",
    "\n",
    "    data = [\n",
    "        (\"Winter\", pd.Timestamp(f\"{year-1}-12-01\"), pd.Timestamp(f\"{year}-02-{feb_days} 23:59\")),\n",
    "        (\"Spring\", pd.Timestamp(f\"{year}-03-01\"), pd.Timestamp(f\"{year}-05-31 23:59\")),\n",
    "        (\"Summer\", pd.Timestamp(f\"{year}-06-01\"), pd.Timestamp(f\"{year}-08-31 23:59\")),\n",
    "        (\"Autumn\", pd.Timestamp(f\"{year}-09-01\"), pd.Timestamp(f\"{year}-11-30 23:59\")),\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(data, columns=[\"season\", \"start\", \"end\"])\n",
    "\n",
    "# üçÇ Assign Irish seasons to a DataFrame in bulk (vectorised)\n",
    "def assign_season_vectorized(df):\n",
    "    \"\"\"\n",
    "    Vectorised season assignment for Irish meteorological seasons.\n",
    "    Much faster than row-wise apply.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    dt = df['datetime']\n",
    "\n",
    "    # Initialise season column\n",
    "    df['season'] = None\n",
    "\n",
    "    # Loop through each year present in the dataset\n",
    "    for year in dt.dt.year.dropna().unique():\n",
    "        feb_days = monthrange(int(year), 2)[1]\n",
    "        seasons = [\n",
    "            (\"Winter\", pd.Timestamp(f\"{year-1}-12-01\"), pd.Timestamp(f\"{year}-02-{feb_days} 23:59\")),\n",
    "            (\"Spring\", pd.Timestamp(f\"{year}-03-01\"), pd.Timestamp(f\"{year}-05-31 23:59\")),\n",
    "            (\"Summer\", pd.Timestamp(f\"{year}-06-01\"), pd.Timestamp(f\"{year}-08-31 23:59\")),\n",
    "            (\"Autumn\", pd.Timestamp(f\"{year}-09-01\"), pd.Timestamp(f\"{year}-11-30 23:59\")),\n",
    "        ]\n",
    "\n",
    "        # Assign seasons in bulk using masks\n",
    "        for season, start, end in seasons:\n",
    "            mask = (dt >= start) & (dt <= end)\n",
    "            df.loc[mask, 'season'] = season\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecf4c3",
   "metadata": {},
   "source": [
    "### üìÇ Step 4 ‚Äì Download Dublin Airport Daily Data and Detect Header Row\n",
    "\n",
    "In this step, the notebook retrieves the **Dublin Airport Daily Data CSV** directly from Met √âireann‚Äôs open data service.  \n",
    "This dataset contains daily weather observations (e.g., precipitation, temperature, wind speed, radiation) recorded at Dublin Airport, which will later be aligned with flight activity logs to analyse rerouting events.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üåê **Downloading the raw CSV** from Met √âireann using the `requests` library.  \n",
    "- üìÇ **Defining a local output path** (`data/dublin_airport_daily.csv`) to store the file inside the project‚Äôs `data` folder.  \n",
    "- ‚úÖ **Checking the HTTP response** to ensure the download was successful.  \n",
    "- üìë **Splitting the file into lines** so the structure can be inspected before loading into pandas.  \n",
    "- üîç **Detecting the header row** using the `detect_header` helper function defined earlier.  \n",
    "  This ensures that column names (such as `date`, `maxtp`, `mintp`, `rain`, `wdsp`) are correctly identified even if the file contains metadata lines at the top.  \n",
    "- üñ®Ô∏è **Printing the detected header row** to confirm the correct starting point for parsing.\n",
    "\n",
    "üìå *Tip: Detecting the header row is important because Met √âireann CSVs often include metadata lines before the actual data table.  \n",
    "By confirming the header row, you avoid misaligned columns and ensure clean parsing in later steps.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cc44a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Header row detected at line 23:\n",
      "date,ind,rain,ind,temp,ind,wetb,dewpt,vappr,rhum,msl,ind,wdsp,ind,wddir,ww,w,sun,vis,clht,clamt\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 4 ‚Äì Download Dublin Airport Hourly Data CSV and Detect Header Row\n",
    "\n",
    "# Note: pathlib.Path and requests are imported earlier in the notebook, so we avoid re-importing them here.\n",
    "\n",
    "# --- Define output path for cleaned CSV ---\n",
    "DATA_PATH = Path(\"data/dublin_airport_hourly.csv\")\n",
    "\n",
    "# --- URL for the hourly CSV (may return 404 if file moved) ---\n",
    "url = \"https://cli.fusio.net/cli/climate_data/webdata/hly532.csv\"\n",
    "\n",
    "# --- Attempt to download the remote CSV, with a safe fallback to a local copy ---\n",
    "try:\n",
    "    response = requests.get(url, timeout=30)\n",
    "except Exception as e:\n",
    "    response = None\n",
    "    print(f\"‚ö†Ô∏è Network error when fetching URL: {e}\")\n",
    "\n",
    "if response is None or getattr(response, \"status_code\", None) != 200:\n",
    "    # If remote download failed, try to use a previously saved local file if available\n",
    "    if DATA_PATH.exists():\n",
    "        print(f\"‚ö†Ô∏è Remote download failed (status: {getattr(response,'status_code',None)}). Falling back to local file: {DATA_PATH}\")\n",
    "        text = DATA_PATH.read_text(encoding=\"utf-8\")\n",
    "        lines = text.splitlines()\n",
    "    else:\n",
    "        raise RuntimeError(f\"‚ùå Failed to download data: HTTP {getattr(response,'status_code',None)} and no local fallback at {DATA_PATH}\")\n",
    "else:\n",
    "    # Successful download ‚Äî use remote content\n",
    "    lines = response.text.splitlines()\n",
    "\n",
    "# --- Detect header row using helper function ---\n",
    "header_index = detect_header(lines)\n",
    "\n",
    "# ‚úÖ Confirm detected header row\n",
    "print(f\"‚úÖ Header row detected at line {header_index}:\")\n",
    "print(lines[header_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f4588",
   "metadata": {},
   "source": [
    "### üìë Step 4b ‚Äì Load and Inspect Dublin Airport Hourly Data\n",
    "\n",
    "After detecting the correct header row in the raw CSV file, the next step is to **load the hourly dataset into pandas**.  \n",
    "This allows us to immediately inspect the structure of the Dublin Airport Hourly Data and confirm that the columns (e.g., `date`, `time`, `temp`, `rain`, `wdsp`, `msl`) are correctly aligned.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üìÇ Reading the CSV into a pandas DataFrame, starting from the detected header row  \n",
    "- üîç Displaying the first few rows with `head()` to verify column names and sample values  \n",
    "- üßæ Using `info()` to check datatypes and identify potential missing values  \n",
    "- üìä Summarising numeric columns with `describe()` to get a quick statistical overview  \n",
    "\n",
    "üìå *Why this matters:* Inspecting the hourly dataset ensures that the header detection worked correctly and that the file is ready for consistent downstream analysis.  \n",
    "Since arrivals and departures depend on **exact times**, hourly weather data provides the necessary granularity to align flight events with Dublin Airport conditions. This step acts as a validation checkpoint before committing the cleaned file to the `data/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34c71e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of Dublin Airport Hourly Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ind</th>\n",
       "      <th>rain</th>\n",
       "      <th>ind.1</th>\n",
       "      <th>temp</th>\n",
       "      <th>ind.2</th>\n",
       "      <th>wetb</th>\n",
       "      <th>dewpt</th>\n",
       "      <th>vappr</th>\n",
       "      <th>rhum</th>\n",
       "      <th>...</th>\n",
       "      <th>ind.3</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>ind.4</th>\n",
       "      <th>wddir</th>\n",
       "      <th>ww</th>\n",
       "      <th>w</th>\n",
       "      <th>sun</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>clamt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-jan-1945 00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.2</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-jan-1945 01:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-jan-1945 02:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4800</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-jan-1945 03:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-jan-1945 04:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date  ind  rain  ind.1  temp  ind.2  wetb  dewpt vappr rhum  \\\n",
       "0  01-jan-1945 00:00    2   0.0      0   4.9      0   4.6    4.4   8.2   95   \n",
       "1  01-jan-1945 01:00    3   0.0      0   5.1      0   4.9    4.4   8.5   97   \n",
       "2  01-jan-1945 02:00    2   0.0      0   5.1      0   4.8    4.4   8.5   97   \n",
       "3  01-jan-1945 03:00    0   0.2      0   5.2      0   5.0    4.4   8.5   97   \n",
       "4  01-jan-1945 04:00    2   0.0      0   5.6      0   5.4    5.0   8.8   97   \n",
       "\n",
       "   ...  ind.3  wdsp  ind.4  wddir  ww  w  sun   vis clht clamt  \n",
       "0  ...      1     0      1      0  50  4  0.0   200    2     8  \n",
       "1  ...      1     0      1      0  45  4  0.0   200    2     8  \n",
       "2  ...      1     0      1      0  50  4  0.0  4800    4     8  \n",
       "3  ...      1     0      1      0  50  4  0.0  6000    4     8  \n",
       "4  ...      1     7      1    250  50  5  0.0  6000    4     8  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 708577 entries, 0 to 708576\n",
      "Data columns (total 21 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   date    708577 non-null  object \n",
      " 1   ind     708577 non-null  int64  \n",
      " 2   rain    708577 non-null  float64\n",
      " 3   ind.1   708577 non-null  int64  \n",
      " 4   temp    708577 non-null  float64\n",
      " 5   ind.2   708577 non-null  int64  \n",
      " 6   wetb    708577 non-null  float64\n",
      " 7   dewpt   708577 non-null  float64\n",
      " 8   vappr   708577 non-null  object \n",
      " 9   rhum    708577 non-null  object \n",
      " 10  msl     708577 non-null  float64\n",
      " 11  ind.3   708577 non-null  int64  \n",
      " 12  wdsp    708577 non-null  int64  \n",
      " 13  ind.4   708577 non-null  int64  \n",
      " 14  wddir   708577 non-null  object \n",
      " 15  ww      708577 non-null  int64  \n",
      " 16  w       708577 non-null  int64  \n",
      " 17  sun     708577 non-null  float64\n",
      " 18  vis     708577 non-null  object \n",
      " 19  clht    708577 non-null  object \n",
      " 20  clamt   708577 non-null  object \n",
      "dtypes: float64(6), int64(8), object(7)\n",
      "memory usage: 113.5+ MB\n",
      "None\n",
      "\n",
      "Summary statistics:\n",
      "                     date            ind           rain          ind.1  \\\n",
      "count              708577  708577.000000  708577.000000  708577.000000   \n",
      "unique             708577            NaN            NaN            NaN   \n",
      "top     01-jan-1945 00:00            NaN            NaN            NaN   \n",
      "freq                    1            NaN            NaN            NaN   \n",
      "mean                  NaN       0.632887       0.086568       0.014206   \n",
      "std                   NaN       1.101550       0.418433       0.118613   \n",
      "min                   NaN       0.000000       0.000000       0.000000   \n",
      "25%                   NaN       0.000000       0.000000       0.000000   \n",
      "50%                   NaN       0.000000       0.000000       0.000000   \n",
      "75%                   NaN       2.000000       0.000000       0.000000   \n",
      "max                   NaN       6.000000      26.500000       2.000000   \n",
      "\n",
      "                 temp          ind.2           wetb          dewpt   vappr  \\\n",
      "count   708577.000000  708577.000000  708577.000000  708577.000000  708577   \n",
      "unique            NaN            NaN            NaN            NaN     212   \n",
      "top               NaN            NaN            NaN            NaN     9.2   \n",
      "freq              NaN            NaN            NaN            NaN    8728   \n",
      "mean         9.665900       0.029293       8.224969       6.604194     NaN   \n",
      "std          4.893787       0.257749       4.409282       4.593159     NaN   \n",
      "min        -11.500000       0.000000     -11.500000     -17.700000     NaN   \n",
      "25%          6.100000       0.000000       5.100000       3.300000     NaN   \n",
      "50%          9.700000       0.000000       8.400000       6.900000     NaN   \n",
      "75%         13.200000       0.000000      11.600000      10.000000     NaN   \n",
      "max         29.100000       6.000000      22.200000      20.500000     NaN   \n",
      "\n",
      "          rhum  ...          ind.3           wdsp          ind.4   wddir  \\\n",
      "count   708577  ...  708577.000000  708577.000000  708577.000000  708577   \n",
      "unique      79  ...            NaN            NaN            NaN      38   \n",
      "top         91  ...            NaN            NaN            NaN     250   \n",
      "freq     26179  ...            NaN            NaN            NaN   48173   \n",
      "mean       NaN  ...       1.352876      10.114745       1.353055     NaN   \n",
      "std        NaN  ...       0.809113       5.680898       0.809156     NaN   \n",
      "min        NaN  ...       0.000000       0.000000       0.000000     NaN   \n",
      "25%        NaN  ...       1.000000       6.000000       1.000000     NaN   \n",
      "50%        NaN  ...       2.000000       9.000000       2.000000     NaN   \n",
      "75%        NaN  ...       2.000000      14.000000       2.000000     NaN   \n",
      "max        NaN  ...       6.000000      46.000000       7.000000     NaN   \n",
      "\n",
      "                   ww              w            sun     vis    clht   clamt  \n",
      "count   708577.000000  708577.000000  708577.000000  708577  708577  708577  \n",
      "unique            NaN            NaN            NaN      96     117      11  \n",
      "top               NaN            NaN            NaN   30000     999       7  \n",
      "freq              NaN            NaN            NaN  132838  187669  215678  \n",
      "mean        15.594647      17.165729       0.167449     NaN     NaN     NaN  \n",
      "std         22.552469      24.207024       0.326705     NaN     NaN     NaN  \n",
      "min          0.000000       0.000000       0.000000     NaN     NaN     NaN  \n",
      "25%          2.000000       2.000000       0.000000     NaN     NaN     NaN  \n",
      "50%          2.000000      11.000000       0.000000     NaN     NaN     NaN  \n",
      "75%         21.000000      11.000000       0.100000     NaN     NaN     NaN  \n",
      "max         97.000000      99.000000       1.000000     NaN     NaN     NaN  \n",
      "\n",
      "[11 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# üìë Step 4b ‚Äì Load and Inspect Dublin Airport Hourly Data\n",
    "\n",
    "# --- Load CSV into pandas using detected header row ---\n",
    "df = pd.read_csv(\n",
    "    \"https://cli.fusio.net/cli/climate_data/webdata/hly532.csv\",  # Dublin Airport hourly dataset\n",
    "    skiprows=header_index,  # skip metadata lines before the header\n",
    "    low_memory=False        # process file in one pass, avoids mixed-type warnings\n",
    ")\n",
    "\n",
    "# ‚úÖ Inspect the first few rows\n",
    "print(\"First 5 rows of Dublin Airport Hourly Data:\")\n",
    "display(df.head())\n",
    "\n",
    "# ‚úÖ Check column types and missing values\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df.info())\n",
    "\n",
    "# ‚úÖ Quick statistical summary\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292af427",
   "metadata": {},
   "source": [
    "### üìÅ Step 5 ‚Äì Save the Cleaned Dublin Airport Hourly Data CSV\n",
    "\n",
    "After detecting the correct header row in the raw Met √âireann dataset, we now save a **cleaned version** of the Dublin Airport Hourly Data file into the project‚Äôs `data/` folder.  \n",
    "\n",
    "This step ensures:\n",
    "\n",
    "- üìÇ The dataset is stored locally for reuse without needing to re-download from Met √âireann each time  \n",
    "- üìë All future analysis references a consistent, structured version of the data (starting at the correct header row)  \n",
    "- üîÑ The workflow remains reproducible and version-controlled, supporting transparent project documentation  \n",
    "- üõ†Ô∏è Analysts and reviewers can always work from the same baseline dataset, avoiding inconsistencies caused by raw file metadata  \n",
    "- ‚è±Ô∏è Hourly granularity is preserved, which is essential for aligning weather conditions with flight arrivals and departures  \n",
    "\n",
    "üìå *Why this matters:*  \n",
    "Saving cleaned hourly data locally is a best practice in data science. It guarantees consistency across runs, makes collaboration easier, and allows you to track changes over time.  \n",
    "For Dublin Airport analysis, hourly weather data provides the necessary detail to study how conditions at specific times affect flight operations, ensuring reproducibility and transparency in your rerouting and delay modelling work.  \n",
    "\n",
    "üìñ Reference:  \n",
    "- [GeeksforGeeks ‚Äì Explain Data Versioning](https://www.geeksforgeeks.org/machine-learning/explain-data-versioning/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fab2a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Saved cleaned climate data for Dublin Airport to: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\data\\dublin_airport_hourly.csv\n"
     ]
    }
   ],
   "source": [
    "# üìÅ Step 5 ‚Äì Save the Cleaned CSV File\n",
    "\n",
    "# --- Ensure 'data' folder exists ---\n",
    "DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Save cleaned data starting from the detected header row ---\n",
    "with open(DATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines[header_index:]:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# ‚úÖ Confirm save location\n",
    "print(f\"üìÅ Saved cleaned climate data for Dublin Airport to: {DATA_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe3644",
   "metadata": {},
   "source": [
    "### üìÇ Step 6 ‚Äì Validate Saved CSV Against Step‚ÄØ4 Output\n",
    "\n",
    "Instead of re‚Äëprinting the same inspection results, this step **confirms that the locally saved CSV file is identical to the hourly dataset inspected in Step‚ÄØ4**.  \n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üìÇ Reloading the locally saved CSV (`data/dublin_airport_hourly.csv`)  \n",
    "- üåê Reloading the online CSV directly from Met √âireann (skipping metadata lines)  \n",
    "- ‚úÖ Comparing the two DataFrames with `equals()` to check for exact match  \n",
    "- üìä Printing a simple confirmation message and shape comparison  \n",
    "\n",
    "üìå *Why this matters:* This validation ensures reproducibility. It proves that the cleaned hourly file saved in Step‚ÄØ5 is a faithful copy of the dataset originally inspected in Step‚ÄØ4.  \n",
    "Reviewers can trust that all downstream analysis is based on the same consistent dataset.  \n",
    "For Dublin Airport analysis, this step is especially important because **hourly granularity** is required to align weather conditions with arrivals and departures at specific times.  \n",
    "\n",
    "üìñ Reference:  \n",
    "- [GeeksforGeeks ‚Äì Create Effective and Reproducible Code Using Pandas](https://www.geeksforgeeks.org/create-effective-and-reproducible-code-using-pandas/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53874e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Validation successful: Local CSV matches the online dataset from Step 4.\n",
      "Local shape: (708577, 21), Online shape: (708577, 21)\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 6 ‚Äì Validate Saved CSV Against Step 4 Output\n",
    "\n",
    "# --- Reload the locally saved CSV (hourly Dublin Airport data) ---\n",
    "df_local = pd.read_csv(\"data/dublin_airport_hourly.csv\", low_memory=False)\n",
    "\n",
    "# --- Reload the online CSV (using header_index from Step 4) ---\n",
    "df_online = pd.read_csv(\n",
    "    \"https://cli.fusio.net/cli/climate_data/webdata/hly532.csv\",\n",
    "    skiprows=header_index,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# ‚úÖ Compare the two DataFrames\n",
    "if df_local.equals(df_online):\n",
    "    print(\"‚úÖ Validation successful: Local CSV matches the online dataset from Step 4.\")\n",
    "else:\n",
    "    print(\"‚ùå Validation failed: Local CSV differs from the online dataset.\")\n",
    "\n",
    "# Optional: show shape comparison\n",
    "print(f\"Local shape: {df_local.shape}, Online shape: {df_online.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b75c65",
   "metadata": {},
   "source": [
    "### üìä Step 7 ‚Äì Enhance Dublin Airport Hourly Weather Data with Seasons\n",
    "\n",
    "After validating the saved hourly dataset, this step enriches the data by preparing timestamps, cleaning numeric weather columns, and tagging each record with its Irish meteorological season.  \n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üìÇ Loading the locally saved hourly CSV (`data/dublin_airport_hourly.csv`)  \n",
    "- üïí Preparing a full `datetime` column by combining `date` and `time`  \n",
    "- üõ†Ô∏è Cleaning mixed‚Äëtype weather columns (`rain`, `temp`, `wdsp`, `msl`) into consistent numeric values  \n",
    "- üçÇ Adding a `season` column using the `get_season_for_date` helper  \n",
    "\n",
    "üìå *Why this matters:*  \n",
    "By making the dataset **season‚Äëaware**, you can easily filter and analyse weather conditions and flight delays by season. This ensures that downstream analysis captures both the **hourly granularity** and the **seasonal context**, which are critical for understanding operational impacts at Dublin Airport.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da9f431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows with season tagging:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>msl</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1945-01-01 00:00:00</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1035.8</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1945-01-01 01:00:00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1035.8</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1945-01-01 02:00:00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1035.8</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1945-01-01 03:00:00</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1036.1</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1945-01-01 04:00:00</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1036.2</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1945-01-01 05:00:00</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1035.9</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1945-01-01 06:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1035.9</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1945-01-01 07:00:00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9</td>\n",
       "      <td>1036.1</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1945-01-01 08:00:00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1036.5</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1945-01-01 09:00:00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1037.3</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  temp  rain  wdsp     msl  season\n",
       "0 1945-01-01 00:00:00   4.9   0.0     0  1035.8  Winter\n",
       "1 1945-01-01 01:00:00   5.1   0.0     0  1035.8  Winter\n",
       "2 1945-01-01 02:00:00   5.1   0.0     0  1035.8  Winter\n",
       "3 1945-01-01 03:00:00   5.2   0.2     0  1036.1  Winter\n",
       "4 1945-01-01 04:00:00   5.6   0.0     7  1036.2  Winter\n",
       "5 1945-01-01 05:00:00   5.6   0.0     9  1035.9  Winter\n",
       "6 1945-01-01 06:00:00   6.0   0.0     9  1035.9  Winter\n",
       "7 1945-01-01 07:00:00   6.1   0.1     9  1036.1  Winter\n",
       "8 1945-01-01 08:00:00   6.1   0.0     9  1036.5  Winter\n",
       "9 1945-01-01 09:00:00   6.1   0.0     5  1037.3  Winter"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìä Step 7 ‚Äì Enhance Dublin Airport Hourly Weather Data with Seasons\n",
    "\n",
    "# --- Load the locally saved hourly CSV ---\n",
    "df_weather = load_cleaned_weather_data()\n",
    "\n",
    "# --- Prepare datetime column (parse combined date+time) ---\n",
    "df_weather = prepare_weather_data(df_weather)\n",
    "\n",
    "# --- Clean numeric weather columns (rain, temp, wind, pressure) ---\n",
    "df_weather = clean_weather_columns(df_weather)\n",
    "\n",
    "# --- Vectorized season assignment ---\n",
    "df_weather = assign_season_vectorized(df_weather)\n",
    "\n",
    "# ‚úÖ Inspect result\n",
    "print(\"First 10 rows with season tagging:\")\n",
    "display(df_weather[['datetime', 'temp', 'rain', 'wdsp', 'msl', 'season']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af503292",
   "metadata": {},
   "source": [
    "### üìë Step 8 ‚Äì Download and Save Flight Activity Data\n",
    "\n",
    "In this step, the notebook retrieves and stores **flight activity data** for Dublin Airport.  \n",
    "This dataset will later be aligned with Met √âireann weather observations to analyse how conditions such as rain, wind, and visibility impact flight punctuality.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üåê Collecting flight schedules and activity logs (arrivals, departures, delays, cancellations) from public APIs or dashboards  \n",
    "- üìÇ Defining a local output path (`data/dublin_airport_flights.csv`) to store the file inside the project‚Äôs `data` folder  \n",
    "- ‚úÖ Checking the response to ensure the download or export was successful  \n",
    "- üìë Parsing the raw data into a structured format, including scheduled vs actual times and delay minutes  \n",
    "- üìÅ Saving a cleaned version of the dataset locally for reproducibility and future analysis  \n",
    "\n",
    "üìå *Why this matters:* Having flight activity data stored locally ensures that the project can consistently align flight events with weather conditions.  \n",
    "It also supports reproducibility, version control, and enables predictive modelling of delays and cancellations without repeatedly querying external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d600a9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-05-21 to 2025-11-19\n"
     ]
    }
   ],
   "source": [
    "# üìë Step 8 ‚Äì Download and Save Flight Activity Data\n",
    "# --- Compute date range for the past six months ---\n",
    "today = date.today()\n",
    "six_months_ago = today - timedelta(days=182)  # approx 6 months\n",
    "\n",
    "DATE_FROM = six_months_ago.isoformat()\n",
    "DATE_TO = today.isoformat()\n",
    "\n",
    "# --- Output directories ---\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DIR = DATA_DIR / \"raw_flights\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Date range: {DATE_FROM} to {DATE_TO}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79b275",
   "metadata": {},
   "source": [
    "### üìë Step 9 ‚Äì Dublin Airport flight information analysis \n",
    "\n",
    "This cell prepares the environment for **Dublin Airport flight information analysis** by defining key date ranges and output directories:\n",
    "\n",
    "- üóìÔ∏è **Date range:**  \n",
    "  - Calculates today‚Äôs date and subtracts ~six months (182 days) to define the analysis window.  \n",
    "  - Converts both dates into ISO format (`YYYY-MM-DD`) for use in API queries.  \n",
    "  - These values (`DATE_FROM`, `DATE_TO`) specify the six‚Äëmonth period of **flight activity data** (arrivals, departures, delays, cancellations) to be downloaded.\n",
    "\n",
    "- üìÇ **Output directories:**  \n",
    "  - Creates a root `data/` folder for project storage.  \n",
    "  - Inside it, a `raw_flights/` subfolder is created to hold raw JSON files retrieved from the Aviation Edge API.  \n",
    "  - This ensures reproducibility and a clear separation between raw flight inputs and processed datasets.\n",
    "\n",
    "- ‚úÖ **Checkpoint:**  \n",
    "  - Prints the computed date range so you can confirm the correct six‚Äëmonth window before downloading flight information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a82153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-05-21 to 2025-11-19\n"
     ]
    }
   ],
   "source": [
    "# üìë Step 9 ‚Äì Dublin Airport flight information analysis \n",
    "# --- Compute date range for the past six months ---\n",
    "today = date.today()\n",
    "six_months_ago = today - timedelta(days=182)  # approx 6 months\n",
    "\n",
    "DATE_FROM = six_months_ago.isoformat()\n",
    "DATE_TO = today.isoformat()\n",
    "\n",
    "# --- Output directories ---\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DIR = DATA_DIR / \"raw_flights\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Date range: {DATE_FROM} to {DATE_TO}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb27b0",
   "metadata": {},
   "source": [
    "### ‚úàÔ∏è Step 10 ‚Äî Download Six Months of Flight History for Dublin (Arrivals and Departures)\n",
    "\n",
    "In this step we use **Aviation Edge‚Äôs Flights History API** to collect six months of flight schedules for Dublin Airport (IATA: DUB).  \n",
    "The endpoint provides detailed records for each flight, including:\n",
    "\n",
    "- **Scheduled, estimated, and actual times** (departure and arrival)\n",
    "- **Delay minutes** (either reported or inferred)\n",
    "- **Flight status** (e.g., scheduled, landed, cancelled, diverted)\n",
    "- **Airline and flight identifiers**\n",
    "\n",
    "We request **both arrivals and departures** for the date range **2025‚Äë05‚Äë20 to 2025‚Äë11‚Äë18**, ensuring coverage of the most recent six months.  \n",
    "The raw JSON files are saved for reproducibility in the folder:\n",
    "\n",
    "- `data/raw_flights/dub_arrival_history.json`  \n",
    "- `data/raw_flights/dub_departure_history.json`\n",
    "\n",
    "Additionally, a `fetch_log.txt` file is generated to record progress, errors, and confirmation of successful downloads.  \n",
    "This log provides transparency and makes troubleshooting easier if API requests fail or return incomplete data.\n",
    "\n",
    "**Important notes for reproducibility:**\n",
    "- The code cell was executed on **18 November 2025** using a private API key from Aviation Edge.\n",
    "- To run the download yourself, you must:\n",
    "  1. Sign up for an account at [aviation-edge.com](https://aviation-edge.com/) and obtain an API key.\n",
    "  2. Store the key securely (e.g., as an environment variable).\n",
    "  3. Set the notebook control flag `RUN_DOWNLOAD = True` to enable downloading.\n",
    "- By default, the notebook will skip downloading if `RUN_DOWNLOAD = False`, and instead use the existing JSON files.  \n",
    "  This prevents unnecessary API calls and ensures consistent results for reviewers.\n",
    "\n",
    "‚ö†Ô∏è **Best practice:** Only re‚Äërun the download when you want to refresh the dataset.  \n",
    "Frequent downloads are unnecessary and may exceed API rate limits.\n",
    "\n",
    "**References:**\n",
    "- [Aviation Edge official site](https://aviation-edge.com/)  \n",
    "- [Aviation Edge API documentation on GitHub](https://github.com/AviationEdgeAPI/Aviation-Edge-Complete-API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c68251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è© Skipping download step (RUN_DOWNLOAD=False). Using existing JSON files.\n"
     ]
    }
   ],
   "source": [
    "# ‚úàÔ∏è Step 10 ‚Äî Download Six Months of Flight History for Dublin (Arrivals and Departures)\n",
    "# --- API setup ---\n",
    "API_KEY = os.getenv(\"AVIATION_EDGE_API_KEY\")   # Read API key from environment variable\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"API key not found. Please set AVIATION_EDGE_API_KEY.\")\n",
    "\n",
    "BASE_URL = \"https://aviation-edge.com/v2/public/flightsHistory\"  # Endpoint for flight history\n",
    "IATA_CODE = \"DUB\"  # Airport code for Dublin\n",
    "\n",
    "# --- Directory setup ---\n",
    "DATA_DIR = Path(\"data\")              # Root data folder\n",
    "RAW_DIR = DATA_DIR / \"raw_flights\"   # Subfolder for raw flight data\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)  # Create folders if missing\n",
    "\n",
    "# --- Log file path ---\n",
    "LOG_FILE = RAW_DIR / \"fetch_log.txt\"  # Text log for progress and errors\n",
    "\n",
    "def log_message(message: str):\n",
    "    \"\"\"Print message and append to log file for tracking progress.\"\"\"\n",
    "    print(message)\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(message + \"\\n\")\n",
    "\n",
    "def fetch_day(iata_code: str, flight_type: str, day: date, retries: int = 3):\n",
    "    \"\"\"\n",
    "    Fetch flight history for a single day (arrival/departure).\n",
    "    Retries up to 'retries' times if errors occur.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"key\": API_KEY,\n",
    "        \"code\": iata_code,\n",
    "        \"type\": flight_type,\n",
    "        \"date_from\": day.isoformat(),\n",
    "        \"date_to\": day.isoformat()\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        resp = requests.get(BASE_URL, params=params, timeout=60)  # API request\n",
    "        if resp.status_code == 200:\n",
    "            try:\n",
    "                data = resp.json()  # Parse JSON response\n",
    "                log_message(f\"‚úÖ {flight_type.capitalize()} {day}: {len(data)} records fetched\")\n",
    "                return data\n",
    "            except Exception:\n",
    "                log_message(f\"‚ö†Ô∏è Non-JSON response on {day}: {resp.text[:200]}\")\n",
    "                return []\n",
    "        else:\n",
    "            wait = 2 ** attempt  # Exponential backoff\n",
    "            log_message(f\"‚ö†Ô∏è Error {resp.status_code} on {day} (attempt {attempt+1}/{retries}). Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    log_message(f\"‚ùå Failed after {retries} retries on {day}\")\n",
    "    return []\n",
    "\n",
    "def fetch_history(iata_code: str, flight_type: str, start_date: date, end_date: date):\n",
    "    \"\"\"\n",
    "    Loop through each day in the date range and fetch daily history.\n",
    "    Append results into one cumulative JSON file (avoids overwriting with empty data).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_days = (end_date - start_date).days + 1\n",
    "    filename = RAW_DIR / f\"{iata_code.lower()}_{flight_type}_history.json\"\n",
    "\n",
    "    # Load existing cumulative file if present\n",
    "    if filename.exists():\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                results = json.load(f)\n",
    "            except Exception:\n",
    "                results = []\n",
    "                log_message(f\"‚ö†Ô∏è Existing {filename.name} could not be read, starting fresh.\")\n",
    "\n",
    "    # Loop through each day in range\n",
    "    for i in range(total_days):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        log_message(f\"Day {i+1}/{total_days}: {day}\")\n",
    "        day_data = fetch_day(iata_code, flight_type, day)\n",
    "\n",
    "        # Save only if data was fetched\n",
    "        if day_data:\n",
    "            results.extend(day_data)\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            log_message(f\"üíæ Saved {len(day_data)} records for {day} into {filename.name}\")\n",
    "        else:\n",
    "            log_message(f\"‚è© Skipped saving {day}, no data returned\")\n",
    "\n",
    "        time.sleep(1)  # Pause politely between requests\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Conditional download control ---\n",
    "if RUN_DOWNLOAD:\n",
    "    # Define date range (last ~6 months)\n",
    "    today = date.today()\n",
    "    start_date = today - timedelta(days=182)\n",
    "    end_date = today\n",
    "\n",
    "    log_message(f\"Fetching flights from {start_date} to {end_date} for {IATA_CODE}...\")\n",
    "\n",
    "    # Fetch arrivals and departures\n",
    "    arrivals = fetch_history(IATA_CODE, \"arrival\", start_date, end_date)\n",
    "    departures = fetch_history(IATA_CODE, \"departure\", start_date, end_date)\n",
    "\n",
    "    log_message(f\"‚úÖ Completed: {len(arrivals)} arrivals and {len(departures)} departures fetched.\")\n",
    "else:\n",
    "    log_message(\"‚è© Skipping download step (RUN_DOWNLOAD=False). Using existing JSON files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a966b",
   "metadata": {},
   "source": [
    "### üìÇ Step 11 ‚Äì Inspect Headings of Downloaded JSON Files\n",
    "\n",
    "Before tidying the flight history data, it‚Äôs important to **inspect the structure of the raw JSON files**.  \n",
    "The Aviation Edge API responses can vary depending on whether the file contains arrivals or departures, and not all fields are always present.\n",
    "\n",
    "**What this step does:**\n",
    "- Loads the raw JSON files for Dublin Airport arrivals (`dub_arrival_history.json`) and departures (`dub_departure_history.json`).\n",
    "- Uses `pandas.json_normalize` to flatten the nested JSON into a tabular structure.\n",
    "- Prints out all available column headings so we can see which fields exist.\n",
    "- Shows a sample record (truncated for readability) to preview the nested structure.\n",
    "\n",
    "**Why this matters:**\n",
    "- Helps identify which fields are consistently available and relevant for analysis.\n",
    "- Prevents errors later by ensuring we only select columns that actually exist.\n",
    "- Guides the design of the tidy DataFrame schema in Step‚ÄØ11 (e.g. keeping scheduled/actual times, delays, status, airline, etc., while dropping baggage or codeshare metadata).\n",
    "\n",
    "üëâ This inspection step is a diagnostic tool: it gives us visibility into the raw data so we can confidently build the parsing logic in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ff4cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Keys in dub_arrival_history.json ---\n",
      "['airline.iataCode', 'airline.icaoCode', 'airline.name', 'arrival.actualRunway', 'arrival.actualTime', 'arrival.baggage', 'arrival.delay', 'arrival.estimatedRunway', 'arrival.estimatedTime', 'arrival.gate', 'arrival.iataCode', 'arrival.icaoCode', 'arrival.scheduledTime', 'arrival.terminal', 'codeshared.airline.iataCode', 'codeshared.airline.icaoCode', 'codeshared.airline.name', 'codeshared.flight.iataNumber', 'codeshared.flight.icaoNumber', 'codeshared.flight.number', 'departure.actualRunway', 'departure.actualTime', 'departure.delay', 'departure.estimatedRunway', 'departure.estimatedTime', 'departure.gate', 'departure.iataCode', 'departure.icaoCode', 'departure.scheduledTime', 'departure.terminal', 'flight.iataNumber', 'flight.icaoNumber', 'flight.number', 'status', 'type']\n",
      "\n",
      "Sample record:\n",
      "{\n",
      "  \"type\": \"arrival\",\n",
      "  \"status\": \"landed\",\n",
      "  \"departure\": {\n",
      "    \"iataCode\": \"vlc\",\n",
      "    \"icaoCode\": \"levc\",\n",
      "    \"terminal\": \"1\",\n",
      "    \"gate\": \"3\",\n",
      "    \"delay\": 49,\n",
      "    \"scheduledTime\": \"2025-05-19t23:10:00.000\",\n",
      "    \"estimatedTime\": \"2025-05-19t23:10:00.000\",\n",
      "    \"actualTime\": \"2025-05-19t23:58:00.000\",\n",
      "    \"estimatedRunway\": \"2025-05-19t23:58:00.000\",\n",
      "    \"actualRunway\": \"2025-05-19t23:58:00.000\"\n",
      "  },\n",
      "  \"arrival\": {\n",
      "    \"iataCode\": \"dub\",\n",
      "    \"icaoCode\": \"eidw\",\n",
      "    \"terminal\": \"t1\",\n",
      "    \"bagga\n",
      "\n",
      "--- Keys in dub_departure_history.json ---\n",
      "['airline.iataCode', 'airline.icaoCode', 'airline.name', 'arrival.actualRunway', 'arrival.actualTime', 'arrival.baggage', 'arrival.delay', 'arrival.estimatedRunway', 'arrival.estimatedTime', 'arrival.gate', 'arrival.iataCode', 'arrival.icaoCode', 'arrival.scheduledTime', 'arrival.terminal', 'codeshared.airline.iataCode', 'codeshared.airline.icaoCode', 'codeshared.airline.name', 'codeshared.flight.iataNumber', 'codeshared.flight.icaoNumber', 'codeshared.flight.number', 'departure.actualRunway', 'departure.actualTime', 'departure.delay', 'departure.estimatedRunway', 'departure.estimatedTime', 'departure.gate', 'departure.iataCode', 'departure.icaoCode', 'departure.scheduledTime', 'departure.terminal', 'flight.iataNumber', 'flight.icaoNumber', 'flight.number', 'status', 'type']\n",
      "\n",
      "Sample record:\n",
      "{\n",
      "  \"type\": \"departure\",\n",
      "  \"status\": \"active\",\n",
      "  \"departure\": {\n",
      "    \"iataCode\": \"dub\",\n",
      "    \"icaoCode\": \"eidw\",\n",
      "    \"delay\": 5,\n",
      "    \"scheduledTime\": \"2025-05-20t02:20:00.000\",\n",
      "    \"estimatedTime\": \"2025-05-20t02:31:00.000\",\n",
      "    \"actualTime\": \"2025-05-20t02:24:00.000\",\n",
      "    \"estimatedRunway\": \"2025-05-20t02:24:00.000\",\n",
      "    \"actualRunway\": \"2025-05-20t02:24:00.000\"\n",
      "  },\n",
      "  \"arrival\": {\n",
      "    \"iataCode\": \"fra\",\n",
      "    \"icaoCode\": \"eddf\",\n",
      "    \"scheduledTime\": \"2025-05-20t05:15:00.000\",\n",
      "    \"estimatedTime\": \n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 11 - inspect headings of downloaded JSON files\n",
    "RAW_DIR = Path(\"data\") / \"raw_flights\"\n",
    "ARR_FILE = RAW_DIR / \"dub_arrival_history.json\"\n",
    "DEP_FILE = RAW_DIR / \"dub_departure_history.json\"\n",
    "\n",
    "def inspect_keys(json_file, sample_size=50):\n",
    "    \"\"\"Inspect nested keys in a JSON file by sampling records.\"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    # Use pandas.json_normalize to flatten structure\n",
    "    import pandas as pd\n",
    "    df = pd.json_normalize(records)\n",
    "\n",
    "    # Show all column headings\n",
    "    print(f\"\\n--- Keys in {json_file.name} ---\")\n",
    "    print(sorted(df.columns.tolist()))\n",
    "\n",
    "    # Optionally preview first record\n",
    "    print(\"\\nSample record:\")\n",
    "    print(json.dumps(records[0], indent=2)[:500])  # truncate for readability\n",
    "\n",
    "# Inspect both files\n",
    "inspect_keys(ARR_FILE)\n",
    "inspect_keys(DEP_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907d287",
   "metadata": {},
   "source": [
    "### üìÇ Step 12 ‚Äì Parse Flight History JSON into Tidy DataFrames\n",
    "\n",
    "This step takes the raw JSON files downloaded from the Aviation Edge API  \n",
    "(`dub_arrival_history.json` and `dub_departure_history.json`) and converts them into clean, analysis‚Äëready DataFrames.\n",
    "\n",
    "**What the code does:**\n",
    "- Loads the raw JSON files for arrivals and departures.\n",
    "- Flattens the nested JSON structure into tabular form using `pandas.json_normalize`.\n",
    "- Keeps only the **relevant fields** for weather analysis:\n",
    "  - Flight number (`flight_iata`)\n",
    "  - Airline name\n",
    "  - Flight status (landed, cancelled, active, etc.)\n",
    "  - Scheduled, estimated, and actual times\n",
    "  - Delay (either reported by API or calculated if missing)\n",
    "  - Optional operational details (terminal, runway)\n",
    "- Converts timestamps into proper datetime objects.\n",
    "- Derives `date` and `time` columns to align flights with hourly weather data.\n",
    "- Adds a `type` column to distinguish arrivals vs departures.\n",
    "- Returns three DataFrames:\n",
    "  - `df_arrivals` ‚Üí tidy arrivals\n",
    "  - `df_departures` ‚Üí tidy departures\n",
    "  - `df_all` ‚Üí combined dataset of both arrivals and departures\n",
    "\n",
    "**Why this matters:**\n",
    "- The raw JSON files are large and verbose; this step filters them down to only the fields needed for analysis.\n",
    "- The tidy DataFrames provide a consistent schema that can be merged directly with weather observations.\n",
    "- Having both separate and combined views makes it easy to analyse arrivals and departures independently or together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1522b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrivals shape: (131556, 14)\n",
      "Departures shape: (137720, 14)\n",
      "Combined shape: (269276, 14)\n",
      "  flight_iata            airline  status               sched  \\\n",
      "0      fr1739            ryanair  landed 2025-05-20 01:00:00   \n",
      "1      fr9612            ryanair  landed 2025-05-20 01:10:00   \n",
      "2       fr651            ryanair  landed 2025-05-20 01:15:00   \n",
      "3      aa8330  american airlines  landed 2025-05-20 04:25:00   \n",
      "4      ba6124    british airways  landed 2025-05-20 04:25:00   \n",
      "\n",
      "                  est                 act  delay terminal  \\\n",
      "0 2025-05-20 01:15:00 2025-05-20 01:15:00   15.0       t1   \n",
      "1 2025-05-20 01:11:00 2025-05-20 01:03:00    NaN       t1   \n",
      "2 2025-05-20 01:12:00 2025-05-20 01:05:00    NaN       t1   \n",
      "3 2025-05-20 03:39:00 2025-05-20 03:39:00    NaN       t2   \n",
      "4 2025-05-20 03:39:00 2025-05-20 03:39:00    NaN       t2   \n",
      "\n",
      "                    runway  delay_calc        date   time  is_cancelled  \\\n",
      "0  2025-05-20t01:15:00.000        15.0  2025-05-20  01:00         False   \n",
      "1  2025-05-20t01:03:00.000         NaN  2025-05-20  01:10         False   \n",
      "2  2025-05-20t01:05:00.000         NaN  2025-05-20  01:15         False   \n",
      "3  2025-05-20t03:39:00.000         NaN  2025-05-20  04:25         False   \n",
      "4  2025-05-20t03:39:00.000         NaN  2025-05-20  04:25         False   \n",
      "\n",
      "      type  \n",
      "0  arrival  \n",
      "1  arrival  \n",
      "2  arrival  \n",
      "3  arrival  \n",
      "4  arrival  \n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 12 ‚Äì Parse Flight History JSON into Tidy DataFrames\n",
    "# NOTE: Rewritten to avoid pandas.json_normalize issues (e.g. nested/circular structures).\n",
    "#       We safely extract nested fields and build a flat DataFrame.\n",
    "\n",
    "RAW_DIR = Path(\"data\") / \"raw_flights\"\n",
    "ARR_FILE = RAW_DIR / \"dub_arrival_history.json\"\n",
    "DEP_FILE = RAW_DIR / \"dub_departure_history.json\"\n",
    "\n",
    "def _safe_get(d, path):\n",
    "    \"\"\"Get nested value from dict using dotted path, return None if missing.\"\"\"\n",
    "    if d is None:\n",
    "        return None\n",
    "    parts = path.split(\".\")\n",
    "    val = d\n",
    "    for p in parts:\n",
    "        if isinstance(val, dict) and p in val:\n",
    "            val = val[p]\n",
    "        else:\n",
    "            return None\n",
    "    return val\n",
    "\n",
    "def parse_flights(json_file, flight_type=\"arrival\"):\n",
    "    \"\"\"\n",
    "    Load a JSON file (arrivals or departures) and return a tidy DataFrame.\n",
    "    Safely extracts nested fields to avoid json_normalize edge-cases.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    if not records:  # safeguard against empty files\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Define mapping: target_column -> dotted path in record\n",
    "    if flight_type == \"arrival\":\n",
    "        mapping = {\n",
    "            \"flight_iata\": \"flight.iataNumber\",\n",
    "            \"airline\": \"airline.name\",\n",
    "            \"status\": \"status\",\n",
    "            \"sched\": \"arrival.scheduledTime\",\n",
    "            \"est\": \"arrival.estimatedTime\",\n",
    "            \"act\": \"arrival.actualTime\",\n",
    "            \"delay\": \"arrival.delay\",\n",
    "            \"terminal\": \"arrival.terminal\",\n",
    "            \"runway\": \"arrival.actualRunway\",\n",
    "        }\n",
    "    else:\n",
    "        mapping = {\n",
    "            \"flight_iata\": \"flight.iataNumber\",\n",
    "            \"airline\": \"airline.name\",\n",
    "            \"status\": \"status\",\n",
    "            \"sched\": \"departure.scheduledTime\",\n",
    "            \"est\": \"departure.estimatedTime\",\n",
    "            \"act\": \"departure.actualTime\",\n",
    "            \"delay\": \"departure.delay\",\n",
    "            \"terminal\": \"departure.terminal\",\n",
    "            \"runway\": \"departure.actualRunway\",\n",
    "        }\n",
    "\n",
    "    rows = []\n",
    "    for rec in records:\n",
    "        row = {}\n",
    "        for col, path in mapping.items():\n",
    "            row[col] = _safe_get(rec, path)\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Parse timestamps into datetimes\n",
    "    for c in [\"sched\", \"est\", \"act\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Compute delay_calc: prefer explicit 'delay' if present, else compute from times\n",
    "    if \"delay\" not in df.columns or df[\"delay\"].isna().all():\n",
    "        # compute from sched/act where possible\n",
    "        df[\"delay_calc\"] = (df[\"act\"] - df[\"sched\"]).dt.total_seconds() / 60.0\n",
    "    else:\n",
    "        df[\"delay_calc\"] = pd.to_numeric(df[\"delay\"], errors=\"coerce\")\n",
    "\n",
    "    # Merge keys for weather alignment\n",
    "    if \"sched\" in df.columns:\n",
    "        df[\"date\"] = df[\"sched\"].dt.date\n",
    "        df[\"time\"] = df[\"sched\"].dt.strftime(\"%H:%M\")\n",
    "    else:\n",
    "        df[\"date\"] = pd.NaT\n",
    "        df[\"time\"] = None\n",
    "\n",
    "    df[\"is_cancelled\"] = df.get(\"status\", pd.Series(dtype=\"object\")).astype(str).str.lower().eq(\"cancelled\")\n",
    "    df[\"type\"] = flight_type  # add explicit type column\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Load both arrivals and departures ---\n",
    "df_arrivals = parse_flights(ARR_FILE, \"arrival\")\n",
    "df_departures = parse_flights(DEP_FILE, \"departure\")\n",
    "\n",
    "# --- Optionally combine into one DataFrame ---\n",
    "df_all = pd.concat([df_arrivals, df_departures], ignore_index=True, sort=False)\n",
    "\n",
    "print(\"Arrivals shape:\", df_arrivals.shape)\n",
    "print(\"Departures shape:\", df_departures.shape)\n",
    "print(\"Combined shape:\", df_all.shape)\n",
    "\n",
    "print(df_all.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef302a60",
   "metadata": {},
   "source": [
    "### üìÇ Step 13 ‚Äì Save Tidy Flight DataFrames\n",
    "\n",
    "In this step, the cleaned flight DataFrames created in Step‚ÄØ11 are written out to disk.  \n",
    "Saving them ensures that the tidy, analysis‚Äëready datasets are preserved and can be reused without re‚Äëparsing the raw JSON files.\n",
    "\n",
    "**What is saved:**\n",
    "- `dub_arrivals_tidy.csv` ‚Üí arrivals into Dublin Airport, with relevant fields (flight number, airline, status, times, delay, etc.).\n",
    "- `dub_departures_tidy.csv` ‚Üí departures from Dublin Airport, with the same tidy structure.\n",
    "- `dub_flights_tidy.csv` ‚Üí combined dataset containing both arrivals and departures, distinguished by a `type` column.\n",
    "\n",
    "**Why this matters:**\n",
    "- The tidy CSVs are much smaller and easier to work with than the raw JSON files.\n",
    "- They provide a consistent schema for merging with weather data (using `date` and `time`).\n",
    "- Reviewers and collaborators can immediately use these files without needing to rerun the full API download.\n",
    "\n",
    "üëâ This step finalises the preprocessing pipeline: raw JSON ‚Üí tidy DataFrames ‚Üí reusable CSVs in the `data/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac5d01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved tidy datasets into data/ folder\n"
     ]
    }
   ],
   "source": [
    "# üìÇ Step 13 ‚Äì Save Tidy Flight DataFrames\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# Save arrivals\n",
    "df_arrivals.to_csv(DATA_DIR / \"dub_arrivals_tidy.csv\", index=False)\n",
    "\n",
    "# Save departures\n",
    "df_departures.to_csv(DATA_DIR / \"dub_departures_tidy.csv\", index=False)\n",
    "\n",
    "# Save combined dataset\n",
    "df_all.to_csv(DATA_DIR / \"dub_flights_tidy.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Saved tidy datasets into data/ folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669cc69",
   "metadata": {},
   "source": [
    "### üå¶Ô∏è Step 14 ‚Äì Define Meteorological Seasons in Ireland for 2025 (Relevant to Dublin Airport Temperature Analysis)\n",
    "\n",
    "Ireland‚Äôs meteorological seasons follow a fixed calendar pattern, as outlined by Met √âireann:\n",
    "\n",
    "- **Winter**: 1 December (previous year) to 28 February  \n",
    "- **Spring**: 1 March to 31 May  \n",
    "- **Summer**: 1 June to 31 August  \n",
    "- **Autumn**: 1 September to 30 November  \n",
    "\n",
    "In this step, we use a helper function to define seasonal boundaries for 2025. These boundaries will later help us filter and analyse temperature data by season.\n",
    "\n",
    "üìå *Reference:* [Met √âireann ‚Äì Irish Seasons](https://www.met.ie/education/outreach-irish-seasons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd4bc4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Irish Seasons for 2025:\n",
      "  Winter: 01-Dec-2024 00:00 ‚Üí 28-Feb-2025 23:59\n",
      "  Spring: 01-Mar-2025 00:00 ‚Üí 31-May-2025 23:59\n",
      "  Summer: 01-Jun-2025 00:00 ‚Üí 31-Aug-2025 23:59\n",
      "  Autumn: 01-Sep-2025 00:00 ‚Üí 30-Nov-2025 23:59\n"
     ]
    }
   ],
   "source": [
    "# üå¶Ô∏è Step 14 ‚Äì Define Meteorological Seasons in Ireland for 2025\n",
    "\n",
    "# --- Generate seasonal boundaries using helper function ---\n",
    "seasons_2025 = define_irish_seasons()\n",
    "\n",
    "# --- Display formatted season ranges ---\n",
    "print(\"üìÖ Irish Seasons for 2025:\")\n",
    "for _, row in seasons_2025.iterrows():\n",
    "    print(f\"  {row['season']}: {row['start'].strftime('%d-%b-%Y %H:%M')} ‚Üí {row['end'].strftime('%d-%b-%Y %H:%M')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3df91",
   "metadata": {},
   "source": [
    "### üìÜ Step 15 ‚Äì Define and Validate a Custom Date Range (Dublin Weather)\n",
    "\n",
    "In this step, we define a **custom date range** for Dublin Airport hourly weather data and validate it against the seasonal boundaries.  \n",
    "This ensures that the selected range is valid, reproducible, and contextually meaningful for downstream analysis.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üìÖ Using the `get_custom_range` helper to parse start and end dates from user input  \n",
    "- ‚ö†Ô∏è Falling back to default values if the input is invalid or cannot be parsed  \n",
    "- üçÇ Checking whether the custom range falls entirely within a single Irish meteorological season  \n",
    "- üïí Preparing the `df_weather` dataset with a full `datetime` column for accurate filtering  \n",
    "- üîç Filtering `df_weather` to include only rows within the validated custom range  \n",
    "\n",
    "üìå *Why this matters:*  \n",
    "Validating custom ranges ensures that analysis is seasonally consistent and avoids mixing weather effects across boundaries.  \n",
    "By confirming whether the range lies within a single season, we maintain clarity in interpretation.  \n",
    "Filtering the dataset to the exact range provides a focused subset of hourly Dublin Airport weather data, ready for alignment with arrivals and departures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5658dd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÜ The custom range falls entirely within: Autumn\n",
      "‚úÖ Filtered Dublin weather data contains 120 rows from 2025-10-27 ‚Üí 2025-10-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ind</th>\n",
       "      <th>rain</th>\n",
       "      <th>ind.1</th>\n",
       "      <th>temp</th>\n",
       "      <th>ind.2</th>\n",
       "      <th>wetb</th>\n",
       "      <th>dewpt</th>\n",
       "      <th>vappr</th>\n",
       "      <th>rhum</th>\n",
       "      <th>...</th>\n",
       "      <th>wddir</th>\n",
       "      <th>ww</th>\n",
       "      <th>w</th>\n",
       "      <th>sun</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>clamt</th>\n",
       "      <th>datetime</th>\n",
       "      <th>hour</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>708456</th>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-10-27 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708457</th>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>9.2</td>\n",
       "      <td>79</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-10-27 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708458</th>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>5.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>79</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>999</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-10-27 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708459</th>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>9.6</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>2025-10-27 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708460</th>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>9.2</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>999</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-10-27 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  ind  rain  ind.1  temp  ind.2  wetb  dewpt vappr rhum  \\\n",
       "708456  2025-10-27    0   0.0      0   9.4      0   8.0    6.4   9.6   81   \n",
       "708457  2025-10-27    0   0.0      0   9.2      0   7.6    5.8   9.2   79   \n",
       "708458  2025-10-27    0   0.0      0   9.1      0   7.5    5.6   9.1   79   \n",
       "708459  2025-10-27    0   0.0      0   9.4      0   8.0    6.4   9.6   81   \n",
       "708460  2025-10-27    0   0.0      0   8.9      0   7.5    5.9   9.2   81   \n",
       "\n",
       "        ...  wddir  ww   w  sun    vis  clht  clamt            datetime hour  \\\n",
       "708456  ...    260   2  11  0.0  20000    27      7 2025-10-27 00:00:00    0   \n",
       "708457  ...    270   2  11  0.0  20000    26      7 2025-10-27 01:00:00    1   \n",
       "708458  ...    270   2  11  0.0  20000   999      3 2025-10-27 02:00:00    2   \n",
       "708459  ...    270   2  11  0.0  20000    37      7 2025-10-27 03:00:00    3   \n",
       "708460  ...    270   2  11  0.0  20000   999      3 2025-10-27 04:00:00    4   \n",
       "\n",
       "        season  \n",
       "708456  Autumn  \n",
       "708457  Autumn  \n",
       "708458  Autumn  \n",
       "708459  Autumn  \n",
       "708460  Autumn  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìÜ Step 15 ‚Äì Define and Validate a Custom Date Range (Dublin Weather)\n",
    "\n",
    "# --- Define custom date range using helper ---\n",
    "custom_start, custom_end = get_custom_range(\"2025-10-27\", \"2025-10-31 23:59\")\n",
    "\n",
    "# --- Fallback to default if input is invalid ---\n",
    "if custom_start is None or custom_end is None:\n",
    "    print(\"‚ö†Ô∏è Invalid custom range returned by get_custom_range(); falling back to defaults.\")\n",
    "    custom_start = pd.Timestamp(\"2025-11-12\")\n",
    "    custom_end = pd.Timestamp(\"2025-11-16 23:59\")\n",
    "\n",
    "# --- Check which season the range falls into ---\n",
    "matched_season = None\n",
    "for _, row in seasons_2025.iterrows():\n",
    "    if row[\"start\"] <= custom_start <= row[\"end\"] and row[\"start\"] <= custom_end <= row[\"end\"]:\n",
    "        matched_season = row[\"season\"]\n",
    "        break\n",
    "\n",
    "# --- Display season match result ---\n",
    "if matched_season:\n",
    "    print(f\"üìÜ The custom range falls entirely within: {matched_season}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è The custom range spans multiple seasons or falls outside defined bounds.\")\n",
    "\n",
    "# --- Prepare filtered weather data for Dublin Airport ---\n",
    "# Ensure full datetime column first\n",
    "df_weather = prepare_datetime(df_weather, date_col='date', time_col='time')\n",
    "\n",
    "# Then filter using the validated custom range\n",
    "range_df = prepare_weather_data(df_weather)\n",
    "range_df = range_df[(range_df['datetime'] >= custom_start) & (range_df['datetime'] <= custom_end)]\n",
    "\n",
    "print(f\"‚úÖ Filtered Dublin weather data contains {len(range_df)} rows from {custom_start.date()} ‚Üí {custom_end.date()}\")\n",
    "display(range_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674175b",
   "metadata": {},
   "source": [
    "### üìä Step 16 ‚Äì Load and Clean Dublin Airport Flight Data\n",
    "\n",
    "In this step, we load the tidy flight datasets created earlier (arrivals, departures, or the combined file) and prepare them for analysis.  \n",
    "The goal is to ensure that the flight records have a consistent **datetime column** and clean numeric delay values, so they can be merged with weather data later.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- üìÇ Checking which tidy flight files are available in the `data/` folder  \n",
    "- üì• Loading the combined dataset if present, or concatenating arrivals and departures as fallback  \n",
    "- üßπ Normalising column names to lowercase and stripping whitespace  \n",
    "- üïí Creating a canonical `datetime` column based on the scheduled time (`sched`)  \n",
    "- üî¢ Converting delay fields into numeric values for analysis  \n",
    "- üö´ Dropping rows without valid datetime values  \n",
    "\n",
    "üìå *Why this matters:*  \n",
    "A clean and consistent flight dataset ensures reproducibility and makes it possible to align flights with weather observations.  \n",
    "By anchoring on the scheduled time (`sched`), we can reliably merge flights with hourly weather data in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e07a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Available files in data/: ['dublin_airport_hourly.csv', 'dub_arrivals_tidy.csv', 'dub_departures_tidy.csv', 'dub_flights_tidy.csv', 'raw_flights']\n",
      "‚úÖ Loaded flight dataset from: combined (269276 rows)\n",
      "üßπ Cleaned flight dataset has 269270 rows with valid datetime\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_iata</th>\n",
       "      <th>airline</th>\n",
       "      <th>status</th>\n",
       "      <th>sched</th>\n",
       "      <th>est</th>\n",
       "      <th>act</th>\n",
       "      <th>delay</th>\n",
       "      <th>terminal</th>\n",
       "      <th>runway</th>\n",
       "      <th>delay_calc</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>is_cancelled</th>\n",
       "      <th>type</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fr1739</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>landed</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>t1</td>\n",
       "      <td>2025-05-20t01:15:00.000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>01:00</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fr9612</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>landed</td>\n",
       "      <td>2025-05-20 01:10:00</td>\n",
       "      <td>2025-05-20 01:11:00</td>\n",
       "      <td>2025-05-20 01:03:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t1</td>\n",
       "      <td>2025-05-20t01:03:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>01:10</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 01:10:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fr651</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>landed</td>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "      <td>2025-05-20 01:12:00</td>\n",
       "      <td>2025-05-20 01:05:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t1</td>\n",
       "      <td>2025-05-20t01:05:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>01:15</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aa8330</td>\n",
       "      <td>american airlines</td>\n",
       "      <td>landed</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 03:39:00</td>\n",
       "      <td>2025-05-20 03:39:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t2</td>\n",
       "      <td>2025-05-20t03:39:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>04:25</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ba6124</td>\n",
       "      <td>british airways</td>\n",
       "      <td>landed</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 03:39:00</td>\n",
       "      <td>2025-05-20 03:39:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t2</td>\n",
       "      <td>2025-05-20t03:39:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>04:25</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  flight_iata            airline  status                sched  \\\n",
       "0      fr1739            ryanair  landed  2025-05-20 01:00:00   \n",
       "1      fr9612            ryanair  landed  2025-05-20 01:10:00   \n",
       "2       fr651            ryanair  landed  2025-05-20 01:15:00   \n",
       "3      aa8330  american airlines  landed  2025-05-20 04:25:00   \n",
       "4      ba6124    british airways  landed  2025-05-20 04:25:00   \n",
       "\n",
       "                   est                  act  delay terminal  \\\n",
       "0  2025-05-20 01:15:00  2025-05-20 01:15:00   15.0       t1   \n",
       "1  2025-05-20 01:11:00  2025-05-20 01:03:00    NaN       t1   \n",
       "2  2025-05-20 01:12:00  2025-05-20 01:05:00    NaN       t1   \n",
       "3  2025-05-20 03:39:00  2025-05-20 03:39:00    NaN       t2   \n",
       "4  2025-05-20 03:39:00  2025-05-20 03:39:00    NaN       t2   \n",
       "\n",
       "                    runway  delay_calc        date   time  is_cancelled  \\\n",
       "0  2025-05-20t01:15:00.000        15.0  2025-05-20  01:00         False   \n",
       "1  2025-05-20t01:03:00.000         NaN  2025-05-20  01:10         False   \n",
       "2  2025-05-20t01:05:00.000         NaN  2025-05-20  01:15         False   \n",
       "3  2025-05-20t03:39:00.000         NaN  2025-05-20  04:25         False   \n",
       "4  2025-05-20t03:39:00.000         NaN  2025-05-20  04:25         False   \n",
       "\n",
       "      type            datetime  \n",
       "0  arrival 2025-05-20 01:00:00  \n",
       "1  arrival 2025-05-20 01:10:00  \n",
       "2  arrival 2025-05-20 01:15:00  \n",
       "3  arrival 2025-05-20 04:25:00  \n",
       "4  arrival 2025-05-20 04:25:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üìä Step 16 ‚Äì Load and clean Dublin Airport flight data\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# --- Inspect available tidy flight files ---\n",
    "print(\"üìÇ Available files in data/:\", os.listdir(DATA_DIR))\n",
    "\n",
    "# --- Load the combined tidy flights file if present; fall back to arrivals+departures ---\n",
    "flights_path = DATA_DIR / \"dub_flights_tidy.csv\"\n",
    "arr_path = DATA_DIR / \"dub_arrivals_tidy.csv\"\n",
    "dep_path = DATA_DIR / \"dub_departures_tidy.csv\"\n",
    "\n",
    "if flights_path.exists():\n",
    "    # Preferred: combined tidy dataset\n",
    "    df_flights = pd.read_csv(flights_path, low_memory=False)\n",
    "    source_used = \"combined\"\n",
    "elif arr_path.exists() and dep_path.exists():\n",
    "    # Fallback: arrivals + departures concatenated\n",
    "    df_arrivals = pd.read_csv(arr_path, low_memory=False)\n",
    "    df_departures = pd.read_csv(dep_path, low_memory=False)\n",
    "    # Harmonise columns before concatenation\n",
    "    common_cols = sorted(set(df_arrivals.columns).intersection(set(df_departures.columns)))\n",
    "    df_flights = pd.concat(\n",
    "        [df_arrivals[common_cols], df_departures[common_cols]],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    source_used = \"arrivals+departures\"\n",
    "else:\n",
    "    raise FileNotFoundError(\"‚ùå No tidy flight files found in data/. Expected dub_flights_tidy.csv or both arrivals/departures.\")\n",
    "\n",
    "print(f\"‚úÖ Loaded flight dataset from: {source_used} ({len(df_flights)} rows)\")\n",
    "\n",
    "# --- Normalise column names and whitespace ---\n",
    "df_flights.columns = df_flights.columns.str.strip().str.lower()\n",
    "\n",
    "# --- Ensure a proper datetime column ---\n",
    "# Use 'sched' (scheduled time) as the canonical datetime for weather alignment\n",
    "if \"sched\" in df_flights.columns:\n",
    "    df_flights[\"datetime\"] = pd.to_datetime(df_flights[\"sched\"], errors=\"coerce\")\n",
    "elif \"datetime\" in df_flights.columns:\n",
    "    df_flights[\"datetime\"] = pd.to_datetime(df_flights[\"datetime\"], errors=\"coerce\")\n",
    "elif \"date\" in df_flights.columns and \"time\" in df_flights.columns:\n",
    "    df_flights = prepare_datetime(df_flights, date_col=\"date\", time_col=\"time\")\n",
    "else:\n",
    "    raise KeyError(\"‚ùå Flight data needs either 'sched', 'datetime', or both 'date' and 'time' columns.\")\n",
    "\n",
    "# --- Convert delay columns to numeric if present ---\n",
    "for col in [\"arr_delay\", \"dep_delay\", \"delay\", \"delay_minutes\", \"delay_calc\"]:\n",
    "    if col in df_flights.columns:\n",
    "        df_flights[col] = pd.to_numeric(df_flights[col], errors=\"coerce\")\n",
    "\n",
    "# --- Drop rows without valid datetime and reset index ---\n",
    "df_flights = df_flights.dropna(subset=[\"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"üßπ Cleaned flight dataset has {len(df_flights)} rows with valid datetime\")\n",
    "display(df_flights.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
