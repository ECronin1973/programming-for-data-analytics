{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae6fbf9",
   "metadata": {},
   "source": [
    "# Flight Punctuality Analysis at Dublin Airport\n",
    "\n",
    "This project examines how weather conditions influence flight punctuality at Dublin Airport.  \n",
    "The analysis combines flight activity data (arrivals, departures, delays, cancellations) with historical and forecast weather data from Met Ã‰ireann to identify trends, quantify the impact of adverse conditions, and project future delay probabilities. \n",
    " \n",
    "By aligning operational flight records with local weather observations, the study provides insights into how rain, wind, and visibility affect airport performance and passenger reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96525b",
   "metadata": {},
   "source": [
    "### Notebook Control Flag Explanation\n",
    "\n",
    "This notebook contains code to download flight history data from the Aviation Edge API.  \n",
    "Because downloading six months of data can take a long time and may stress the API, we use a **control flag** called `RUN_DOWNLOAD` to decide whether the download should run.\n",
    "\n",
    "- **RUN_DOWNLOAD = False** â†’ The download section is skipped.  \n",
    "  Use this setting when you want to run analysis, visualizations, or other notebook functionality without refreshing the data.\n",
    "\n",
    "- **RUN_DOWNLOAD = True** â†’ The download section executes.  \n",
    "  Use this setting only when you deliberately want to refresh the flight history data and update the cumulative JSON files.\n",
    "\n",
    "This design ensures:\n",
    "- The notebook can be safely re-run without triggering unwanted downloads.\n",
    "- Existing JSON files are preserved and can be loaded for analysis.\n",
    "- You have full control over when heavy API calls are made.\n",
    "\n",
    "ğŸ‘‰ In practice: keep `RUN_DOWNLOAD = False` most of the time, and flip it to `True` only when you need new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c9585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Control flag to enable/disable data refresh ---\n",
    "RUN_DOWNLOAD = False   # Change to True only when you want to refresh data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037f7b5",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Step 2 â€“ Install and Import Required Libraries\n",
    "\n",
    "This step prepares the environment for the Dublin Airport Flight Rerouting Project.  \n",
    "It ensures that all required Python packages are available and sets up the projectâ€™s directory structure inside the `project` root.\n",
    "\n",
    "The notebook imports essential libraries for:\n",
    "\n",
    "- ğŸ“Š **Data manipulation** (`pandas`, `numpy`)\n",
    "- ğŸ“… **Date and time handling** (`datetime`, `matplotlib.dates`)\n",
    "- ğŸ“ˆ **Plotting and visualisation** (`matplotlib`, `seaborn`, `plotly`)\n",
    "- ğŸ¤– **Machine learning and model persistence** (`scikit-learn`, `joblib`)\n",
    "- ğŸ“‚ **File handling and paths** (`os`, `pathlib`, `json`)\n",
    "- ğŸŒ **Web access** (`requests`)\n",
    "- ğŸ§© **Interactivity and display** (`ipywidgets`, `IPython.display`)\n",
    "\n",
    "It also defines key directories (`data`, `outputs`, `models`, `docs`) inside the `project` folder and ensures they exist.  \n",
    "This structure keeps raw data, processed outputs, trained models, and documentation organised and reproducible.\n",
    "\n",
    "ğŸ“Œ *Note: `%pip install` commands can be used inside Jupyter notebooks if a package is missing.  \n",
    "For scripts or terminal use, run `pip install` directly.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86305781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Project root: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\n",
      "Data directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\data\n",
      "Output directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\outputs\n",
      "Model directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\models\n",
      "Docs directory: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\docs\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly --quiet\n",
    "\n",
    "# --- Core Python modules ---\n",
    "import json              # config files / JSON handling\n",
    "import os                # operating system interactions\n",
    "import time              # time management\n",
    "import warnings          # manage warnings\n",
    "from datetime import date, timedelta  # date calculations\n",
    "from pathlib import Path              # path management\n",
    "from calendar import monthrange       # leap-year safe month calculations\n",
    "\n",
    "# --- Data science / numerical libraries ---\n",
    "import numpy as np       # numerical operations\n",
    "import pandas as pd      # data manipulation\n",
    "\n",
    "# --- Plotting libraries ---\n",
    "import matplotlib.pyplot as plt   # static plotting\n",
    "import plotly.express as px       # interactive plotting\n",
    "import seaborn as sns             # enhanced plotting\n",
    "\n",
    "# --- Machine learning libraries ---\n",
    "import joblib                     # model persistence\n",
    "from sklearn.ensemble import GradientBoostingClassifier   # example model\n",
    "from sklearn.linear_model import LogisticRegression       # example model\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score\n",
    ")  # model evaluation\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    train_test_split\n",
    ")  # model validation\n",
    "\n",
    "# --- API / external requests ---\n",
    "import requests                   # API calls\n",
    "\n",
    "# --- Plotting style ---\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# --- Explicit project root: programming-for-data-analytics/project ---\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name != \"project\":\n",
    "    # climb up until we find project folder\n",
    "    for parent in ROOT.parents:\n",
    "        if parent.name == \"project\":\n",
    "            ROOT = parent\n",
    "            break\n",
    "\n",
    "# --- Define key directories inside project ---\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\"\n",
    "MODEL_DIR = ROOT / \"models\"\n",
    "DOCS_DIR = ROOT / \"docs\"\n",
    "\n",
    "# --- Ensure directories exist ---\n",
    "for path in [DATA_DIR, OUTPUT_DIR, MODEL_DIR, DOCS_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Docs directory: {DOCS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468332e9",
   "metadata": {},
   "source": [
    "### Step 3 â€“ Utilise Helper Functions for Dublin Airport Data Processing\n",
    "\n",
    "This section defines a set of reusable helper functions that simplify common tasks in the project.  \n",
    "They are designed specifically to support the analysis of **Dublin Airport flight activity and weather data** by handling messy inputs and preparing clean datasets for exploration and modelling.\n",
    "\n",
    "The functions help with:\n",
    "\n",
    "- âœ… Detecting and parsing inconsistent datetime formats in flight and weather logs  \n",
    "- âœ… Standardising and cleaning temperature and precipitation columns from Met Ã‰ireann datasets  \n",
    "- âœ… Loading and preparing Dublin Airport daily weather data from local CSV files  \n",
    "- âœ… Defining Irish seasonal boundaries (Winter, Spring, Summer, Autumn) for comparative analysis  \n",
    "- âœ… Filtering weather data for a custom date range to align with flight events  \n",
    "- âœ… Validating user-provided date inputs for reproducible analysis  \n",
    "- âœ… Detecting header rows in raw CSV files downloaded from dashboards  \n",
    "\n",
    "Each helper is **modular** â€” it performs one clear task and can be reused across notebooks and scripts.  \n",
    "This improves readability, reduces duplication, and supports good programming practices for the final project.\n",
    "\n",
    "ğŸ“Œ *Tip: These helpers are written to be beginner-friendly, with comments explaining their purpose and logic. They make it easier to align flight activity with weather conditions when investigating delays and cancellations.*\n",
    "\n",
    "ğŸ“– References:  \n",
    "- [Real Python â€“ Python Helper Functions](https://realpython.com/defining-your-own-python-function/)  \n",
    "- [GeeksforGeeks â€“ Python Helper Functions](https://www.geeksforgeeks.org/python-helper-functions/)  \n",
    "- [Wikipedia â€“ DRY Principle (Don't Repeat Yourself)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607be6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‚ Helper Functions for Dublin Airport Project\n",
    "# These functions handle parsing dates, cleaning weather data, preparing ranges,\n",
    "# defining Irish seasons, and detecting CSV headers.\n",
    "# Keep them in one cell so they are easy to reuse across the notebook.\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from calendar import monthrange\n",
    "\n",
    "# ğŸ” Detect the header row in a CSV file\n",
    "def detect_header(lines, keywords=(\"station\",\"date\",\"rain\",\"temp\",\"wind\")):\n",
    "    \"\"\"\n",
    "    Detect the most likely header row in a CSV file.\n",
    "    Looks for lines containing known weather keywords and multiple columns.\n",
    "    \"\"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        line_lower = line.strip().lower()\n",
    "        if any(line_lower.startswith(k) for k in keywords) and \",\" in line:\n",
    "            columns = line.split(\",\")\n",
    "            if len(columns) > 3:  # header rows usually have multiple columns\n",
    "                return i\n",
    "    print(\"âš ï¸ Warning: header row not found. Defaulting to first line.\")\n",
    "    return 0\n",
    "\n",
    "# ğŸ“… Detect the most likely datetime format from sample strings\n",
    "def detect_datetime_format(samples, formats, dayfirst=True, min_match_ratio=0.7, min_absolute=5):\n",
    "    \"\"\"\n",
    "    Try each format and return the one that matches at least 70% of samples\n",
    "    or at least 'min_absolute' matches. Helps ensure consistent parsing of date strings.\n",
    "    \"\"\"\n",
    "    for fmt in formats:\n",
    "        parsed = pd.to_datetime(samples, format=fmt, dayfirst=dayfirst, errors='coerce')\n",
    "        matches = parsed.notna().sum()\n",
    "        if matches >= max(min_absolute, int(len(samples) * min_match_ratio)):\n",
    "            return fmt\n",
    "    return None\n",
    "\n",
    "# ğŸ“… Parse a datetime column using format detection or fallback\n",
    "def parse_datetime_column(df, date_col, candidate_formats=None, dayfirst=True):\n",
    "    \"\"\"\n",
    "    Parse a datetime column using known formats.\n",
    "    Falls back to flexible parsing if none match.\n",
    "    \"\"\"\n",
    "    if candidate_formats is None:\n",
    "        candidate_formats = [\n",
    "            '%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%d-%b-%Y %H:%M',\n",
    "            '%d/%m/%Y %H:%M:%S', '%d/%m/%Y %H:%M', '%d-%m-%Y %H:%M',\n",
    "            '%d %b %Y %H:%M', '%d %B %Y %H:%M',\n",
    "        ]\n",
    "\n",
    "    sample_vals = df[date_col].dropna().astype(str).head(100).tolist()\n",
    "    chosen_fmt = detect_datetime_format(sample_vals, candidate_formats, dayfirst=dayfirst)\n",
    "\n",
    "    if chosen_fmt:\n",
    "        print(f\"âœ… Detected datetime format: {chosen_fmt}\")\n",
    "        return pd.to_datetime(df[date_col], format=chosen_fmt, dayfirst=dayfirst, errors='coerce')\n",
    "    else:\n",
    "        print(\"âš ï¸ No single format matched. Falling back to flexible parsing.\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', message='Could not infer format')\n",
    "            return pd.to_datetime(df[date_col], dayfirst=dayfirst, errors='coerce')\n",
    "\n",
    "# ğŸ•’ Ensure full datetime column for arrivals/departures and weather\n",
    "def prepare_datetime(df, date_col='date', time_col=None):\n",
    "    \"\"\"\n",
    "    Ensure DataFrame has a full datetime column.\n",
    "    Works for datasets with combined 'date' + time or already combined datetime.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    if 'datetime' in df.columns:\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    elif date_col in df.columns and time_col:\n",
    "        dt_strings = df[date_col].astype(str) + \" \" + df[time_col].astype(str)\n",
    "        df['datetime'] = pd.to_datetime(dt_strings, format=\"%d-%b-%Y %H:%M\", errors='coerce')\n",
    "    elif date_col in df.columns:\n",
    "        # Explicit format for Met Ã‰ireann hourly data: '01-jan-1945 00:00'\n",
    "        df['datetime'] = pd.to_datetime(df[date_col], format=\"%d-%b-%Y %H:%M\", errors='coerce')\n",
    "    else:\n",
    "        raise KeyError(\"No suitable date/time columns found\")\n",
    "\n",
    "    # Add convenience fields\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "    return df.dropna(subset=['datetime']).reset_index(drop=True)\n",
    "\n",
    "# ğŸŒ¡ï¸ Ensure temperature column is numeric and named 'temp'\n",
    "def parse_temperature_column(df, col_name='temp'):\n",
    "    \"\"\"\n",
    "    Convert the temperature column to numeric and rename it to 'temp'.\n",
    "    If no exact match, look for any column containing 'temp'.\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        col_name = next((c for c in df.columns if 'temp' in c.lower()), None)\n",
    "        if col_name is None:\n",
    "            raise KeyError(\"No temperature column found.\")\n",
    "    if col_name != 'temp':\n",
    "        df.rename(columns={col_name: 'temp'}, inplace=True)\n",
    "    df['temp'] = pd.to_numeric(df['temp'], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# ğŸ“‚ Load cleaned weather data from local CSV\n",
    "def load_cleaned_weather_data(filepath=\"data/dublin_airport_hourly.csv\"):\n",
    "    \"\"\"\n",
    "    Load weather dataset from CSV and strip spaces from column names.\n",
    "    Default path now points to hourly Dublin Airport data.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "\n",
    "# ğŸ“Š Prepare weather data with proper datetime column\n",
    "def prepare_weather_data(df, date_col='date'):\n",
    "    \"\"\"\n",
    "    Ensure weather DataFrame has a proper datetime column.\n",
    "    Works for Met Ã‰ireann hourly datasets where 'date' already includes time.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    if 'datetime' in df.columns:\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    elif date_col in df.columns:\n",
    "        # Explicit format: '01-jan-1945 00:00'\n",
    "        df['datetime'] = pd.to_datetime(df[date_col], format=\"%d-%b-%Y %H:%M\", errors='coerce')\n",
    "    else:\n",
    "        raise ValueError(\"No suitable date column found in weather dataset\")\n",
    "\n",
    "    df['date'] = df['datetime'].dt.date\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    return df.dropna(subset=['datetime']).reset_index(drop=True)\n",
    "\n",
    "# ğŸ› ï¸ Clean and standardise key weather columns\n",
    "def clean_weather_columns(df):\n",
    "    \"\"\"\n",
    "    Standardise Dublin Airport hourly weather data:\n",
    "    - Convert rainfall, temperature, wind speed, and pressure to numeric\n",
    "    - Handle 'Tr' (trace) rainfall as 0.0\n",
    "    - Ensure consistent column naming\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # Handle rainfall column\n",
    "    if 'rain' in df.columns:\n",
    "        df['rain'] = df['rain'].replace('Tr', 0.0)   # trace rainfall â†’ 0\n",
    "        df['rain'] = pd.to_numeric(df['rain'], errors='coerce')\n",
    "\n",
    "    # Handle temperature column\n",
    "    temp_col = next((c for c in df.columns if 'temp' in c), None)\n",
    "    if temp_col:\n",
    "        df['temp'] = pd.to_numeric(df[temp_col], errors='coerce')\n",
    "\n",
    "    # Handle wind speed column\n",
    "    if 'wdsp' in df.columns:\n",
    "        df['wdsp'] = pd.to_numeric(df['wdsp'], errors='coerce')\n",
    "\n",
    "    # Handle pressure column\n",
    "    if 'msl' in df.columns:\n",
    "        df['msl'] = pd.to_numeric(df['msl'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "# ğŸ“† Convert user input strings into a validated date range\n",
    "def get_custom_range(start_str, end_str):\n",
    "    \"\"\"\n",
    "    Convert string inputs into datetime objects and validate order.\n",
    "    Handles ISO (YYYY-MM-DD) and European (DD/MM/YYYY) formats gracefully.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try ISO format first\n",
    "        try:\n",
    "            start = pd.to_datetime(start_str, format=\"%Y-%m-%d\", errors=\"raise\")\n",
    "        except Exception:\n",
    "            start = pd.to_datetime(start_str, dayfirst=True)\n",
    "\n",
    "        try:\n",
    "            end = pd.to_datetime(end_str, format=\"%Y-%m-%d %H:%M\", errors=\"raise\")\n",
    "        except Exception:\n",
    "            end = pd.to_datetime(end_str, dayfirst=True)\n",
    "\n",
    "        if start > end:\n",
    "            raise ValueError(\"Start date must be before end date.\")\n",
    "        return start, end\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Invalid date range: {e}\")\n",
    "        return None, None\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from calendar import monthrange\n",
    "\n",
    "# ğŸ‚ Define Irish seasonal boundaries for a given year (leap year safe)\n",
    "def define_irish_seasons(year=2025):\n",
    "    feb_days = monthrange(year, 2)[1]\n",
    "    data = [\n",
    "        (\"Winter\", pd.Timestamp(f\"{year-1}-12-01\"), pd.Timestamp(f\"{year}-02-{feb_days} 23:59\")),\n",
    "        (\"Spring\", pd.Timestamp(f\"{year}-03-01\"), pd.Timestamp(f\"{year}-05-31 23:59\")),\n",
    "        (\"Summer\", pd.Timestamp(f\"{year}-06-01\"), pd.Timestamp(f\"{year}-08-31 23:59\")),\n",
    "        (\"Autumn\", pd.Timestamp(f\"{year}-09-01\"), pd.Timestamp(f\"{year}-11-30 23:59\")),\n",
    "    ]\n",
    "    return pd.DataFrame(data, columns=[\"season\", \"start\", \"end\"])\n",
    "\n",
    "# ğŸ‚ Assign Irish seasons to a DataFrame in bulk (vectorised)\n",
    "def assign_season_vectorized(df, datetime_col=\"datetime\"):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure datetime dtype\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n",
    "        df[datetime_col] = pd.to_datetime(df[datetime_col], errors=\"coerce\")\n",
    "\n",
    "    month = df[datetime_col].dt.month\n",
    "\n",
    "    # Define conditions and choices\n",
    "    conditions = [\n",
    "        month.isin([12, 1, 2]),   # Winter\n",
    "        month.isin([3, 4, 5]),    # Spring\n",
    "        month.isin([6, 7, 8]),    # Summer\n",
    "        month.isin([9, 10, 11])   # Autumn\n",
    "    ]\n",
    "    choices = [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]\n",
    "\n",
    "    # Vectorised assignment\n",
    "    df[\"season\"] = np.select(conditions, choices, default=\"Unknown\")\n",
    "\n",
    "    # Make categorical for clarity\n",
    "    df[\"season\"] = pd.Categorical(\n",
    "        df[\"season\"],\n",
    "        categories=choices + [\"Unknown\"],\n",
    "        ordered=True\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ecf4c3",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Step 4 â€“ Download Dublin Airport Daily Data and Detect Header Row\n",
    "\n",
    "In this step, the notebook retrieves the **Dublin Airport Daily Data CSV** directly from Met Ã‰ireannâ€™s open data service.  \n",
    "This dataset contains daily weather observations (e.g., precipitation, temperature, wind speed, radiation) recorded at Dublin Airport, which will later be aligned with flight activity logs to analyse rerouting events.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸŒ **Downloading the raw CSV** from Met Ã‰ireann using the `requests` library.  \n",
    "- ğŸ“‚ **Defining a local output path** (`data/dublin_airport_daily.csv`) to store the file inside the projectâ€™s `data` folder.  \n",
    "- âœ… **Checking the HTTP response** to ensure the download was successful.  \n",
    "- ğŸ“‘ **Splitting the file into lines** so the structure can be inspected before loading into pandas.  \n",
    "- ğŸ” **Detecting the header row** using the `detect_header` helper function defined earlier.  \n",
    "  This ensures that column names (such as `date`, `maxtp`, `mintp`, `rain`, `wdsp`) are correctly identified even if the file contains metadata lines at the top.  \n",
    "- ğŸ–¨ï¸ **Printing the detected header row** to confirm the correct starting point for parsing.\n",
    "\n",
    "ğŸ“Œ *Tip: Detecting the header row is important because Met Ã‰ireann CSVs often include metadata lines before the actual data table.  \n",
    "By confirming the header row, you avoid misaligned columns and ensure clean parsing in later steps.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cc44a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Header row detected at line 23:\n",
      "date,ind,rain,ind,temp,ind,wetb,dewpt,vappr,rhum,msl,ind,wdsp,ind,wddir,ww,w,sun,vis,clht,clamt\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ Step 4 â€“ Download Dublin Airport Hourly Data CSV and Detect Header Row\n",
    "\n",
    "# Note: pathlib.Path and requests are imported earlier in the notebook, so we avoid re-importing them here.\n",
    "\n",
    "# --- Define output path for cleaned CSV ---\n",
    "DATA_PATH = Path(\"data/dublin_airport_hourly.csv\")\n",
    "\n",
    "# --- URL for the hourly CSV (may return 404 if file moved) ---\n",
    "url = \"https://cli.fusio.net/cli/climate_data/webdata/hly532.csv\"\n",
    "\n",
    "# --- Attempt to download the remote CSV, with a safe fallback to a local copy ---\n",
    "try:\n",
    "    response = requests.get(url, timeout=30)\n",
    "except Exception as e:\n",
    "    response = None\n",
    "    print(f\"âš ï¸ Network error when fetching URL: {e}\")\n",
    "\n",
    "if response is None or getattr(response, \"status_code\", None) != 200:\n",
    "    # If remote download failed, try to use a previously saved local file if available\n",
    "    if DATA_PATH.exists():\n",
    "        print(f\"âš ï¸ Remote download failed (status: {getattr(response,'status_code',None)}). Falling back to local file: {DATA_PATH}\")\n",
    "        text = DATA_PATH.read_text(encoding=\"utf-8\")\n",
    "        lines = text.splitlines()\n",
    "    else:\n",
    "        raise RuntimeError(f\"âŒ Failed to download data: HTTP {getattr(response,'status_code',None)} and no local fallback at {DATA_PATH}\")\n",
    "else:\n",
    "    # Successful download â€” use remote content\n",
    "    lines = response.text.splitlines()\n",
    "\n",
    "# --- Detect header row using helper function ---\n",
    "header_index = detect_header(lines)\n",
    "\n",
    "# âœ… Confirm detected header row\n",
    "print(f\"âœ… Header row detected at line {header_index}:\")\n",
    "print(lines[header_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146915b",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 4a â€“ Define Delay-Relevant Weather Schema\n",
    "\n",
    "In this step, we define a specific schema for the weather data that focuses on columns relevant to flight delays and rerouting at Dublin Airport.  \n",
    "This schema will be used when loading and processing the weather dataset to ensure we only keep the necessary information for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4247df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Delay-relevant weather schema defined\n",
      "['datetime', 'datetime_hour', 'temp', 'rain', 'wdsp', 'wddir', 'vis', 'clht', 'ww', 'w', 'season']\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‘ Step 4a â€“ Define Delay-Relevant Weather Schema\n",
    "\n",
    "# These are the only weather variables we will keep and analyse\n",
    "delay_relevant_weather_cols = [\n",
    "    \"datetime\",      # original timestamp (needed for season assignment)\n",
    "    \"datetime_hour\", # floored datetime for alignment with flights\n",
    "    \"temp\",          # air temperature\n",
    "    \"rain\",          # precipitation\n",
    "    \"wdsp\",          # wind speed\n",
    "    \"wddir\",         # wind direction\n",
    "    \"vis\",           # visibility\n",
    "    \"clht\",          # cloud height\n",
    "    \"ww\",            # present weather code\n",
    "    \"w\",             # past weather code\n",
    "    \"season\"         # derived categorical label\n",
    "]\n",
    "\n",
    "print(\"âœ… Delay-relevant weather schema defined\")\n",
    "print(delay_relevant_weather_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f4588",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 4b â€“ Load and Inspect Dublin Airport Hourly Weather Data\n",
    "\n",
    "The raw hourly weather dataset from Met Ã‰ireann is provided as a CSV file with a metadata block at the top.  \n",
    "Before we can clean and merge it with flight data, we need to:\n",
    "\n",
    "1. ğŸ“¥ **Download the raw CSV** directly from the Met Ã‰ireann climate data portal.  \n",
    "2. ğŸ§¾ **Detect the header row** (the first line with 20+ commaâ€‘separated fields) to skip the metadata block.  \n",
    "3. ğŸ“„ **Print the metadata block** for transparency, showing station details and measurement notes.  \n",
    "4. ğŸ“Š **Load the actual data into a DataFrame** (`df_weather`) using the detected header row.  \n",
    "5. ğŸ” **Inspect the dataset** by displaying the first few rows, checking column types, and reviewing missing values.  \n",
    "6. ğŸ“ˆ **Generate a statistical summary** to understand ranges, distributions, and potential anomalies in the weather variables.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "This inspection step ensures that the dataset is correctly loaded, the structure is understood, and reviewers can see the raw data context before any cleaning or transformations are applied. It provides transparency and validates that the ingestion process is reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c71e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Metadata block (before header):\n",
      "Station Name: DUBLIN AIRPORT\n",
      "Station Height: 71 M\n",
      "Latitude:53.428  ,Longitude: -6.241\n",
      "\n",
      "\n",
      "date:  -  Date and Time (utc)\n",
      "rain:  -  Precipitation Amount (mm)\n",
      "temp:  -  Air Temperature (C)\n",
      "wetb:  -  Wet Bulb Temperature (C)\n",
      "dewpt: -  Dew Point Temperature (C)\n",
      "rhum:  -  Relative Humidity (%)\n",
      "vappr: -  Vapour Pressure (hPa)\n",
      "msl:   -  Mean Sea Level Pressure (hPa)\n",
      "wdsp:  -  Mean Wind Speed (knot)\n",
      "wddir: -  Predominant Wind Direction (degree)\n",
      "ww:    -  Synop code for Present Weather\n",
      "w:     -  Synop code for Past Weather\n",
      "sun:   -  Sunshine duration (hours)\n",
      "vis:   -  Visibility (m)\n",
      "clht:  -  Cloud height (100's of ft) - 999 if none\n",
      "clamt: -  Cloud amount\n",
      "ind:   -  Indicator\n",
      "\n",
      "\n",
      "First 5 rows of Dublin Airport Hourly Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ind</th>\n",
       "      <th>rain</th>\n",
       "      <th>ind.1</th>\n",
       "      <th>temp</th>\n",
       "      <th>ind.2</th>\n",
       "      <th>wetb</th>\n",
       "      <th>dewpt</th>\n",
       "      <th>vappr</th>\n",
       "      <th>rhum</th>\n",
       "      <th>...</th>\n",
       "      <th>ind.3</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>ind.4</th>\n",
       "      <th>wddir</th>\n",
       "      <th>ww</th>\n",
       "      <th>w</th>\n",
       "      <th>sun</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>clamt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-jan-1945 00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.2</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01-jan-1945 01:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01-jan-1945 02:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4800</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01-jan-1945 03:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01-jan-1945 04:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date  ind  rain  ind.1  temp  ind.2  wetb  dewpt vappr rhum  \\\n",
       "0  01-jan-1945 00:00    2   0.0      0   4.9      0   4.6    4.4   8.2   95   \n",
       "1  01-jan-1945 01:00    3   0.0      0   5.1      0   4.9    4.4   8.5   97   \n",
       "2  01-jan-1945 02:00    2   0.0      0   5.1      0   4.8    4.4   8.5   97   \n",
       "3  01-jan-1945 03:00    0   0.2      0   5.2      0   5.0    4.4   8.5   97   \n",
       "4  01-jan-1945 04:00    2   0.0      0   5.6      0   5.4    5.0   8.8   97   \n",
       "\n",
       "   ...  ind.3  wdsp  ind.4  wddir  ww  w  sun   vis clht clamt  \n",
       "0  ...      1     0      1      0  50  4  0.0   200    2     8  \n",
       "1  ...      1     0      1      0  45  4  0.0   200    2     8  \n",
       "2  ...      1     0      1      0  50  4  0.0  4800    4     8  \n",
       "3  ...      1     0      1      0  50  4  0.0  6000    4     8  \n",
       "4  ...      1     7      1    250  50  5  0.0  6000    4     8  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 708577 entries, 0 to 708576\n",
      "Data columns (total 21 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   date    708577 non-null  object \n",
      " 1   ind     708577 non-null  int64  \n",
      " 2   rain    708577 non-null  float64\n",
      " 3   ind.1   708577 non-null  int64  \n",
      " 4   temp    708577 non-null  float64\n",
      " 5   ind.2   708577 non-null  int64  \n",
      " 6   wetb    708577 non-null  float64\n",
      " 7   dewpt   708577 non-null  float64\n",
      " 8   vappr   708577 non-null  object \n",
      " 9   rhum    708577 non-null  object \n",
      " 10  msl     708577 non-null  float64\n",
      " 11  ind.3   708577 non-null  int64  \n",
      " 12  wdsp    708577 non-null  int64  \n",
      " 13  ind.4   708577 non-null  int64  \n",
      " 14  wddir   708577 non-null  object \n",
      " 15  ww      708577 non-null  int64  \n",
      " 16  w       708577 non-null  int64  \n",
      " 17  sun     708577 non-null  float64\n",
      " 18  vis     708577 non-null  object \n",
      " 19  clht    708577 non-null  object \n",
      " 20  clamt   708577 non-null  object \n",
      "dtypes: float64(6), int64(8), object(7)\n",
      "memory usage: 113.5+ MB\n",
      "None\n",
      "\n",
      "Summary statistics:\n",
      "                     date            ind           rain          ind.1  \\\n",
      "count              708577  708577.000000  708577.000000  708577.000000   \n",
      "unique             708577            NaN            NaN            NaN   \n",
      "top     01-jan-1945 00:00            NaN            NaN            NaN   \n",
      "freq                    1            NaN            NaN            NaN   \n",
      "mean                  NaN       0.632887       0.086568       0.014206   \n",
      "std                   NaN       1.101550       0.418433       0.118613   \n",
      "min                   NaN       0.000000       0.000000       0.000000   \n",
      "25%                   NaN       0.000000       0.000000       0.000000   \n",
      "50%                   NaN       0.000000       0.000000       0.000000   \n",
      "75%                   NaN       2.000000       0.000000       0.000000   \n",
      "max                   NaN       6.000000      26.500000       2.000000   \n",
      "\n",
      "                 temp          ind.2           wetb          dewpt   vappr  \\\n",
      "count   708577.000000  708577.000000  708577.000000  708577.000000  708577   \n",
      "unique            NaN            NaN            NaN            NaN     212   \n",
      "top               NaN            NaN            NaN            NaN     9.2   \n",
      "freq              NaN            NaN            NaN            NaN    8728   \n",
      "mean         9.665900       0.029293       8.224969       6.604194     NaN   \n",
      "std          4.893787       0.257749       4.409282       4.593159     NaN   \n",
      "min        -11.500000       0.000000     -11.500000     -17.700000     NaN   \n",
      "25%          6.100000       0.000000       5.100000       3.300000     NaN   \n",
      "50%          9.700000       0.000000       8.400000       6.900000     NaN   \n",
      "75%         13.200000       0.000000      11.600000      10.000000     NaN   \n",
      "max         29.100000       6.000000      22.200000      20.500000     NaN   \n",
      "\n",
      "          rhum  ...          ind.3           wdsp          ind.4   wddir  \\\n",
      "count   708577  ...  708577.000000  708577.000000  708577.000000  708577   \n",
      "unique      79  ...            NaN            NaN            NaN      38   \n",
      "top         91  ...            NaN            NaN            NaN     250   \n",
      "freq     26179  ...            NaN            NaN            NaN   48173   \n",
      "mean       NaN  ...       1.352876      10.114745       1.353055     NaN   \n",
      "std        NaN  ...       0.809113       5.680898       0.809156     NaN   \n",
      "min        NaN  ...       0.000000       0.000000       0.000000     NaN   \n",
      "25%        NaN  ...       1.000000       6.000000       1.000000     NaN   \n",
      "50%        NaN  ...       2.000000       9.000000       2.000000     NaN   \n",
      "75%        NaN  ...       2.000000      14.000000       2.000000     NaN   \n",
      "max        NaN  ...       6.000000      46.000000       7.000000     NaN   \n",
      "\n",
      "                   ww              w            sun     vis    clht   clamt  \n",
      "count   708577.000000  708577.000000  708577.000000  708577  708577  708577  \n",
      "unique            NaN            NaN            NaN      96     117      11  \n",
      "top               NaN            NaN            NaN   30000     999       7  \n",
      "freq              NaN            NaN            NaN  132838  187669  215678  \n",
      "mean        15.594647      17.165729       0.167449     NaN     NaN     NaN  \n",
      "std         22.552469      24.207024       0.326705     NaN     NaN     NaN  \n",
      "min          0.000000       0.000000       0.000000     NaN     NaN     NaN  \n",
      "25%          2.000000       2.000000       0.000000     NaN     NaN     NaN  \n",
      "50%          2.000000      11.000000       0.000000     NaN     NaN     NaN  \n",
      "75%         21.000000      11.000000       0.100000     NaN     NaN     NaN  \n",
      "max         97.000000      99.000000       1.000000     NaN     NaN     NaN  \n",
      "\n",
      "[11 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‘ Step 4b â€“ Load and Inspect Dublin Airport Hourly Data\n",
    "\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# --- Download raw CSV from Met Ã‰ireann ---\n",
    "url = \"https://cli.fusio.net/cli/climate_data/webdata/hly532.csv\"\n",
    "response = requests.get(url)\n",
    "lines = response.text.splitlines()\n",
    "\n",
    "# --- Detect header row: first line with 20+ comma-separated fields ---\n",
    "header_index = next(i for i, line in enumerate(lines) if line.count(\",\") >= 20)\n",
    "\n",
    "# --- Print metadata block (everything before header) ---\n",
    "print(\"ğŸ“„ Metadata block (before header):\")\n",
    "for line in lines[:header_index]:\n",
    "    print(line.strip())\n",
    "\n",
    "# --- Load CSV using detected header row ---\n",
    "csv_data = \"\\n\".join(lines[header_index:])  # header + data only\n",
    "df_weather = pd.read_csv(StringIO(csv_data), low_memory=False)\n",
    "\n",
    "# âœ… Inspect the first few rows of actual data\n",
    "print(\"\\nFirst 5 rows of Dublin Airport Hourly Data:\")\n",
    "display(df_weather.head())\n",
    "\n",
    "# âœ… Check column types and missing values\n",
    "print(\"\\nDataFrame info:\")\n",
    "print(df_weather.info())\n",
    "\n",
    "# âœ… Quick statistical summary\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df_weather.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0506e38",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 4c â€“ Clean Dublin Airport Hourly Weather Data (Reduced Schema)\n",
    "\n",
    "The raw hourly weather data from Met Ã‰ireann includes timestamps and meteorological variables.  \n",
    "To prepare it for integration with the flight dataset, we standardise the schema and ensure  \n",
    "continuous hourly coverage.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "1. ğŸ•’ **Parse timestamps**  \n",
    "   - Convert the `date` column into a proper `datetime` object.  \n",
    "   - Ensures consistent time handling across the dataset.\n",
    "\n",
    "2. â± **Create `datetime_hour`**  \n",
    "   - Floor each timestamp to the nearest hour.  \n",
    "   - Provides a common key for merging with flights in Stepâ€¯17.\n",
    "\n",
    "3. ğŸ”„ **Resolve gaps in hourly data**  \n",
    "   - Sort by `datetime_hour`.  \n",
    "   - Apply forward and backward fill (`ffill`/`bfill`) to smooth missing observations.  \n",
    "   - Maintains continuity for variables like `temp`, `rain`, `wdsp`, and `msl`.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "By cleaning and aligning weather data to hourly granularity, we ensure reproducible integration  \n",
    "with the flight dataset. This step guarantees that every flight record can be matched to a  \n",
    "corresponding weather observation, enabling meaningful delay vs. weather analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1602b6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Weather dataset cleaned and aligned to hourly granularity\n",
      "             datetime       datetime_hour  temp  rain  wdsp wddir   vis clht  \\\n",
      "0 1945-01-01 00:00:00 1945-01-01 00:00:00   4.9   0.0     0     0   200    2   \n",
      "1 1945-01-01 01:00:00 1945-01-01 01:00:00   5.1   0.0     0     0   200    2   \n",
      "2 1945-01-01 02:00:00 1945-01-01 02:00:00   5.1   0.0     0     0  4800    4   \n",
      "3 1945-01-01 03:00:00 1945-01-01 03:00:00   5.2   0.2     0     0  6000    4   \n",
      "4 1945-01-01 04:00:00 1945-01-01 04:00:00   5.6   0.0     7   250  6000    4   \n",
      "\n",
      "   ww  w  \n",
      "0  50  4  \n",
      "1  45  4  \n",
      "2  50  4  \n",
      "3  50  4  \n",
      "4  50  5  \n"
     ]
    }
   ],
   "source": [
    "# --- Parse 'date' column into proper datetime ---\n",
    "df_weather['datetime'] = pd.to_datetime(\n",
    "    df_weather['date'],\n",
    "    format=\"%d-%b-%Y %H:%M\",   # matches \"01-jan-1945 00:00\"\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# --- Floor to nearest hour for alignment with flights ---\n",
    "df_weather['datetime_hour'] = df_weather['datetime'].dt.floor('h')\n",
    "\n",
    "# --- Sort and forward/backward fill to resolve gaps ---\n",
    "df_weather = df_weather.sort_values('datetime_hour').ffill().bfill()\n",
    "\n",
    "# --- Restrict to schema (season will be added in Step 4d) ---\n",
    "df_weather = df_weather[[c for c in delay_relevant_weather_cols if c in df_weather.columns]]\n",
    "\n",
    "print(\"âœ… Weather dataset cleaned and aligned to hourly granularity\")\n",
    "print(df_weather.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9946b5",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 4d â€“ Assign Seasons to Dublin Airport Hourly Weather Data\n",
    "\n",
    "To enrich the weather dataset with seasonal context, we assign each record to one of the four Irish seasons.  \n",
    "This provides categorical grouping for delay analysis across Winter, Spring, Summer, and Autumn.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "1. ğŸ‚ **Seasonal boundaries (Irish definition)**  \n",
    "   - Winter: Decemberâ€“February (leap year safe)  \n",
    "   - Spring: Marchâ€“May  \n",
    "   - Summer: Juneâ€“August  \n",
    "   - Autumn: Septemberâ€“November  \n",
    "\n",
    "2. ğŸ•’ **Vectorised assignment via helper function**  \n",
    "   - Extract the month from each `datetime`.  \n",
    "   - Use the `assign_season_vectorized` helper (defined in the projectâ€™s helper section) to map months to seasons.  \n",
    "   - Default to `\"Unknown\"` if the timestamp is missing or invalid.\n",
    "\n",
    "3. ğŸ›  **Categorical column for clarity**  \n",
    "   - Store `season` as an ordered categorical variable (`Winter â†’ Spring â†’ Summer â†’ Autumn â†’ Unknown`).  \n",
    "   - Ensures consistent grouping and reproducibility in downstream analysis.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "Adding a `season` column enables exploration of **seasonal patterns in flight delays**.  \n",
    "This categorical context complements the continuous weather variables, providing richer insights  \n",
    "when merged with the flight dataset in Stepâ€¯17.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d242dab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Season labels assigned to weather dataset\n",
      "             datetime       datetime_hour  temp  rain  wdsp wddir   vis clht  \\\n",
      "0 1945-01-01 00:00:00 1945-01-01 00:00:00   4.9   0.0     0     0   200    2   \n",
      "1 1945-01-01 01:00:00 1945-01-01 01:00:00   5.1   0.0     0     0   200    2   \n",
      "2 1945-01-01 02:00:00 1945-01-01 02:00:00   5.1   0.0     0     0  4800    4   \n",
      "3 1945-01-01 03:00:00 1945-01-01 03:00:00   5.2   0.2     0     0  6000    4   \n",
      "4 1945-01-01 04:00:00 1945-01-01 04:00:00   5.6   0.0     7   250  6000    4   \n",
      "\n",
      "   ww  w  season  \n",
      "0  50  4  Winter  \n",
      "1  45  4  Winter  \n",
      "2  50  4  Winter  \n",
      "3  50  4  Winter  \n",
      "4  50  5  Winter  \n"
     ]
    }
   ],
   "source": [
    "# --- Apply seasonal assignment using helper function ---\n",
    "df_weather = assign_season_vectorized(df_weather, datetime_col=\"datetime\")\n",
    "\n",
    "# --- Ensure schema consistency ---\n",
    "df_weather = df_weather[[c for c in delay_relevant_weather_cols if c in df_weather.columns]]\n",
    "\n",
    "print(\"âœ… Season labels assigned to weather dataset\")\n",
    "print(df_weather.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1bbcc",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 4e â€“ Analyse Missing Values in Weather Dataset\n",
    "\n",
    "After cleaning and tagging seasons, it is important to check whether the **delayâ€‘relevant weather variables** contain any missing values.  \n",
    "This ensures transparency and helps reviewers understand the reliability of the dataset before it is merged with flights.\n",
    "\n",
    "1. ğŸ” **Columns examined**  \n",
    "   - `temp` (air temperature)  \n",
    "   - `rain` (precipitation amount)  \n",
    "   - `wdsp` (wind speed)  \n",
    "   - `wddir` (wind direction)  \n",
    "   - `vis` (visibility)  \n",
    "   - `clht` (cloud height)  \n",
    "   - `ww` (present weather code)  \n",
    "   - `w` (past weather code)  \n",
    "   - `season` (derived categorical label)  \n",
    "   - Plus `datetime` and `datetime_hour` for alignment  \n",
    "\n",
    "2. ğŸ“‰ **Why this matters**  \n",
    "   - Missing values in these fields can distort delay analysis.  \n",
    "   - For example, gaps in `rain` or `vis` could hide weather events that explain flight disruptions.  \n",
    "   - By checking percentages of missing values, we can decide whether to impute, forward/backward fill, or leave them as `NaN`.\n",
    "\n",
    "3. âœ… **Outcome**  \n",
    "   - If no missing values are found, the dataset is ready for merging with flights.  \n",
    "   - If gaps exist, they will be documented and addressed before Stepâ€¯17 (merge).\n",
    "\n",
    "ğŸ“Œ *This diagnostic step ensures that only the weather variables most relevant to flight delays are validated, keeping the workflow transparent and reviewerâ€‘friendly.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3993c1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Missing values per column (weather only):\n",
      "datetime         0\n",
      "datetime_hour    0\n",
      "temp             0\n",
      "rain             0\n",
      "wdsp             0\n",
      "wddir            0\n",
      "vis              0\n",
      "clht             0\n",
      "ww               0\n",
      "w                0\n",
      "season           0\n",
      "dtype: int64\n",
      "\n",
      "ğŸ“Š Percentage of missing values per column (weather only):\n",
      "datetime         0.0\n",
      "datetime_hour    0.0\n",
      "temp             0.0\n",
      "rain             0.0\n",
      "wdsp             0.0\n",
      "wddir            0.0\n",
      "vis              0.0\n",
      "clht             0.0\n",
      "ww               0.0\n",
      "w                0.0\n",
      "season           0.0\n",
      "dtype: float64\n",
      "\n",
      "âš ï¸ Weather columns with missing values: []\n"
     ]
    }
   ],
   "source": [
    "# --- Restrict to schema ---\n",
    "df_weather_check = df_weather[[c for c in delay_relevant_weather_cols if c in df_weather.columns]]\n",
    "\n",
    "# --- Check for missing values ---\n",
    "missing_summary_weather = df_weather_check.isna().sum().sort_values(ascending=False)\n",
    "missing_percent_weather = (df_weather_check.isna().mean() * 100).round(2)\n",
    "missing_columns_weather = missing_summary_weather[missing_summary_weather > 0].index.tolist()\n",
    "\n",
    "print(\"ğŸ” Missing values per column (weather only):\")\n",
    "print(missing_summary_weather)\n",
    "print(\"\\nğŸ“Š Percentage of missing values per column (weather only):\")\n",
    "print(missing_percent_weather)\n",
    "print(f\"\\nâš ï¸ Weather columns with missing values: {missing_columns_weather}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292af427",
   "metadata": {},
   "source": [
    "### ğŸ“ Step 5 â€“ Save the Cleaned Dublin Airport Hourly Data CSV\n",
    "\n",
    "After detecting the correct header row in the raw Met Ã‰ireann dataset, we now save a **cleaned version** of the Dublin Airport Hourly Data file into the projectâ€™s `data/` folder.  \n",
    "\n",
    "This step ensures:\n",
    "\n",
    "- ğŸ“‚ The dataset is stored locally for reuse without needing to re-download from Met Ã‰ireann each time  \n",
    "- ğŸ“‘ All future analysis references a consistent, structured version of the data (starting at the correct header row)  \n",
    "- ğŸ”„ The workflow remains reproducible and version-controlled, supporting transparent project documentation  \n",
    "- ğŸ› ï¸ Analysts and reviewers can always work from the same baseline dataset, avoiding inconsistencies caused by raw file metadata  \n",
    "- â±ï¸ Hourly granularity is preserved, which is essential for aligning weather conditions with flight arrivals and departures  \n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "Saving cleaned hourly data locally is a best practice in data science. It guarantees consistency across runs, makes collaboration easier, and allows you to track changes over time.  \n",
    "For Dublin Airport analysis, hourly weather data provides the necessary detail to study how conditions at specific times affect flight operations, ensuring reproducibility and transparency in your rerouting and delay modelling work.  \n",
    "\n",
    "ğŸ“– Reference:  \n",
    "- [GeeksforGeeks â€“ Explain Data Versioning](https://www.geeksforgeeks.org/machine-learning/explain-data-versioning/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fab2a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Saved cleaned climate data for Dublin Airport to: C:\\Users\\eCron\\OneDrive\\Documents\\ATU_CourseWork\\Programming For Data Analytics\\programming-for-data-analytics\\project\\data\\dublin_airport_hourly.csv\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ Step 5 â€“ Save the Cleaned CSV File\n",
    "\n",
    "# --- Ensure 'data' folder exists ---\n",
    "DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Save cleaned data starting from the detected header row ---\n",
    "with open(DATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines[header_index:]:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "# âœ… Confirm save location\n",
    "print(f\"ğŸ“ Saved cleaned climate data for Dublin Airport to: {DATA_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe3644",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Step 6 â€“ Validate Saved CSV Against Stepâ€¯4 Output\n",
    "\n",
    "Instead of reâ€‘printing the same inspection results, this step **confirms that the locally saved CSV file is identical to the hourly dataset inspected in Stepâ€¯4**.  \n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸ“‚ Reloading the locally saved CSV (`data/dublin_airport_hourly.csv`)  \n",
    "- ğŸŒ Reloading the online CSV directly from Met Ã‰ireann (skipping metadata lines)  \n",
    "- âœ… Comparing the two DataFrames with `equals()` to check for exact match  \n",
    "- ğŸ“Š Printing a simple confirmation message and shape comparison  \n",
    "\n",
    "ğŸ“Œ *Why this matters:* This validation ensures reproducibility. It proves that the cleaned hourly file saved in Stepâ€¯5 is a faithful copy of the dataset originally inspected in Stepâ€¯4.  \n",
    "Reviewers can trust that all downstream analysis is based on the same consistent dataset.  \n",
    "For Dublin Airport analysis, this step is especially important because **hourly granularity** is required to align weather conditions with arrivals and departures at specific times.  \n",
    "\n",
    "ğŸ“– Reference:  \n",
    "- [GeeksforGeeks â€“ Create Effective and Reproducible Code Using Pandas](https://www.geeksforgeeks.org/create-effective-and-reproducible-code-using-pandas/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53874e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Validation successful: Local CSV matches the online dataset (structured data is consistent).\n",
      "Local shape: (708577, 21), Online shape: (708577, 21)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ Step 6 â€“ Validate Saved CSV Against Step 4 Output (DataFrame Equality)\n",
    "\n",
    "# --- Reload the locally saved CSV (hourly Dublin Airport data) ---\n",
    "df_local = pd.read_csv(\"data/dublin_airport_hourly.csv\", low_memory=False)\n",
    "\n",
    "# --- Reload the online CSV (using header_index from Step 4) ---\n",
    "df_online = pd.read_csv(\n",
    "    \"https://cli.fusio.net/cli/climate_data/webdata/hly532.csv\",\n",
    "    skiprows=header_index,\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# âœ… Compare the two DataFrames for structural equality\n",
    "if df_local.equals(df_online):\n",
    "    print(\"âœ… Validation successful: Local CSV matches the online dataset (structured data is consistent).\")\n",
    "else:\n",
    "    print(\"âŒ Validation failed: Local CSV differs from the online dataset after parsing.\")\n",
    "\n",
    "# --- Optional: show shape comparison for transparency ---\n",
    "print(f\"Local shape: {df_local.shape}, Online shape: {df_online.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b75c65",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 7 â€“ Enhance Dublin Airport Hourly Weather Data with Seasons\n",
    "\n",
    "After validating the saved hourly dataset, this step enriches the data by preparing timestamps, cleaning numeric weather columns, and tagging each record with its Irish meteorological season.  \n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸ“‚ Loading the locally saved hourly CSV (`data/dublin_airport_hourly.csv`)  \n",
    "- ğŸ•’ Preparing a full `datetime` column by combining `date` and `time`  \n",
    "- ğŸ› ï¸ Cleaning mixedâ€‘type weather columns (`rain`, `temp`, `wdsp`, `msl`) into consistent numeric values  \n",
    "- ğŸ‚ Adding a `season` column using the `get_season_for_date` helper  \n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "By making the dataset **seasonâ€‘aware**, you can easily filter and analyse weather conditions and flight delays by season. This ensures that downstream analysis captures both the **hourly granularity** and the **seasonal context**, which are critical for understanding operational impacts at Dublin Airport.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9f431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows with season tagging (full schema):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>wddir</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>ww</th>\n",
       "      <th>w</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1945-01-01 00:00:00</td>\n",
       "      <td>4.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1945-01-01 01:00:00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1945-01-01 02:00:00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4800</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1945-01-01 03:00:00</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6000</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1945-01-01 04:00:00</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>250</td>\n",
       "      <td>6000</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1945-01-01 05:00:00</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>270</td>\n",
       "      <td>7000</td>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1945-01-01 06:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>270</td>\n",
       "      <td>4000</td>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1945-01-01 07:00:00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9</td>\n",
       "      <td>260</td>\n",
       "      <td>4000</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1945-01-01 08:00:00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>250</td>\n",
       "      <td>10000</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1945-01-01 09:00:00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>260</td>\n",
       "      <td>20000</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  temp  rain  wdsp wddir    vis clht  ww  w  season\n",
       "0 1945-01-01 00:00:00   4.9   0.0     0     0    200    2  50  4  Winter\n",
       "1 1945-01-01 01:00:00   5.1   0.0     0     0    200    2  45  4  Winter\n",
       "2 1945-01-01 02:00:00   5.1   0.0     0     0   4800    4  50  4  Winter\n",
       "3 1945-01-01 03:00:00   5.2   0.2     0     0   6000    4  50  4  Winter\n",
       "4 1945-01-01 04:00:00   5.6   0.0     7   250   6000    4  50  5  Winter\n",
       "5 1945-01-01 05:00:00   5.6   0.0     9   270   7000   12  51  5  Winter\n",
       "6 1945-01-01 06:00:00   6.0   0.0     9   270   4000   12  51  4  Winter\n",
       "7 1945-01-01 07:00:00   6.1   0.1     9   260   4000   13  50  5  Winter\n",
       "8 1945-01-01 08:00:00   6.1   0.0     9   250  10000   18  20  5  Winter\n",
       "9 1945-01-01 09:00:00   6.1   0.0     5   260  20000   23   2  5  Winter"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ğŸ“Š Step 7 â€“ Enhance Dublin Airport Hourly Weather Data with Seasons\n",
    "\n",
    "# --- Load the locally saved hourly CSV ---\n",
    "df_weather = load_cleaned_weather_data()\n",
    "\n",
    "# --- Prepare datetime column (parse combined date+time) ---\n",
    "df_weather = prepare_weather_data(df_weather)\n",
    "\n",
    "# --- Clean numeric weather columns (rain, temp, wind, pressure, etc.) ---\n",
    "df_weather = clean_weather_columns(df_weather)\n",
    "\n",
    "# --- Vectorised season assignment ---\n",
    "df_weather = assign_season_vectorized(df_weather, datetime_col=\"datetime\")\n",
    "\n",
    "# --- Restrict to unified weather schema ---\n",
    "df_weather = df_weather[[c for c in delay_relevant_weather_cols if c in df_weather.columns]]\n",
    "\n",
    "# âœ… Inspect result (all relevant columns)\n",
    "print(\"First 10 rows with season tagging (full schema):\")\n",
    "display(df_weather.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af503292",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 8 â€“ Download and Save Flight Activity Data\n",
    "\n",
    "In this step, the notebook retrieves and stores **flight activity data** for Dublin Airport.  \n",
    "This dataset will later be aligned with Met Ã‰ireann weather observations to analyse how conditions such as rain, wind, and visibility impact flight punctuality.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸŒ Collecting flight schedules and activity logs (arrivals, departures, delays, cancellations) from public APIs or dashboards  \n",
    "- ğŸ“‚ Defining a local output path (`data/dublin_airport_flights.csv`) to store the file inside the projectâ€™s `data` folder  \n",
    "- âœ… Checking the response to ensure the download or export was successful  \n",
    "- ğŸ“‘ Parsing the raw data into a structured format, including scheduled vs actual times and delay minutes  \n",
    "- ğŸ“ Saving a cleaned version of the dataset locally for reproducibility and future analysis  \n",
    "\n",
    "ğŸ“Œ *Why this matters:* Having flight activity data stored locally ensures that the project can consistently align flight events with weather conditions.  \n",
    "It also supports reproducibility, version control, and enables predictive modelling of delays and cancellations without repeatedly querying external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d600a9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-05-21 to 2025-11-19\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‘ Step 8 â€“ Download and Save Flight Activity Data\n",
    "# --- Compute date range for the past six months ---\n",
    "today = date.today()\n",
    "six_months_ago = today - timedelta(days=182)  # approx 6 months\n",
    "\n",
    "DATE_FROM = six_months_ago.isoformat()\n",
    "DATE_TO = today.isoformat()\n",
    "\n",
    "# --- Output directories ---\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DIR = DATA_DIR / \"raw_flights\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Date range: {DATE_FROM} to {DATE_TO}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79b275",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 9 â€“ Dublin Airport flight information analysis \n",
    "\n",
    "This cell prepares the environment for **Dublin Airport flight information analysis** by defining key date ranges and output directories:\n",
    "\n",
    "- ğŸ—“ï¸ **Date range:**  \n",
    "  - Calculates todayâ€™s date and subtracts ~six months (182 days) to define the analysis window.  \n",
    "  - Converts both dates into ISO format (`YYYY-MM-DD`) for use in API queries.  \n",
    "  - These values (`DATE_FROM`, `DATE_TO`) specify the sixâ€‘month period of **flight activity data** (arrivals, departures, delays, cancellations) to be downloaded.\n",
    "\n",
    "- ğŸ“‚ **Output directories:**  \n",
    "  - Creates a root `data/` folder for project storage.  \n",
    "  - Inside it, a `raw_flights/` subfolder is created to hold raw JSON files retrieved from the Aviation Edge API.  \n",
    "  - This ensures reproducibility and a clear separation between raw flight inputs and processed datasets.\n",
    "\n",
    "- âœ… **Checkpoint:**  \n",
    "  - Prints the computed date range so you can confirm the correct sixâ€‘month window before downloading flight information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a82153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2025-05-21 to 2025-11-19\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‘ Step 9 â€“ Dublin Airport flight information analysis \n",
    "# --- Compute date range for the past six months ---\n",
    "today = date.today()\n",
    "six_months_ago = today - timedelta(days=182)  # approx 6 months\n",
    "\n",
    "DATE_FROM = six_months_ago.isoformat()\n",
    "DATE_TO = today.isoformat()\n",
    "\n",
    "# --- Output directories ---\n",
    "DATA_DIR = Path(\"data\")\n",
    "RAW_DIR = DATA_DIR / \"raw_flights\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Date range: {DATE_FROM} to {DATE_TO}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb27b0",
   "metadata": {},
   "source": [
    "### âœˆï¸ Step 10 â€” Download Six Months of Flight History for Dublin (Arrivals and Departures)\n",
    "\n",
    "In this step we use **Aviation Edgeâ€™s Flights History API** to collect six months of flight schedules for Dublin Airport (IATA: DUB).  \n",
    "The endpoint provides detailed records for each flight, including:\n",
    "\n",
    "- **Scheduled, estimated, and actual times** (departure and arrival)\n",
    "- **Delay minutes** (either reported or inferred)\n",
    "- **Flight status** (e.g., scheduled, landed, cancelled, diverted)\n",
    "- **Airline and flight identifiers**\n",
    "\n",
    "We request **both arrivals and departures** for the date range **2025â€‘05â€‘20 to 2025â€‘11â€‘18**, ensuring coverage of the most recent six months.  \n",
    "The raw JSON files are saved for reproducibility in the folder:\n",
    "\n",
    "- `data/raw_flights/dub_arrival_history.json`  \n",
    "- `data/raw_flights/dub_departure_history.json`\n",
    "\n",
    "Additionally, a `fetch_log.txt` file is generated to record progress, errors, and confirmation of successful downloads.  \n",
    "This log provides transparency and makes troubleshooting easier if API requests fail or return incomplete data.\n",
    "\n",
    "**Important notes for reproducibility:**\n",
    "- The code cell was executed on **18 November 2025** using a private API key from Aviation Edge.\n",
    "- To run the download yourself, you must:\n",
    "  1. Sign up for an account at [aviation-edge.com](https://aviation-edge.com/) and obtain an API key.\n",
    "  2. Store the key securely (e.g., as an environment variable).\n",
    "  3. Set the notebook control flag `RUN_DOWNLOAD = True` to enable downloading.\n",
    "- By default, the notebook will skip downloading if `RUN_DOWNLOAD = False`, and instead use the existing JSON files.  \n",
    "  This prevents unnecessary API calls and ensures consistent results for reviewers.\n",
    "\n",
    "âš ï¸ **Best practice:** Only reâ€‘run the download when you want to refresh the dataset.  \n",
    "Frequent downloads are unnecessary and may exceed API rate limits.\n",
    "\n",
    "**References:**\n",
    "- [Aviation Edge official site](https://aviation-edge.com/)  \n",
    "- [Aviation Edge API documentation on GitHub](https://github.com/AviationEdgeAPI/Aviation-Edge-Complete-API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c68251a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â© Skipping download step (RUN_DOWNLOAD=False). Using existing JSON files.\n"
     ]
    }
   ],
   "source": [
    "# âœˆï¸ Step 10 â€” Download Six Months of Flight History for Dublin (Arrivals and Departures)\n",
    "# --- API setup ---\n",
    "API_KEY = os.getenv(\"AVIATION_EDGE_API_KEY\")   # Read API key from environment variable\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"API key not found. Please set AVIATION_EDGE_API_KEY.\")\n",
    "\n",
    "BASE_URL = \"https://aviation-edge.com/v2/public/flightsHistory\"  # Endpoint for flight history\n",
    "IATA_CODE = \"DUB\"  # Airport code for Dublin\n",
    "\n",
    "# --- Directory setup ---\n",
    "DATA_DIR = Path(\"data\")              # Root data folder\n",
    "RAW_DIR = DATA_DIR / \"raw_flights\"   # Subfolder for raw flight data\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)  # Create folders if missing\n",
    "\n",
    "# --- Log file path ---\n",
    "LOG_FILE = RAW_DIR / \"fetch_log.txt\"  # Text log for progress and errors\n",
    "\n",
    "def log_message(message: str):\n",
    "    \"\"\"Print message and append to log file for tracking progress.\"\"\"\n",
    "    print(message)\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as log:\n",
    "        log.write(message + \"\\n\")\n",
    "\n",
    "def fetch_day(iata_code: str, flight_type: str, day: date, retries: int = 3):\n",
    "    \"\"\"\n",
    "    Fetch flight history for a single day (arrival/departure).\n",
    "    Retries up to 'retries' times if errors occur.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"key\": API_KEY,\n",
    "        \"code\": iata_code,\n",
    "        \"type\": flight_type,\n",
    "        \"date_from\": day.isoformat(),\n",
    "        \"date_to\": day.isoformat()\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        resp = requests.get(BASE_URL, params=params, timeout=60)  # API request\n",
    "        if resp.status_code == 200:\n",
    "            try:\n",
    "                data = resp.json()  # Parse JSON response\n",
    "                log_message(f\"âœ… {flight_type.capitalize()} {day}: {len(data)} records fetched\")\n",
    "                return data\n",
    "            except Exception:\n",
    "                log_message(f\"âš ï¸ Non-JSON response on {day}: {resp.text[:200]}\")\n",
    "                return []\n",
    "        else:\n",
    "            wait = 2 ** attempt  # Exponential backoff\n",
    "            log_message(f\"âš ï¸ Error {resp.status_code} on {day} (attempt {attempt+1}/{retries}). Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    log_message(f\"âŒ Failed after {retries} retries on {day}\")\n",
    "    return []\n",
    "\n",
    "def fetch_history(iata_code: str, flight_type: str, start_date: date, end_date: date):\n",
    "    \"\"\"\n",
    "    Loop through each day in the date range and fetch daily history.\n",
    "    Append results into one cumulative JSON file (avoids overwriting with empty data).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_days = (end_date - start_date).days + 1\n",
    "    filename = RAW_DIR / f\"{iata_code.lower()}_{flight_type}_history.json\"\n",
    "\n",
    "    # Load existing cumulative file if present\n",
    "    if filename.exists():\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                results = json.load(f)\n",
    "            except Exception:\n",
    "                results = []\n",
    "                log_message(f\"âš ï¸ Existing {filename.name} could not be read, starting fresh.\")\n",
    "\n",
    "    # Loop through each day in range\n",
    "    for i in range(total_days):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        log_message(f\"Day {i+1}/{total_days}: {day}\")\n",
    "        day_data = fetch_day(iata_code, flight_type, day)\n",
    "\n",
    "        # Save only if data was fetched\n",
    "        if day_data:\n",
    "            results.extend(day_data)\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            log_message(f\"ğŸ’¾ Saved {len(day_data)} records for {day} into {filename.name}\")\n",
    "        else:\n",
    "            log_message(f\"â© Skipped saving {day}, no data returned\")\n",
    "\n",
    "        time.sleep(1)  # Pause politely between requests\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Conditional download control ---\n",
    "if RUN_DOWNLOAD:\n",
    "    # Define date range (last ~6 months)\n",
    "    today = date.today()\n",
    "    start_date = today - timedelta(days=182)\n",
    "    end_date = today\n",
    "\n",
    "    log_message(f\"Fetching flights from {start_date} to {end_date} for {IATA_CODE}...\")\n",
    "\n",
    "    # Fetch arrivals and departures\n",
    "    arrivals = fetch_history(IATA_CODE, \"arrival\", start_date, end_date)\n",
    "    departures = fetch_history(IATA_CODE, \"departure\", start_date, end_date)\n",
    "\n",
    "    log_message(f\"âœ… Completed: {len(arrivals)} arrivals and {len(departures)} departures fetched.\")\n",
    "else:\n",
    "    log_message(\"â© Skipping download step (RUN_DOWNLOAD=False). Using existing JSON files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a966b",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Step 11 â€“ Inspect Headings of Downloaded JSON Files\n",
    "\n",
    "Before tidying the flight history data, itâ€™s important to **inspect the structure of the raw JSON files**.  \n",
    "The Aviation Edge API responses can vary depending on whether the file contains arrivals or departures, and not all fields are always present.\n",
    "\n",
    "**What this step does:**\n",
    "- Loads the raw JSON files for Dublin Airport arrivals (`dub_arrival_history.json`) and departures (`dub_departure_history.json`).\n",
    "- Uses `pandas.json_normalize` to flatten the nested JSON into a tabular structure.\n",
    "- Prints out all available column headings so we can see which fields exist.\n",
    "- Shows a sample record (truncated for readability) to preview the nested structure.\n",
    "\n",
    "**Why this matters:**\n",
    "- Helps identify which fields are consistently available and relevant for analysis.\n",
    "- Prevents errors later by ensuring we only select columns that actually exist.\n",
    "- Guides the design of the tidy DataFrame schema in Stepâ€¯11 (e.g. keeping scheduled/actual times, delays, status, airline, etc., while dropping baggage or codeshare metadata).\n",
    "\n",
    "ğŸ‘‰ This inspection step is a diagnostic tool: it gives us visibility into the raw data so we can confidently build the parsing logic in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ff4cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Keys in dub_arrival_history.json ---\n",
      "['airline.iataCode', 'airline.icaoCode', 'airline.name', 'arrival.actualRunway', 'arrival.actualTime', 'arrival.baggage', 'arrival.delay', 'arrival.estimatedRunway', 'arrival.estimatedTime', 'arrival.gate', 'arrival.iataCode', 'arrival.icaoCode', 'arrival.scheduledTime', 'arrival.terminal', 'codeshared.airline.iataCode', 'codeshared.airline.icaoCode', 'codeshared.airline.name', 'codeshared.flight.iataNumber', 'codeshared.flight.icaoNumber', 'codeshared.flight.number', 'departure.actualRunway', 'departure.actualTime', 'departure.delay', 'departure.estimatedRunway', 'departure.estimatedTime', 'departure.gate', 'departure.iataCode', 'departure.icaoCode', 'departure.scheduledTime', 'departure.terminal', 'flight.iataNumber', 'flight.icaoNumber', 'flight.number', 'status', 'type']\n",
      "\n",
      "Sample record:\n",
      "{\n",
      "  \"type\": \"arrival\",\n",
      "  \"status\": \"landed\",\n",
      "  \"departure\": {\n",
      "    \"iataCode\": \"vlc\",\n",
      "    \"icaoCode\": \"levc\",\n",
      "    \"terminal\": \"1\",\n",
      "    \"gate\": \"3\",\n",
      "    \"delay\": 49,\n",
      "    \"scheduledTime\": \"2025-05-19t23:10:00.000\",\n",
      "    \"estimatedTime\": \"2025-05-19t23:10:00.000\",\n",
      "    \"actualTime\": \"2025-05-19t23:58:00.000\",\n",
      "    \"estimatedRunway\": \"2025-05-19t23:58:00.000\",\n",
      "    \"actualRunway\": \"2025-05-19t23:58:00.000\"\n",
      "  },\n",
      "  \"arrival\": {\n",
      "    \"iataCode\": \"dub\",\n",
      "    \"icaoCode\": \"eidw\",\n",
      "    \"terminal\": \"t1\",\n",
      "    \"bagga\n",
      "\n",
      "--- Keys in dub_departure_history.json ---\n",
      "['airline.iataCode', 'airline.icaoCode', 'airline.name', 'arrival.actualRunway', 'arrival.actualTime', 'arrival.baggage', 'arrival.delay', 'arrival.estimatedRunway', 'arrival.estimatedTime', 'arrival.gate', 'arrival.iataCode', 'arrival.icaoCode', 'arrival.scheduledTime', 'arrival.terminal', 'codeshared.airline.iataCode', 'codeshared.airline.icaoCode', 'codeshared.airline.name', 'codeshared.flight.iataNumber', 'codeshared.flight.icaoNumber', 'codeshared.flight.number', 'departure.actualRunway', 'departure.actualTime', 'departure.delay', 'departure.estimatedRunway', 'departure.estimatedTime', 'departure.gate', 'departure.iataCode', 'departure.icaoCode', 'departure.scheduledTime', 'departure.terminal', 'flight.iataNumber', 'flight.icaoNumber', 'flight.number', 'status', 'type']\n",
      "\n",
      "Sample record:\n",
      "{\n",
      "  \"type\": \"departure\",\n",
      "  \"status\": \"active\",\n",
      "  \"departure\": {\n",
      "    \"iataCode\": \"dub\",\n",
      "    \"icaoCode\": \"eidw\",\n",
      "    \"delay\": 5,\n",
      "    \"scheduledTime\": \"2025-05-20t02:20:00.000\",\n",
      "    \"estimatedTime\": \"2025-05-20t02:31:00.000\",\n",
      "    \"actualTime\": \"2025-05-20t02:24:00.000\",\n",
      "    \"estimatedRunway\": \"2025-05-20t02:24:00.000\",\n",
      "    \"actualRunway\": \"2025-05-20t02:24:00.000\"\n",
      "  },\n",
      "  \"arrival\": {\n",
      "    \"iataCode\": \"fra\",\n",
      "    \"icaoCode\": \"eddf\",\n",
      "    \"scheduledTime\": \"2025-05-20t05:15:00.000\",\n",
      "    \"estimatedTime\": \n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ Step 11 - inspect headings of downloaded JSON files\n",
    "RAW_DIR = Path(\"data\") / \"raw_flights\"\n",
    "ARR_FILE = RAW_DIR / \"dub_arrival_history.json\"\n",
    "DEP_FILE = RAW_DIR / \"dub_departure_history.json\"\n",
    "\n",
    "def inspect_keys(json_file, sample_size=50):\n",
    "    \"\"\"Inspect nested keys in a JSON file by sampling records.\"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    # Use pandas.json_normalize to flatten structure\n",
    "    import pandas as pd\n",
    "    df = pd.json_normalize(records)\n",
    "\n",
    "    # Show all column headings\n",
    "    print(f\"\\n--- Keys in {json_file.name} ---\")\n",
    "    print(sorted(df.columns.tolist()))\n",
    "\n",
    "    # Optionally preview first record\n",
    "    print(\"\\nSample record:\")\n",
    "    print(json.dumps(records[0], indent=2)[:500])  # truncate for readability\n",
    "\n",
    "# Inspect both files\n",
    "inspect_keys(ARR_FILE)\n",
    "inspect_keys(DEP_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36440fc5",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Step 12 â€“ Parse Dublin Airport Flight History JSON into Tidy DataFrames (Reduced Schema)\n",
    "\n",
    "The raw flight history data (arrivals and departures) is stored as nested JSON.  \n",
    "To make it usable for analysis and compatible with the weather dataset, we streamline  \n",
    "the schema to include only the essential fields needed for delay vs. weather analysis.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "1. ğŸ“¥ **Safely extract nested fields** using a custom `_safe_get` function, avoiding `json_normalize` issues.  \n",
    "2. ğŸ•’ **Parse scheduled timestamps (`sched`) into proper datetimes** to anchor each flight record.  \n",
    "3. â± **Create a `datetime_hour` column floored to the hour**, ensuring alignment with the hourly weather dataset.  \n",
    "4. â± **Compute `delay_calc`**:  \n",
    "   - Use the explicit `delay` field if available.  \n",
    "   - Leave as `NaN` if no reliable delay information is present.  \n",
    "5. ğŸ›  **Fill missing categorical fields** (`airline`, `flight_iata`) with placeholders for clarity.  \n",
    "6. ğŸš« **Flag cancelled flights** with an `is_cancelled` column for downstream filtering.  \n",
    "7. âœˆï¸ **Tag each record with its type** (`arrival` or `departure`) and combine into a single `df_flights` DataFrame.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "By reducing to a minimal set of columns, this step minimises missing values while  \n",
    "retaining all variables necessary for analysis. The resulting tidy DataFrame is  \n",
    "clean, reproducible, and ready to merge with weather data in Stepâ€¯17.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e1522b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrivals shape: (131556, 10)\n",
      "Departures shape: (137720, 10)\n",
      "Combined shape: (269276, 10)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ Step 12 â€“ Parse Flight History JSON into Tidy DataFrames\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DIR = Path(\"data\") / \"raw_flights\"\n",
    "ARR_FILE = RAW_DIR / \"dub_arrival_history.json\"\n",
    "DEP_FILE = RAW_DIR / \"dub_departure_history.json\"\n",
    "\n",
    "def _safe_get(d, path):\n",
    "    \"\"\"Get nested value from dict using dotted path, return None if missing.\"\"\"\n",
    "    if d is None:\n",
    "        return None\n",
    "    parts = path.split(\".\")\n",
    "    val = d\n",
    "    for p in parts:\n",
    "        if isinstance(val, dict) and p in val:\n",
    "            val = val[p]\n",
    "        else:\n",
    "            return None\n",
    "    return val\n",
    "\n",
    "def parse_flights(json_file, flight_type=\"arrival\"):\n",
    "    \"\"\"\n",
    "    Load a JSON file (arrivals or departures) and return a tidy DataFrame.\n",
    "    Extract only the fields needed for the essential flight schema.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        records = json.load(f)\n",
    "\n",
    "    if not records:  # safeguard against empty files\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Minimal mapping: only fields required for schema\n",
    "    if flight_type == \"arrival\":\n",
    "        mapping = {\n",
    "            \"flight_iata\": \"flight.iataNumber\",\n",
    "            \"airline\": \"airline.name\",\n",
    "            \"status\": \"status\",\n",
    "            \"sched\": \"arrival.scheduledTime\",\n",
    "            \"delay\": \"arrival.delay\",\n",
    "        }\n",
    "    else:\n",
    "        mapping = {\n",
    "            \"flight_iata\": \"flight.iataNumber\",\n",
    "            \"airline\": \"airline.name\",\n",
    "            \"status\": \"status\",\n",
    "            \"sched\": \"departure.scheduledTime\",\n",
    "            \"delay\": \"departure.delay\",\n",
    "        }\n",
    "\n",
    "    rows = []\n",
    "    for rec in records:\n",
    "        row = {}\n",
    "        for col, path in mapping.items():\n",
    "            row[col] = _safe_get(rec, path)\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Parse scheduled time into datetime\n",
    "    df[\"sched\"] = pd.to_datetime(df[\"sched\"], errors=\"coerce\")\n",
    "\n",
    "    # Keep both exact datetime and floored hour\n",
    "    df[\"datetime\"] = df[\"sched\"]                # exact timestamp (same as sched in reduced schema)\n",
    "    df[\"datetime_hour\"] = df[\"sched\"].dt.floor(\"h\")\n",
    "\n",
    "    # Compute delay_calc from raw delay\n",
    "    df[\"delay_calc\"] = pd.to_numeric(df[\"delay\"], errors=\"coerce\")\n",
    "\n",
    "    # Fill categorical placeholders\n",
    "    df[\"airline\"] = df[\"airline\"].fillna(\"Unknown\")\n",
    "    df[\"flight_iata\"] = df[\"flight_iata\"].fillna(\"UNK\")\n",
    "\n",
    "    # Cancellation flag\n",
    "    df[\"is_cancelled\"] = df.get(\"status\", pd.Series(dtype=\"object\")).astype(str).str.lower().eq(\"cancelled\")\n",
    "\n",
    "    # Flight type (arrival or departure)\n",
    "    df[\"type\"] = flight_type\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Load both arrivals and departures ---\n",
    "df_arrivals = parse_flights(ARR_FILE, \"arrival\")\n",
    "df_departures = parse_flights(DEP_FILE, \"departure\")\n",
    "\n",
    "# --- Combine into one DataFrame ---\n",
    "df_flights = pd.concat([df_arrivals, df_departures], ignore_index=True, sort=False)\n",
    "\n",
    "print(\"Arrivals shape:\", df_arrivals.shape)\n",
    "print(\"Departures shape:\", df_departures.shape)\n",
    "print(\"Combined shape:\", df_flights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebf6ed",
   "metadata": {},
   "source": [
    "### ğŸ“‘ Step 12a â€“ Define Essential Flight Schema (for Delay Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "525874f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flight dataset reduced to essential schema for delay analysis\n",
      "                sched            datetime       datetime_hour  delay  \\\n",
      "0 2025-05-20 01:00:00 2025-05-20 01:00:00 2025-05-20 01:00:00   15.0   \n",
      "1 2025-05-20 01:10:00 2025-05-20 01:10:00 2025-05-20 01:00:00    NaN   \n",
      "2 2025-05-20 01:15:00 2025-05-20 01:15:00 2025-05-20 01:00:00    NaN   \n",
      "3 2025-05-20 04:25:00 2025-05-20 04:25:00 2025-05-20 04:00:00    NaN   \n",
      "4 2025-05-20 04:25:00 2025-05-20 04:25:00 2025-05-20 04:00:00    NaN   \n",
      "\n",
      "   delay_calc            airline flight_iata  status  is_cancelled     type  \n",
      "0        15.0            ryanair      fr1739  landed         False  arrival  \n",
      "1         NaN            ryanair      fr9612  landed         False  arrival  \n",
      "2         NaN            ryanair       fr651  landed         False  arrival  \n",
      "3         NaN  american airlines      aa8330  landed         False  arrival  \n",
      "4         NaN    british airways      ba6124  landed         False  arrival  \n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‘ Step 12a â€“ Define Essential Flight Schema (for Delay Analysis)\n",
    "\n",
    "essential_flight_cols = [\n",
    "    \"sched\",         # scheduled departure/arrival time\n",
    "    \"datetime\",      # canonical timestamp (same as sched here)\n",
    "    \"datetime_hour\", # floored datetime for weather alignment\n",
    "    \"delay\",         # raw delay from API\n",
    "    \"delay_calc\",    # cleaned/calculated delay (minutes)\n",
    "    \"airline\",       # airline name\n",
    "    \"flight_iata\",   # flight identifier (IATA code)\n",
    "    \"status\",        # flight status (landed, scheduled, cancelled)\n",
    "    \"is_cancelled\",  # boolean flag for cancellations\n",
    "    \"type\"           # flight type (arrival or departure)\n",
    "]\n",
    "\n",
    "# --- Restrict DataFrame to essential flight schema ---\n",
    "df_flights = df_flights[[c for c in essential_flight_cols if c in df_flights.columns]]\n",
    "\n",
    "print(\"âœ… Flight dataset reduced to essential schema for delay analysis\")\n",
    "print(df_flights.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bd8799",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Step 12b â€“ Postâ€‘Cleaning Adjustments for Flights\n",
    "\n",
    "After parsing the raw JSON into tidy DataFrames, a few final adjustments make the flight dataset\n",
    "cleaner and more consistent for analysis:\n",
    "\n",
    "1. ğŸ›  **Fill categorical placeholders**  \n",
    "   - Replace missing values in `airline` with `\"Unknown\"` and in `flight_iata` with `\"UNK\"`.  \n",
    "   - This ensures reviewers donâ€™t encounter raw NaNs in key categorical fields.  \n",
    "   - (Note: `terminal` is excluded from the reduced schema, so no placeholder is required.)\n",
    "\n",
    "2. ğŸš« **Handle cancelled flights**  \n",
    "   - For records flagged as cancelled, set `delay_calc = 0`.  \n",
    "   - This keeps the delay column numeric and avoids misleading nulls.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "These adjustments improve readability and consistency. By ensuring categorical fields are always populated\n",
    "and cancelled flights have a defined delay value, the dataset becomes more reviewerâ€‘friendly and ready\n",
    "for merging with weather data in Stepâ€¯17.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5edb63eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Flight dataset cleaned and aligned\n",
      "                sched            datetime       datetime_hour  delay  \\\n",
      "0 2025-05-20 01:00:00 2025-05-20 01:00:00 2025-05-20 01:00:00   15.0   \n",
      "1 2025-05-20 01:10:00 2025-05-20 01:10:00 2025-05-20 01:00:00    NaN   \n",
      "2 2025-05-20 01:15:00 2025-05-20 01:15:00 2025-05-20 01:00:00    NaN   \n",
      "3 2025-05-20 04:25:00 2025-05-20 04:25:00 2025-05-20 04:00:00    NaN   \n",
      "4 2025-05-20 04:25:00 2025-05-20 04:25:00 2025-05-20 04:00:00    NaN   \n",
      "\n",
      "   delay_calc            airline flight_iata  status  is_cancelled     type  \n",
      "0        15.0            ryanair      fr1739  landed         False  arrival  \n",
      "1         NaN            ryanair      fr9612  landed         False  arrival  \n",
      "2         NaN            ryanair       fr651  landed         False  arrival  \n",
      "3         NaN  american airlines      aa8330  landed         False  arrival  \n",
      "4         NaN    british airways      ba6124  landed         False  arrival  \n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ Step 12b â€“ Post-cleaning Adjustments for Flights (Refined)\n",
    "\n",
    "# Fill categorical placeholders\n",
    "df_flights['airline'] = df_flights['airline'].fillna(\"Unknown\")\n",
    "df_flights['flight_iata'] = df_flights['flight_iata'].fillna(\"UNK\")\n",
    "\n",
    "# For cancelled flights, set delay_calc = 0\n",
    "df_flights.loc[df_flights['is_cancelled'], 'delay_calc'] = 0\n",
    "\n",
    "# Keep raw 'delay' untouched for transparency\n",
    "# (only drop it later if reviewers decide it's unnecessary)\n",
    "\n",
    "print(\"âœ… Flight dataset cleaned and aligned\")\n",
    "print(df_flights[['sched','datetime','datetime_hour','delay','delay_calc',\n",
    "                  'airline','flight_iata','status','is_cancelled','type']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef302a60",
   "metadata": {},
   "source": [
    "### ğŸ“‚ Step 13 â€“ Save Tidy Flight DataFrames\n",
    "\n",
    "After parsing and cleaning the raw JSON flight history, we now persist the tidy DataFrames\n",
    "to disk as reproducible checkpoints in the workflow.\n",
    "\n",
    "This step performs three key actions:\n",
    "\n",
    "1. ğŸ’¾ **Save arrivals** â†’ `dub_arrivals_tidy.csv`  \n",
    "   - Contains all parsed and cleaned arrival records.\n",
    "\n",
    "2. ğŸ’¾ **Save departures** â†’ `dub_departures_tidy.csv`  \n",
    "   - Contains all parsed and cleaned departure records.\n",
    "\n",
    "3. ğŸ’¾ **Save combined dataset** â†’ `dub_flights_tidy.csv`  \n",
    "   - Concatenates arrivals and departures into a single file for holistic analysis.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "Saving tidy datasets ensures that the parsing and cleaning steps donâ€™t need to be rerun each time.  \n",
    "They serve as transparent, versionâ€‘controlled artifacts that can be shared with reviewers or reused\n",
    "in later steps (e.g., merging with weather data in Stepâ€¯17).  \n",
    "If the `data/` folder doesnâ€™t exist, it is created automatically, making the process robust and portable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac5d01f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved tidy datasets into data/ folder\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ Step 13 â€“ Save Tidy Flight DataFrames\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)  # ensure folder exists\n",
    "\n",
    "# Save arrivals (raw form, includes all extracted fields)\n",
    "df_arrivals.to_csv(DATA_DIR / \"dub_arrivals_tidy.csv\", index=False)\n",
    "\n",
    "# Save departures (raw form, includes all extracted fields)\n",
    "df_departures.to_csv(DATA_DIR / \"dub_departures_tidy.csv\", index=False)\n",
    "\n",
    "# Save combined dataset (already restricted to essential schema in Step 12a)\n",
    "df_flights.to_csv(DATA_DIR / \"dub_flights_tidy.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved tidy datasets into data/ folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669cc69",
   "metadata": {},
   "source": [
    "### ğŸŒ¦ï¸ Step 14 â€“ Define Meteorological Seasons in Ireland for 2025\n",
    "\n",
    "Weather analysis often benefits from grouping observations into seasonal periods.  \n",
    "In Ireland, meteorological seasons are defined as:\n",
    "\n",
    "- ğŸŒ± **Spring** â†’ March, April, May  \n",
    "- â˜€ï¸ **Summer** â†’ June, July, August  \n",
    "- ğŸ‚ **Autumn** â†’ September, October, November  \n",
    "- â„ï¸ **Winter** â†’ December, January, February  \n",
    "\n",
    "This step uses a helper function (`define_irish_seasons`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd4bc4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… Irish Seasons for 2025:\n",
      "  Winter: 01-Dec-2024 00:00 â†’ 28-Feb-2025 23:59\n",
      "  Spring: 01-Mar-2025 00:00 â†’ 31-May-2025 23:59\n",
      "  Summer: 01-Jun-2025 00:00 â†’ 31-Aug-2025 23:59\n",
      "  Autumn: 01-Sep-2025 00:00 â†’ 30-Nov-2025 23:59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Winter</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>2025-02-28 23:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spring</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>2025-05-31 23:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summer</td>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>2025-08-31 23:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Autumn</td>\n",
       "      <td>2025-09-01</td>\n",
       "      <td>2025-11-30 23:59:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season      start                 end\n",
       "0  Winter 2024-12-01 2025-02-28 23:59:00\n",
       "1  Spring 2025-03-01 2025-05-31 23:59:00\n",
       "2  Summer 2025-06-01 2025-08-31 23:59:00\n",
       "3  Autumn 2025-09-01 2025-11-30 23:59:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ğŸŒ¦ï¸ Step 14 â€“ Define Meteorological Seasons in Ireland for 2025\n",
    "\n",
    "# --- Generate seasonal boundaries using helper function ---\n",
    "seasons_2025 = define_irish_seasons()\n",
    "\n",
    "# --- Display formatted season ranges ---\n",
    "print(\"ğŸ“… Irish Seasons for 2025:\")\n",
    "for _, row in seasons_2025.iterrows():\n",
    "    print(f\"  {row['season']}: {row['start'].strftime('%d-%b-%Y %H:%M')} â†’ {row['end'].strftime('%d-%b-%Y %H:%M')}\")\n",
    "\n",
    "# --- Display the DataFrame ---    \n",
    "display(seasons_2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3df91",
   "metadata": {},
   "source": [
    "### ğŸ“† Step 15 â€“ Define and Validate a Custom Date Range (Dublin Weather)\n",
    "\n",
    "For targeted analysis, we often need to filter weather data to a specific custom date range.  \n",
    "This step validates the chosen range, checks its seasonal context, and applies it to the dataset.\n",
    "\n",
    "Key actions performed:\n",
    "\n",
    "1. ğŸ“… **Define custom range**  \n",
    "   - Use the helper `get_custom_range` to parse start and end dates.  \n",
    "   - Example: `\"2025-10-27\"` â†’ `\"2025-10-31 23:59\"`.\n",
    "\n",
    "2. âš ï¸ **Fallback handling**  \n",
    "   - If the helper returns invalid values, default dates are applied to avoid runtime errors.\n",
    "\n",
    "3. ğŸŒ¦ **Season validation**  \n",
    "   - Instead of stopping at the first match, the code now collects **all overlapping seasons**.  \n",
    "   - This captures cases where the range spans multiple seasons (e.g., Autumn â†’ Winter).  \n",
    "   - The result is printed as a commaâ€‘separated list of seasons.\n",
    "\n",
    "4. ğŸ›  **Prepare weather data**  \n",
    "   - Ensure a proper `datetime` column exists using `prepare_datetime`.  \n",
    "   - Apply `prepare_weather_data` to clean and align the dataset.\n",
    "\n",
    "5. ğŸ” **Filter by range**  \n",
    "   - Subset the Dublin Airport weather data to only include rows within the validated custom range.  \n",
    "   - Display the number of rows and preview the filtered dataset.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "By validating and filtering weather data to a custom range, we create a reproducible slice of the dataset  \n",
    "for focused analysis (e.g., examining delays during a specific week). Collecting all overlapping seasons  \n",
    "ensures reviewers understand the full seasonal context of the chosen period, which will be critical when  \n",
    "merging with flight data in Stepâ€¯17.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5658dd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“† The custom range overlaps with: Autumn\n",
      "âœ… Filtered Dublin weather data contains 120 rows from 2025-10-27 â†’ 2025-10-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>wddir</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>ww</th>\n",
       "      <th>w</th>\n",
       "      <th>season</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>708456</th>\n",
       "      <td>2025-10-27 00:00:00</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>260</td>\n",
       "      <td>20000</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708457</th>\n",
       "      <td>2025-10-27 01:00:00</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>270</td>\n",
       "      <td>20000</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708458</th>\n",
       "      <td>2025-10-27 02:00:00</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>270</td>\n",
       "      <td>20000</td>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708459</th>\n",
       "      <td>2025-10-27 03:00:00</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>270</td>\n",
       "      <td>20000</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708460</th>\n",
       "      <td>2025-10-27 04:00:00</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18</td>\n",
       "      <td>270</td>\n",
       "      <td>20000</td>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Autumn</td>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime  temp  rain  wdsp wddir    vis clht  ww   w  \\\n",
       "708456 2025-10-27 00:00:00   9.4   0.0    17   260  20000   27   2  11   \n",
       "708457 2025-10-27 01:00:00   9.2   0.0    17   270  20000   26   2  11   \n",
       "708458 2025-10-27 02:00:00   9.1   0.0    17   270  20000  999   2  11   \n",
       "708459 2025-10-27 03:00:00   9.4   0.0    15   270  20000   37   2  11   \n",
       "708460 2025-10-27 04:00:00   8.9   0.0    18   270  20000  999   2  11   \n",
       "\n",
       "        season        date  hour  \n",
       "708456  Autumn  2025-10-27     0  \n",
       "708457  Autumn  2025-10-27     1  \n",
       "708458  Autumn  2025-10-27     2  \n",
       "708459  Autumn  2025-10-27     3  \n",
       "708460  Autumn  2025-10-27     4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ğŸ“† Step 15 â€“ Define and Validate a Custom Date Range (Dublin Weather)\n",
    "\n",
    "# --- Define custom date range using helper ---\n",
    "custom_start, custom_end = get_custom_range(\"2025-10-27\", \"2025-10-31 23:59\")\n",
    "\n",
    "# --- Fallback to default if input is invalid ---\n",
    "if custom_start is None or custom_end is None:\n",
    "    print(\"âš ï¸ Invalid custom range returned by get_custom_range(); falling back to defaults.\")\n",
    "    custom_start = pd.Timestamp(\"2025-11-12\")\n",
    "    custom_end = pd.Timestamp(\"2025-11-16 23:59\")\n",
    "\n",
    "# --- Check which seasons the range overlaps ---\n",
    "matched_seasons = []\n",
    "for _, row in seasons_2025.iterrows():\n",
    "    # If either start or end falls inside a season, or the season fully covers the range\n",
    "    if (\n",
    "        (row[\"start\"] <= custom_start <= row[\"end\"]) or\n",
    "        (row[\"start\"] <= custom_end <= row[\"end\"]) or\n",
    "        (custom_start <= row[\"start\"] and custom_end >= row[\"end\"])\n",
    "    ):\n",
    "        matched_seasons.append(row[\"season\"])\n",
    "\n",
    "# --- Display season match result ---\n",
    "if matched_seasons:\n",
    "    print(f\"ğŸ“† The custom range overlaps with: {', '.join(matched_seasons)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ The custom range falls outside defined seasonal bounds.\")\n",
    "\n",
    "# --- Prepare filtered weather data for Dublin Airport ---\n",
    "# Ensure full datetime column first\n",
    "df_weather = prepare_datetime(df_weather, date_col='date', time_col='time')\n",
    "\n",
    "# Then filter using the validated custom range\n",
    "range_df = prepare_weather_data(df_weather)\n",
    "range_df = range_df[(range_df['datetime'] >= custom_start) & (range_df['datetime'] <= custom_end)]\n",
    "\n",
    "print(f\"âœ… Filtered Dublin weather data contains {len(range_df)} rows from {custom_start.date()} â†’ {custom_end.date()}\")\n",
    "display(range_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2f331",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 15b â€“ Reconfigure Weather Range to Match Flights\n",
    "\n",
    "To ensure consistency between flight and weather datasets, we reconfigure the weather data  \n",
    "to match the sixâ€‘month flight window (May â†’ November 2025). This guarantees that both datasets  \n",
    "cover the same period for downstream merging and analysis.\n",
    "\n",
    "Key actions performed:\n",
    "\n",
    "1. ğŸ•’ **Prepare datetime column**  \n",
    "   - Use `prepare_datetime` to ensure the weather dataset has a proper `datetime` field.  \n",
    "   - This step standardises raw date/time strings into usable timestamps.\n",
    "\n",
    "2. ğŸ“… **Define sixâ€‘month custom range**  \n",
    "   - Explicitly set start and end boundaries:  \n",
    "     - `2025-05-20 00:00` â†’ `2025-11-15 23:59`.  \n",
    "   - Matches the flight dataset timeframe for direct comparability.\n",
    "\n",
    "3. ğŸ” **Filter weather data**  \n",
    "   - Apply `prepare_weather_data` for cleaning and alignment.  \n",
    "   - Subset rows to only include records within the sixâ€‘month window.  \n",
    "   - Ensures only relevant weather observations are retained.\n",
    "\n",
    "4. ğŸ“Š **Output transparency**  \n",
    "   - Print the number of rows in the filtered dataset.  \n",
    "   - Display the min/max dates to confirm the range.  \n",
    "   - Preview the first few rows for reviewer validation.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "Aligning weather data with the flight dataset ensures analyses are based on a consistent timeframe.  \n",
    "This step creates a reproducible slice of weather observations that can be merged with flight records  \n",
    "in Stepâ€¯17, enabling meaningful comparisons of delays against seasonal and meteorological conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d849be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reconfigured weather dataset contains 3960 rows\n",
      "ğŸ“† Weather range: 2025-05-20 â†’ 2025-10-31\n",
      "âœˆï¸ Flight dataset now contains 249427 rows\n",
      "ğŸ“† Flight range: 2025-05-20 â†’ 2025-10-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>wddir</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>ww</th>\n",
       "      <th>w</th>\n",
       "      <th>season</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>704616</th>\n",
       "      <td>2025-05-20 00:00:00</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>20000</td>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704617</th>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>20000</td>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704618</th>\n",
       "      <td>2025-05-20 02:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>20000</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704619</th>\n",
       "      <td>2025-05-20 03:00:00</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>20000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704620</th>\n",
       "      <td>2025-05-20 04:00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>20000</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime  temp  rain  wdsp wddir    vis clht  ww   w  \\\n",
       "704616 2025-05-20 00:00:00   3.9   0.0     3   110  20000  999   2  11   \n",
       "704617 2025-05-20 01:00:00   3.9   0.0     3   120  20000  999   2  11   \n",
       "704618 2025-05-20 02:00:00   5.0   0.0     2   110  20000   23   2  11   \n",
       "704619 2025-05-20 03:00:00   4.4   0.0     2   180  20000   22   2  11   \n",
       "704620 2025-05-20 04:00:00   7.0   0.0     2   200  20000   24   2  11   \n",
       "\n",
       "        season        date  hour  \n",
       "704616  Spring  2025-05-20     0  \n",
       "704617  Spring  2025-05-20     1  \n",
       "704618  Spring  2025-05-20     2  \n",
       "704619  Spring  2025-05-20     3  \n",
       "704620  Spring  2025-05-20     4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sched</th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime_hour</th>\n",
       "      <th>delay</th>\n",
       "      <th>delay_calc</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight_iata</th>\n",
       "      <th>status</th>\n",
       "      <th>is_cancelled</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>fr1739</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-20 01:10:00</td>\n",
       "      <td>2025-05-20 01:10:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>fr9612</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>fr651</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>american airlines</td>\n",
       "      <td>aa8330</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>british airways</td>\n",
       "      <td>ba6124</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sched            datetime       datetime_hour  delay  \\\n",
       "0 2025-05-20 01:00:00 2025-05-20 01:00:00 2025-05-20 01:00:00   15.0   \n",
       "1 2025-05-20 01:10:00 2025-05-20 01:10:00 2025-05-20 01:00:00    NaN   \n",
       "2 2025-05-20 01:15:00 2025-05-20 01:15:00 2025-05-20 01:00:00    NaN   \n",
       "3 2025-05-20 04:25:00 2025-05-20 04:25:00 2025-05-20 04:00:00    NaN   \n",
       "4 2025-05-20 04:25:00 2025-05-20 04:25:00 2025-05-20 04:00:00    NaN   \n",
       "\n",
       "   delay_calc            airline flight_iata  status  is_cancelled     type  \n",
       "0        15.0            ryanair      fr1739  landed         False  arrival  \n",
       "1         NaN            ryanair      fr9612  landed         False  arrival  \n",
       "2         NaN            ryanair       fr651  landed         False  arrival  \n",
       "3         NaN  american airlines      aa8330  landed         False  arrival  \n",
       "4         NaN    british airways      ba6124  landed         False  arrival  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ğŸ“Š Step 15b â€“ Reconfigure Weather & Flight Range to Match Available Data\n",
    "\n",
    "# Prepare weather dataset with proper datetime\n",
    "range_df = prepare_weather_data(df_weather)\n",
    "\n",
    "# Define custom range to match flights (May â†’ Oct 2025, since weather ends 31-Oct)\n",
    "custom_start = pd.Timestamp(\"2025-05-20 00:00\")\n",
    "custom_end   = pd.Timestamp(\"2025-10-31 23:59\")\n",
    "\n",
    "# Filter weather data to available window\n",
    "range_df = range_df[(range_df['datetime'] >= custom_start) & (range_df['datetime'] <= custom_end)]\n",
    "\n",
    "print(f\"âœ… Reconfigured weather dataset contains {len(range_df)} rows\")\n",
    "print(f\"ğŸ“† Weather range: {range_df['datetime'].min().date()} â†’ {range_df['datetime'].max().date()}\")\n",
    "\n",
    "# --- Align flights to same cutoff ---\n",
    "df_flights = df_flights[(df_flights['datetime'] >= custom_start) & (df_flights['datetime'] <= custom_end)]\n",
    "\n",
    "print(f\"âœˆï¸ Flight dataset now contains {len(df_flights)} rows\")\n",
    "print(f\"ğŸ“† Flight range: {df_flights['datetime'].min().date()} â†’ {df_flights['datetime'].max().date()}\")\n",
    "\n",
    "display(range_df.head())\n",
    "display(df_flights.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674175b",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 16 â€“ Load and Clean Dublin Airport Flight Data\n",
    "\n",
    "In this step, we load the tidy flight datasets created earlier (arrivals, departures, or the combined file) and prepare them for analysis.  \n",
    "The goal is to ensure that the flight records have a consistent **datetime column** and clean numeric delay values, so they can be merged with weather data later.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸ“‚ **File resilience** â†’ Check which tidy flight files are available in the `data/` folder  \n",
    "  - Prefer the combined dataset if present, otherwise fall back to arrivals + departures.  \n",
    "  - Raises a clear error if no tidy files are found.  \n",
    "\n",
    "- ğŸ“¥ **Load dataset** â†’ Read the appropriate file(s) into memory.  \n",
    "  - Harmonise columns before concatenation to ensure schema consistency.  \n",
    "\n",
    "- ğŸ§¹ **Normalise schema** â†’ Standardise column names to lowercase and strip whitespace.  \n",
    "\n",
    "- ğŸ•’ **Canonical datetime** â†’ Create a `datetime` column based on the scheduled time (`sched`).  \n",
    "  - Falls back to existing `datetime` or `date+time` if needed.  \n",
    "\n",
    "- ğŸ”¢ **Numeric delays** â†’ Convert delay fields into numeric values for analysis.  \n",
    "\n",
    "- ğŸš« **Final cleaning** â†’ Drop rows without valid datetime values and reset the index.  \n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "This step ensures the flight dataset is reproducible, resilient, and ready for analysis.  \n",
    "By anchoring on the scheduled time (`sched`), we create a consistent temporal reference point that can be reliably merged with hourly weather data in **Stepâ€¯17**, enabling meaningful delay vs. weather comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Available files in data/: ['dublin_airport_hourly.csv', 'dub_arrivals_tidy.csv', 'dub_departures_tidy.csv', 'dub_flights_tidy.csv', 'dub_flights_weather_6months.csv', 'raw_flights']\n",
      "âœ… Loaded flight dataset from: combined (269276 rows)\n",
      "ğŸ§¹ Cleaned flight dataset has 249427 rows with valid datetime\n",
      "âœˆï¸ Flight dataset aligned to weather availability: 249427 rows\n",
      "ğŸ“† Flight range: 2025-05-20 â†’ 2025-10-31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sched</th>\n",
       "      <th>datetime</th>\n",
       "      <th>datetime_hour</th>\n",
       "      <th>delay</th>\n",
       "      <th>delay_calc</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight_iata</th>\n",
       "      <th>status</th>\n",
       "      <th>is_cancelled</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>fr1739</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-20 01:10:00</td>\n",
       "      <td>2025-05-20 01:10:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>fr9612</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>fr651</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>american airlines</td>\n",
       "      <td>aa8330</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>british airways</td>\n",
       "      <td>ba6124</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sched            datetime        datetime_hour  delay  \\\n",
       "0  2025-05-20 01:00:00 2025-05-20 01:00:00  2025-05-20 01:00:00   15.0   \n",
       "1  2025-05-20 01:10:00 2025-05-20 01:10:00  2025-05-20 01:00:00    NaN   \n",
       "2  2025-05-20 01:15:00 2025-05-20 01:15:00  2025-05-20 01:00:00    NaN   \n",
       "3  2025-05-20 04:25:00 2025-05-20 04:25:00  2025-05-20 04:00:00    NaN   \n",
       "4  2025-05-20 04:25:00 2025-05-20 04:25:00  2025-05-20 04:00:00    NaN   \n",
       "\n",
       "   delay_calc            airline flight_iata  status  is_cancelled     type  \n",
       "0        15.0            ryanair      fr1739  landed         False  arrival  \n",
       "1         NaN            ryanair      fr9612  landed         False  arrival  \n",
       "2         NaN            ryanair       fr651  landed         False  arrival  \n",
       "3         NaN  american airlines      aa8330  landed         False  arrival  \n",
       "4         NaN    british airways      ba6124  landed         False  arrival  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ğŸ“Š Step 16 â€“ Load and clean Dublin Airport flight data\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# --- Inspect available tidy flight files ---\n",
    "print(\"ğŸ“‚ Available files in data/:\", os.listdir(DATA_DIR))\n",
    "\n",
    "# --- Load the combined tidy flights file if present; fall back to arrivals+departures ---\n",
    "flights_path = DATA_DIR / \"dub_flights_tidy.csv\"\n",
    "arr_path = DATA_DIR / \"dub_arrivals_tidy.csv\"\n",
    "dep_path = DATA_DIR / \"dub_departures_tidy.csv\"\n",
    "\n",
    "if flights_path.exists():\n",
    "    # Preferred: combined tidy dataset\n",
    "    df_flights = pd.read_csv(flights_path, low_memory=False)\n",
    "    source_used = \"combined\"\n",
    "elif arr_path.exists() and dep_path.exists():\n",
    "    # Fallback: arrivals + departures concatenated\n",
    "    df_arrivals = pd.read_csv(arr_path, low_memory=False)\n",
    "    df_departures = pd.read_csv(dep_path, low_memory=False)\n",
    "    # Harmonise columns before concatenation\n",
    "    common_cols = sorted(set(df_arrivals.columns).intersection(set(df_departures.columns)))\n",
    "    df_flights = pd.concat(\n",
    "        [df_arrivals[common_cols], df_departures[common_cols]],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    source_used = \"arrivals+departures\"\n",
    "else:\n",
    "    raise FileNotFoundError(\"âŒ No tidy flight files found in data/. Expected dub_flights_tidy.csv or both arrivals/departures.\")\n",
    "\n",
    "print(f\"âœ… Loaded flight dataset from: {source_used} ({len(df_flights)} rows)\")\n",
    "\n",
    "# --- Normalise column names and whitespace ---\n",
    "df_flights.columns = df_flights.columns.str.strip().str.lower()\n",
    "\n",
    "# --- Ensure a proper datetime column ---\n",
    "# Use 'sched' (scheduled time) as the canonical datetime for weather alignment\n",
    "if \"sched\" in df_flights.columns:\n",
    "    df_flights[\"datetime\"] = pd.to_datetime(df_flights[\"sched\"], errors=\"coerce\")\n",
    "elif \"datetime\" in df_flights.columns:\n",
    "    df_flights[\"datetime\"] = pd.to_datetime(df_flights[\"datetime\"], errors=\"coerce\")\n",
    "elif \"date\" in df_flights.columns and \"time\" in df_flights.columns:\n",
    "    df_flights = prepare_datetime(df_flights, date_col=\"date\", time_col=\"time\")\n",
    "else:\n",
    "    raise KeyError(\"âŒ Flight data needs either 'sched', 'datetime', or both 'date' and 'time' columns.\")\n",
    "\n",
    "# --- Convert delay columns to numeric if present ---\n",
    "for col in [\"arr_delay\", \"dep_delay\", \"delay\", \"delay_minutes\", \"delay_calc\"]:\n",
    "    if col in df_flights.columns:\n",
    "        df_flights[col] = pd.to_numeric(df_flights[col], errors=\"coerce\")\n",
    "\n",
    "# --- Drop rows without valid datetime and reset index ---\n",
    "df_flights = df_flights.dropna(subset=[\"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "# --- Restrict flights to weather availability (up to 31-Oct-2025) ---\n",
    "cutoff_end = pd.Timestamp(\"2025-10-31 23:59\")\n",
    "df_flights = df_flights[df_flights[\"datetime\"] <= cutoff_end].reset_index(drop=True)\n",
    "\n",
    "print(f\"ğŸ§¹ Cleaned flight dataset has {len(df_flights)} rows with valid datetime\")\n",
    "print(f\"âœˆï¸ Flight dataset aligned to weather availability: {len(df_flights)} rows\")\n",
    "print(f\"ğŸ“† Flight range: {df_flights['datetime'].min().date()} â†’ {df_flights['datetime'].max().date()}\")\n",
    "\n",
    "display(df_flights.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbe1a5",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 17 â€“ Merge Flights with Weather (Floor-to-Hour Alignment)\n",
    "\n",
    "In this step, we merge the cleaned flight dataset with the filtered Dublin Airport weather data.  \n",
    "Instead of relying on `merge_asof` tolerances, we **floor both datasets to the nearest hour**.  \n",
    "This ensures that every flight scheduled within a given hour is matched to the corresponding hourly weather observation.\n",
    "\n",
    "The process includes:\n",
    "\n",
    "- ğŸ•’ Creating a `datetime_hour` column in both flights and weather by flooring to the hour  \n",
    "- ğŸ”— Performing a left merge on `datetime_hour` so each flight inherits weather conditions **and seasonal labels**  \n",
    "- âœ… Inspecting the merged dataset to confirm six months of flights align with six months of weather observations  \n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "Flooring to the hour guarantees reproducible alignment between flights and weather.  \n",
    "It avoids NaN values caused by minute/second mismatches and ensures every flight has a valid weather context.  \n",
    "This merged dataset forms the foundation for subsequent analyses of how meteorological conditions and seasons  \n",
    "correlate with flight delays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "415a3932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged dataset: 249427 flight records with hourly weather attached\n",
      "ğŸ“† Flight range:  2025-05-20 â†’ 2025-10-31\n",
      "ğŸ“† Weather range: 2025-05-20 â†’ 2025-10-31\n",
      "ğŸ” Weather datetime match rate: 100.0%\n",
      "ğŸ” Season assignment rate:     100.0%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sched</th>\n",
       "      <th>datetime_x</th>\n",
       "      <th>datetime_hour</th>\n",
       "      <th>delay</th>\n",
       "      <th>delay_calc</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight_iata</th>\n",
       "      <th>status</th>\n",
       "      <th>is_cancelled</th>\n",
       "      <th>type</th>\n",
       "      <th>datetime_y</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>wddir</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>ww</th>\n",
       "      <th>w</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>fr1739</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>20000</td>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-20 01:10:00</td>\n",
       "      <td>2025-05-20 01:10:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>fr9612</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>20000</td>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "      <td>2025-05-20 01:15:00</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ryanair</td>\n",
       "      <td>fr651</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 01:00:00</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>20000</td>\n",
       "      <td>999</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>american airlines</td>\n",
       "      <td>aa8330</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 04:00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>20000</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:25:00</td>\n",
       "      <td>2025-05-20 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>british airways</td>\n",
       "      <td>ba6124</td>\n",
       "      <td>landed</td>\n",
       "      <td>False</td>\n",
       "      <td>arrival</td>\n",
       "      <td>2025-05-20 04:00:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>20000</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Spring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sched          datetime_x       datetime_hour  delay  \\\n",
       "0  2025-05-20 01:00:00 2025-05-20 01:00:00 2025-05-20 01:00:00   15.0   \n",
       "1  2025-05-20 01:10:00 2025-05-20 01:10:00 2025-05-20 01:00:00    NaN   \n",
       "2  2025-05-20 01:15:00 2025-05-20 01:15:00 2025-05-20 01:00:00    NaN   \n",
       "3  2025-05-20 04:25:00 2025-05-20 04:25:00 2025-05-20 04:00:00    NaN   \n",
       "4  2025-05-20 04:25:00 2025-05-20 04:25:00 2025-05-20 04:00:00    NaN   \n",
       "\n",
       "   delay_calc            airline flight_iata  status  is_cancelled     type  \\\n",
       "0        15.0            ryanair      fr1739  landed         False  arrival   \n",
       "1         NaN            ryanair      fr9612  landed         False  arrival   \n",
       "2         NaN            ryanair       fr651  landed         False  arrival   \n",
       "3         NaN  american airlines      aa8330  landed         False  arrival   \n",
       "4         NaN    british airways      ba6124  landed         False  arrival   \n",
       "\n",
       "           datetime_y  temp  rain  wdsp wddir    vis clht  ww   w  season  \n",
       "0 2025-05-20 01:00:00   3.9   0.0     3   120  20000  999   2  11  Spring  \n",
       "1 2025-05-20 01:00:00   3.9   0.0     3   120  20000  999   2  11  Spring  \n",
       "2 2025-05-20 01:00:00   3.9   0.0     3   120  20000  999   2  11  Spring  \n",
       "3 2025-05-20 04:00:00   7.0   0.0     2   200  20000   24   2  11  Spring  \n",
       "4 2025-05-20 04:00:00   7.0   0.0     2   200  20000   24   2  11  Spring  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sched</th>\n",
       "      <th>datetime_x</th>\n",
       "      <th>datetime_hour</th>\n",
       "      <th>delay</th>\n",
       "      <th>delay_calc</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight_iata</th>\n",
       "      <th>status</th>\n",
       "      <th>is_cancelled</th>\n",
       "      <th>type</th>\n",
       "      <th>datetime_y</th>\n",
       "      <th>temp</th>\n",
       "      <th>rain</th>\n",
       "      <th>wdsp</th>\n",
       "      <th>wddir</th>\n",
       "      <th>vis</th>\n",
       "      <th>clht</th>\n",
       "      <th>ww</th>\n",
       "      <th>w</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249422</th>\n",
       "      <td>2025-10-31 19:15:00</td>\n",
       "      <td>2025-10-31 19:15:00</td>\n",
       "      <td>2025-10-31 19:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>british airways</td>\n",
       "      <td>ba8952</td>\n",
       "      <td>active</td>\n",
       "      <td>False</td>\n",
       "      <td>departure</td>\n",
       "      <td>2025-10-31 19:00:00</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>160</td>\n",
       "      <td>35000</td>\n",
       "      <td>130</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249423</th>\n",
       "      <td>2025-10-31 19:15:00</td>\n",
       "      <td>2025-10-31 19:15:00</td>\n",
       "      <td>2025-10-31 19:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aer lingus</td>\n",
       "      <td>ei3258</td>\n",
       "      <td>active</td>\n",
       "      <td>False</td>\n",
       "      <td>departure</td>\n",
       "      <td>2025-10-31 19:00:00</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>160</td>\n",
       "      <td>35000</td>\n",
       "      <td>130</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249424</th>\n",
       "      <td>2025-10-31 19:15:00</td>\n",
       "      <td>2025-10-31 19:15:00</td>\n",
       "      <td>2025-10-31 19:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>british airways</td>\n",
       "      <td>ba557</td>\n",
       "      <td>active</td>\n",
       "      <td>False</td>\n",
       "      <td>departure</td>\n",
       "      <td>2025-10-31 19:00:00</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>160</td>\n",
       "      <td>35000</td>\n",
       "      <td>130</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249425</th>\n",
       "      <td>2025-10-31 19:50:00</td>\n",
       "      <td>2025-10-31 19:50:00</td>\n",
       "      <td>2025-10-31 19:00:00</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>transavia</td>\n",
       "      <td>hv7909</td>\n",
       "      <td>active</td>\n",
       "      <td>False</td>\n",
       "      <td>departure</td>\n",
       "      <td>2025-10-31 19:00:00</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>160</td>\n",
       "      <td>35000</td>\n",
       "      <td>130</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249426</th>\n",
       "      <td>2025-10-31 23:00:00</td>\n",
       "      <td>2025-10-31 23:00:00</td>\n",
       "      <td>2025-10-31 23:00:00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>hisky</td>\n",
       "      <td>h7474</td>\n",
       "      <td>active</td>\n",
       "      <td>False</td>\n",
       "      <td>departure</td>\n",
       "      <td>2025-10-31 23:00:00</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11</td>\n",
       "      <td>160</td>\n",
       "      <td>11000</td>\n",
       "      <td>45</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>Autumn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sched          datetime_x       datetime_hour  delay  \\\n",
       "249422  2025-10-31 19:15:00 2025-10-31 19:15:00 2025-10-31 19:00:00    NaN   \n",
       "249423  2025-10-31 19:15:00 2025-10-31 19:15:00 2025-10-31 19:00:00    NaN   \n",
       "249424  2025-10-31 19:15:00 2025-10-31 19:15:00 2025-10-31 19:00:00    NaN   \n",
       "249425  2025-10-31 19:50:00 2025-10-31 19:50:00 2025-10-31 19:00:00   31.0   \n",
       "249426  2025-10-31 23:00:00 2025-10-31 23:00:00 2025-10-31 23:00:00   54.0   \n",
       "\n",
       "        delay_calc          airline flight_iata  status  is_cancelled  \\\n",
       "249422         NaN  british airways      ba8952  active         False   \n",
       "249423         NaN       aer lingus      ei3258  active         False   \n",
       "249424         NaN  british airways       ba557  active         False   \n",
       "249425        31.0        transavia      hv7909  active         False   \n",
       "249426        54.0            hisky       h7474  active         False   \n",
       "\n",
       "             type          datetime_y  temp  rain  wdsp wddir    vis clht  ww  \\\n",
       "249422  departure 2025-10-31 19:00:00  11.2   0.0     7   160  35000  130   2   \n",
       "249423  departure 2025-10-31 19:00:00  11.2   0.0     7   160  35000  130   2   \n",
       "249424  departure 2025-10-31 19:00:00  11.2   0.0     7   160  35000  130   2   \n",
       "249425  departure 2025-10-31 19:00:00  11.2   0.0     7   160  35000  130   2   \n",
       "249426  departure 2025-10-31 23:00:00  10.6   0.1    11   160  11000   45  80   \n",
       "\n",
       "         w  season  \n",
       "249422  11  Autumn  \n",
       "249423  11  Autumn  \n",
       "249424  11  Autumn  \n",
       "249425  11  Autumn  \n",
       "249426  82  Autumn  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ğŸ“Š Step 17 â€“ Merge Flights with Weather (Unified Schema)\n",
    "\n",
    "# --- Floor both datasets to nearest hour ---\n",
    "df_flights['datetime_hour'] = df_flights['datetime'].dt.floor('h')\n",
    "range_df['datetime_hour']   = range_df['datetime'].dt.floor('h')\n",
    "\n",
    "# --- Ensure seasons are assigned before merging ---\n",
    "range_df = assign_season_vectorized(range_df, datetime_col=\"datetime\")\n",
    "\n",
    "# --- Restrict to unified schemas ---\n",
    "df_flights = df_flights[[c for c in essential_flight_cols if c in df_flights.columns]]\n",
    "range_df   = range_df[[c for c in delay_relevant_weather_cols if c in range_df.columns]]\n",
    "\n",
    "# --- Merge flights with weather schema ---\n",
    "merged_df = pd.merge(\n",
    "    df_flights,\n",
    "    range_df,\n",
    "    on=\"datetime_hour\",\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\"   # âœ… ensures each flight hour maps to at most one weather record\n",
    ")\n",
    "\n",
    "print(f\"âœ… Merged dataset: {len(merged_df)} flight records with hourly weather attached\")\n",
    "\n",
    "# --- Flight range with safe fallback ---\n",
    "if \"datetime\" in merged_df.columns and merged_df[\"datetime\"].notna().any():\n",
    "    flight_min = merged_df[\"datetime\"].min().date()\n",
    "    flight_max = merged_df[\"datetime\"].max().date()\n",
    "elif \"datetime_hour\" in merged_df.columns and merged_df[\"datetime_hour\"].notna().any():\n",
    "    flight_min = merged_df[\"datetime_hour\"].min().date()\n",
    "    flight_max = merged_df[\"datetime_hour\"].max().date()\n",
    "else:\n",
    "    raise KeyError(\"No valid flight timestamp column found in merged_df\")\n",
    "\n",
    "# --- Weather range ---\n",
    "weather_min = range_df['datetime'].min().date()\n",
    "weather_max = range_df['datetime'].max().date()\n",
    "\n",
    "print(f\"ğŸ“† Flight range:  {flight_min} â†’ {flight_max}\")\n",
    "print(f\"ğŸ“† Weather range: {weather_min} â†’ {weather_max}\")\n",
    "\n",
    "# --- Explicit cutoff confirmation ---\n",
    "if flight_max > weather_max:\n",
    "    print(f\"âš ï¸ Cutoff applied: Flights after {weather_max} were excluded to align with weather availability.\")\n",
    "\n",
    "# --- Match rate diagnostics ---\n",
    "weather_match_rate = merged_df['datetime_hour'].notna().mean()\n",
    "season_match_rate  = merged_df['season'].notna().mean()\n",
    "\n",
    "print(f\"ğŸ” Weather datetime match rate: {weather_match_rate:.1%}\")\n",
    "print(f\"ğŸ” Season assignment rate:     {season_match_rate:.1%}\")\n",
    "\n",
    "# --- Preview merged dataset ---\n",
    "display(merged_df.head())\n",
    "display(merged_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7ae29",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 18 â€“ Save Merged Flights + Weather Dataset\n",
    "\n",
    "With flights and weather successfully merged in Stepâ€¯17, we now persist the unified dataset  \n",
    "to disk as a reproducible artifact. This ensures that the integration step does not need to  \n",
    "be rerun each time and provides a transparent checkpoint for downstream analysis.\n",
    "\n",
    "Key actions performed:\n",
    "\n",
    "1. ğŸ’¾ **Define output path**  \n",
    "   - Save the merged dataset to `data/dub_flights_weather_6months.csv`.  \n",
    "   - The filename reflects both the content (flights + weather) and the sixâ€‘month window.\n",
    "\n",
    "2. ğŸ“¥ **Write to CSV**  \n",
    "   - Export the DataFrame without the index for a clean tabular structure.  \n",
    "   - Ensures portability and easy reâ€‘loading in later steps.\n",
    "\n",
    "3. âœ… **Confirm save operation**  \n",
    "   - Print the file path and row count to verify successful export.  \n",
    "   - Provides reviewers with immediate feedback on dataset size and location.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "Saving the merged dataset creates a reproducible, versionâ€‘controlled artifact that can be shared  \n",
    "with reviewers or reused in subsequent analyses. It marks the transition from data preparation  \n",
    "to exploratory analysis, ensuring that both flight and weather records are consistently aligned  \n",
    "and ready for deeper investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10d3a483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved merged flights+weather dataset to data\\dub_flights_weather_6months.csv\n",
      "ğŸ“Š Final dataset contains 249427 rows\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š Step 18 â€“ Save Merged Flights + Weather Dataset\n",
    "\n",
    "OUTPUT_FILE = DATA_DIR / \"dub_flights_weather_6months.csv\"\n",
    "\n",
    "# Save merged dataset to CSV\n",
    "merged_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"ğŸ’¾ Saved merged flights+weather dataset to {OUTPUT_FILE}\")\n",
    "print(f\"ğŸ“Š Final dataset contains {len(merged_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a39e30",
   "metadata": {},
   "source": [
    "### ğŸ“Š Step 19 â€“ Audit Missing Values in Merged Dataset\n",
    "\n",
    "After saving and reloading the merged flights + weather dataset, we now perform a  \n",
    "data quality check to identify missing values across all columns. This ensures the  \n",
    "dataset is complete and reliable before moving into exploratory analysis.\n",
    "\n",
    "Key actions performed:\n",
    "\n",
    "1. ğŸ“¥ **Reload dataset**  \n",
    "   - Load `dub_flights_weather_6months.csv` back into memory for validation.\n",
    "\n",
    "2. ğŸ” **Count missing values**  \n",
    "   - Use `.isna().sum()` to calculate the number of missing entries per column.  \n",
    "   - Sort results to highlight the most affected fields.\n",
    "\n",
    "3. ğŸ“Š **Calculate percentages**  \n",
    "   - Express missing values as percentages of total rows for each column.  \n",
    "   - Provides a clearer sense of data quality impact.\n",
    "\n",
    "4. âš ï¸ **Identify problematic columns**  \n",
    "   - Flag all columns with any missing values.  \n",
    "   - Helps reviewers quickly see which variables may need imputation or exclusion.\n",
    "\n",
    "ğŸ“Œ *Why this matters:*  \n",
    "Auditing missing values is a critical step in ensuring dataset integrity.  \n",
    "By quantifying both counts and percentages, reviewers can assess whether  \n",
    "data gaps are minor (e.g., a few missing weather readings) or significant  \n",
    "enough to affect downstream analyses of flight delays vs. weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3999c104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Missing values per column (merged dataset):\n",
      "delay            75494\n",
      "delay_calc       73692\n",
      "flight_iata       1209\n",
      "airline           1122\n",
      "sched                0\n",
      "datetime_hour        0\n",
      "status               0\n",
      "is_cancelled         0\n",
      "type                 0\n",
      "datetime_hour        0\n",
      "temp                 0\n",
      "rain                 0\n",
      "wdsp                 0\n",
      "wddir                0\n",
      "vis                  0\n",
      "clht                 0\n",
      "ww                   0\n",
      "w                    0\n",
      "season               0\n",
      "dtype: int64\n",
      "\n",
      "ğŸ“Š Percentage of missing values per column:\n",
      "sched             0.00\n",
      "datetime_hour     0.00\n",
      "delay            30.27\n",
      "delay_calc       29.54\n",
      "airline           0.45\n",
      "flight_iata       0.48\n",
      "status            0.00\n",
      "is_cancelled      0.00\n",
      "type              0.00\n",
      "datetime_hour     0.00\n",
      "temp              0.00\n",
      "rain              0.00\n",
      "wdsp              0.00\n",
      "wddir             0.00\n",
      "vis               0.00\n",
      "clht              0.00\n",
      "ww                0.00\n",
      "w                 0.00\n",
      "season            0.00\n",
      "dtype: float64\n",
      "\n",
      "âš ï¸ Columns with missing values: ['delay', 'delay_calc', 'flight_iata', 'airline']\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š Step 19 â€“ Analyse Missing Values in Merged Dataset (Unified Schema)\n",
    "\n",
    "# Load the merged dataset back from file\n",
    "merged_df = pd.read_csv(DATA_DIR / \"dub_flights_weather_6months.csv\", low_memory=False)\n",
    "\n",
    "# --- Combine flight + weather schemas ---\n",
    "merged_schema = essential_flight_cols + delay_relevant_weather_cols\n",
    "\n",
    "# --- Restrict to unified schema ---\n",
    "merged_df = merged_df[[c for c in merged_schema if c in merged_df.columns]]\n",
    "\n",
    "# --- Check for missing values across unified schema ---\n",
    "missing_summary = merged_df.isna().sum().sort_values(ascending=False)\n",
    "missing_percent = (merged_df.isna().mean() * 100).round(2)\n",
    "missing_columns = missing_summary[missing_summary > 0].index.tolist()\n",
    "\n",
    "print(\"ğŸ” Missing values per column (merged dataset):\")\n",
    "print(missing_summary)\n",
    "print(\"\\nğŸ“Š Percentage of missing values per column:\")\n",
    "print(missing_percent)\n",
    "print(f\"\\nâš ï¸ Columns with missing values: {missing_columns}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
